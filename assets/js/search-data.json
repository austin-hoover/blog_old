{
  
    
        "post0": {
            "title": "Painting a particle beam",
            "content": "This post explains an imporant step in the creation of intense proton beams in the Spallation Neutron Source (SNS). The method is often called &quot;phase space painting&quot; or simply &quot;painting&quot;, hence the title. . A quick tour of the SNS . There is a great scene in the otherwise disappointing movie Iron Man 2 in which Tony Stark builds a particle accelerator in his house. . The movie makes it look like pretty complicated process, but modern large-scale accelerators are immensely more complicated than the one Stark builds. In fact, I&#39;m actually quite amazed that such machines have been built and work as expected. One example is the SNS (overhead diagram below). . . The goal of the SNS is to produce extremely bright, pulsed neutron beams for neutron scattering experiments. These neutrons are produced through the process of spallation by colliding a proton beam with a Mercury target. The power of the proton beam must be as high as possible to maximize the brightness of the neutron beam, and creating such a high-power beam is a multi-step process; particles must travel all the way from the ion source on the far left to the target on the far right, passing though many different sections on their journey. We&#39;re first going to mention the basic function of each of these sections, and then we&#39;re going to look in more detail at the relatively small but extremely important injection region of the machine. This is the point where the HEBT meets the accumulator ring in the above diagram. . Ion source, front end, and linac . The beam originates in the ion source. As explained later, the beam is not actually made of protons at this point, but is instead made of H$^-$ ions (proton + two electrons). The ion source consists of a vacuum chamber filled with gas, and an oscillating electric field which ionizes the gas to form a glowing, pink plasma. The H$^-$ particles are then extracted from the plasma; I asked one of the researchers at SNS (who doesn&#39;t work on the ion source) how exactly these ions are extracted, and they said it&#39;s &quot;black magic&quot;, so I guess it&#39;s not straightforward. . . Diagram of the SNS ion source. From [1]. The H$^-$ beam is then accelerated to around 2.5 MeV, focused, and &quot;chopped&quot; into 1000 minipulses. Each minipulse is about 700 nanoseconds long, and they&#39;re separated by a gap of about 300 nanoseconds. The dynamics in this region are greatly affected by space charge, and it&#39;s hard to know exactly what the beam looks like when it exits the ion source. The SNS has built an exact replica of this front end called the Beam Test Facility (BTF) which is currently being used to better understand this section of the machine [2]. . Each minipulse is now ready to be accelerated. The next section of the machine is called the linac (linear accelerator), a long, straight section whose purpose is to accelerate the minipulses up to 1 GeV (around 90% of the speed of light) while maintaining an acceptable beam size. This is done using a series of normal-conducting and superconducting radio-frequency cavities. There is a lot to talk about in the linac, but I&#39;ll stop here since I don&#39;t have much knowledge of this area of the machine (yet). . HEBT, injection region, and accumulator ring . The high-energy beam transport (HEBT, pronounced &quot;hebbet&quot;) guides the fully accelerated minipulse from the linac to the left edge of the accumulator ring. At this point, all the ions in the minipulse are converted to protons and injected into the accumulator ring (more on this in the next section). The minpulse takes 1 microsecond to travel around the ring, at which point a second minipulse is injected and the circulating beam doubles in intensity. This repeats 1000 times over the course of one millisecond until the final beam, called a pulse, contains around $1.5 times 10^{14}$ protons. That seems like a lot until you consider that Avagadros number is one billion times larger! 60 of these pulses contains about the same energy as a stick of dynamite. . RTBT and target . Finally, the entire pulse is extracted from the ring and travels down the ring-target-beam-transport (RTBT) in which it is directed to the Mercury target, producing neutrons. These neutrons are then cooled and transported to various instrumental halls for use in neutron scattering experiments. . . Injection . Now we&#39;re going to discuss the injection region in more detail (see image below). Somehow, all the negatively charged ions need to be converted to positively charge protons, and the beam from the linac needs to merge with the circulating beam in the ring without derailing its trajectory; it&#39;s as if the ions were trying to merge onto a busy highway. The specific method used at SNS is charge exchange injection, which we discuss first. We&#39;ll then move on to discuss phase space painting, which is used to mitigate the effects of space charge in intense beams. . . H$^-$ charge exchange . Consider two oppositely charged beams which have the same kinetic energy but opposite charges. Also assume that we&#39;re dealing with point particles which do not interact with each other and have no transverse velocity. If these beams are sent thought a dipole magnet, their paths will be bent in opposite directions with the same radius of curvature. If they additionally have opposite angles of incidence, there will be a point in the dipole at which both beams are moving parallel to each other. Now imagine that, at this very instance, the charge of all the particles in one of the beams changes sign. The two beams would then be identical and would continue along the same trajectory, although there may be an offset. It&#39;s possible to also choose the horizontal and vertical positions of the two beams such that they converge and travel along identical paths. . . Oppositely charged particles are bent in different directions in a dipole magnetic field. The idea is to do this with the two beams in the SNS: the negatively charged H$^-$ beam the linac and the positively charged proton beam which is circulating in the ring. Russian scientists developed a method to do this in the 1960&#39;s using a thin foil which strips the two electrons from the Hydrogen ions but leaves the protons. The foil properties need to be chosen carefully. It needs to be the right material and thick enough to have a high stripping efficiency (number of ions successfully stripped divided by total number of ions), but not so thick that most of the protons are scattered. It also needs to be able to survive high numbers of foil hits without being destroyed. Thus, the choice of foil parameters requires a knowledge of materials science. The SNS uses diamond foils as in the following images. . . Dealing with extra particles . Some of the H$^-$ hold on to their electrons as they pass through the foil, and some only lose one electron, becoming H$^0$. To deal with these particles, the foil is placed in a dipole field. Because many of the H$^0$ particles are in excited states, it is likely that their electron will be stripped by the magnetic field soon after the foil; this is known as Lorentz stripping. So a lot of these will become protons and join the circulating beam, just a bit late to the party. The remaining non-protons continue away from the ring and encounter another foil which removes the electrons so that they can be guided to a beam dump. There is also the need to catch the stripped electrons, which can have significant kinetic energies, but I won&#39;t discuss that here. . Is Liouville&#39;s theorem violated? . Those familiar with Liouville&#39;s theorem may object to the charge-exchange method. Liouville&#39;s theorem applies to any system which obeys Hamilton&#39;s equations: . $$ dot{ mathbf{q}} = frac{ partial mathbf{H}}{ partial mathbf{p}} , quad dot{ mathbf{p}} = - frac{ partial mathbf{H}}{ partial mathbf{q}},$$ . where $ mathbf{q}$ are the coordinates and $ mathbf{p}$ are the momenta. Imagine we took a volume of phase space and started to fill it with particles; in fact, we fill all of the infinite number of points inside the volume. Then we evolve the system in time. The final distribution of particles may have changed shape, but Liouville&#39;s theorem states that its volume will not have changed. Mathematically, this is due to Hamilton&#39;s equations being equivalent to a coordinate transformation whose Jacobian has a determinant equal to one. So the objection is that the phase space volume of the entire system (linac beam + circulating beam) seems to decrease when they are merged, i.e., the linac beam is stacked directly on top of the circulating beam, and that this should be disallowed by Liouville&#39;s theorem. Is this true? . I read a paper by A. Ruggiero which helped to clarify this issue [3]. The key point is that Liouville&#39;s theorem deals with distributions rather than finite numbers of particles. Any finite number of particles will not fill up every point in phase space, so there is nothing preventing another finite number of particles from being stacked on top of the original group. The real limitation is how to stack the two groups, which is where the charge exchange technique becomes necessary. . The future: lasers . A major research project at the SNS is to demonstrate laser-assisted charge exchange (LACE), in which a laser is used to strip the electrons intead of a foil. This would overcome the scattering losses from foils as well as their finite lifetimes, and is a very promising direction as machines continue to increase in power [4]. . Phase space painting . In the last section, we assumed that the particles didn&#39;t interact with each other, but in reality, space charge is the fundamental limit on the intensity in high-power hadron accelerators. Injecting at the same position in space will cause the beam to become very dense, and the beam will then expand due to the increased space charge forces. It&#39;s likely that this will produce a very non-uniform distribution which, as mentioned in this post, is undesirable. This is the motivation for so-called phase space painting or simply painting. The idea is to change the transverse position and momentum of the circulating beam over time in order to slowly fill or &quot;paint&quot; the beam in phase space and hopefully produce a more uniform density beam. Another motivation for painting is to avoid excessive foil hits, since these lead to shorter foil lifetimes and also beam scattering. . Time-dependent kicker magnets . Here is a zoomed in view of the injection region. . . The blue elements which aren&#39;t labeled in the ring are just quadrupoles used to focus the beam. This leaves the &quot;bump&quot; or &quot;kicker&quot; magnets and the &quot;Chicane&quot; magnets. These are both dipoles, but they are a bit different. The chicane dipoles provide a fixed horizontal bump to the closed orbit so that it is aligned with the beam from the linac. The kickers, on the other hand, can move the closed orbit horizontally or vertically, and they&#39;re time-dependent. Regarding the latter point, the current from the magnet&#39;s power supply, and therefore the magnetic field, can be varied during injection. Let&#39;s take a look at the vertical closed orbit with the kickers turned on (black line). . . The dark blue boxes are quadrupoles, the red boxes are Chicane dipoles, and the remaining elements are the horizontal (green) and vertical (yellow) kickers. Without the kickers, the closed orbit will just go straight through the center of each magnet, and a similar thing holds in the horizontal plane. Thus, we have control over the horizontal and vertical position of the circulating beam relative to the injected beam. But there is also the possibility that the trajectory is converging or diverging at the foil, so we also have control over the horizontal and vertical circulating beam slope relative to the injected beam. These eight kickers therefore give full control over the transverse phase space coordinates of the circulating beam relative to the injected beam at every point during the injection. . Production painting . The time-dependence of each kicker magnet is determined by a waveform which determines the current given to its power supply as a function of time; for example, we could have a linear waveform, square root waveform, etc. Choosing these waveforms amounts to choosing the initial and final position/slope of the circulating beam, as well as the rate of change in the position/slope. The standard &quot;production&quot; scheme in the SNS (as in neutron production) is to use a square root waveform so that . $$ x_{inj} - x_{co} = (x_{max} - x_{min}) sqrt{t / t_{max}} + x_{min}, $$ $$ y_{inj} - y_{co} = (y_{max} - y_{min}) sqrt{t / t_{max}} + y_{min}, $$ . where the co subscript means &quot;closed orbit&quot; and inj means &quot;injected beam&quot;. The slope of the circulating beam is kept at zero in this scheme. . It&#39;s probably best to use some visualizations at this point. I simulated the injection painting using PyORBIT; included in this simulation are effects such as space charge, nonlinear magnetic fringe fields, scattering from the foil, etc., so the results should be somewhat similar to the real world. 260 simulation particles were injected on each turn to give a final number of 260,000, which should provide good statistics for the space charge solver which operates on a $128 times 128$ transverse grid. The whole simulation took a few hours to run on my laptop. (The space charge solver I used makes some approximations in the longitudinal dimension; a more realistic solver will push the execution time from a few hours to a few days.) . The following animation shows this simulated beam at the injection point during the first 35 turns in the ring. The off-diagonal subplots show the correlations between the four phase space variables (a sample of 10,000 particles is used) and the on-diagonal subplots show the histograms for each variable. The foil location is shown by the red dot in the $x$-$y$ plane. . . Keep in mind that each little cluster is actually a bunch of particles; it&#39;s hard to resolve because the width is small compared to the full beam. Notice that, since the circulating and injected beams are offset to begin with, the injected particles start to trace ellipses in the $x$-$x&#39;$ and $y$-$y&#39;$ projections. The frequencies at which the particles oscillate in each plane are not the same, so the path in $x$-$y$ space is constantly changing, eventually filling a rectangular region. The next animation shows the beam over all 1000 turns. . . Notice that the beam size is slowly increasing, and also that the density is steadily increasing; this is only apparent from the histograms since I&#39;m using a random sample of particles in the scatter plots. Space charge, as well as nonlinear effects, tend to round the hard edges of the the originally rectangular beam. The beam also exhibits some interesting dynamics after turn 100, and again after turn 800, when it begins to tilt back and forth, which is probably due to space charge coupling the two planes. In the end, the beam has a somewhat uniform density, is not tilted, is not rotating, and is quite intense ($1.5 times 10^{14}$ particles), so the basic goals of the painting scheme have been achieved. The beam can now be extracted and collided with the target to produce neutrons. . Conclusion . This post briefly outlined a method to &quot;paint&quot; a high-intensity proton beam in a ring. One exciting project which is going on right now is to modify the painting method to produce an elliptical, uniform density, rotating beam which approximates the Danilov distribution. This basically involves changing the slope along with the position of the circulating beam. . References . [1] &quot;The Spallation Neutron Source accelerator system design,&quot; Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment 763:610–673 (2014). https://doi.org/https://doi.org/10.1016/j.nima.2014.03.067. . [2] B. Cathey, S. Cousineau, A. Aleksandrov, &amp; A. Zhukov, &quot;First Six Dimensional Phase Space Measurement of an Accelerator Beam,&quot; Phys. Rev. Lett. 121:064804 (2018). https://doi.org/10.1103/PhysRevLett.121.064804. . [3] A. G. Ruggiero, &quot;Are We Beating Liouville’s Theorem?,&quot; eConf C7803272:123 (1978). . [4] S. Cousineau, A. Rakhman, M. Kay, A. Aleksandrov, V. Danilov, T. Gorlov, Y. Liu, M. Plum, A. Shishlo, &amp; D. Johnson, &quot;First Demonstration of Laser-Assisted Charge Exchange for Microsecond Duration Beams,&quot; Phys. Rev. Lett. 118:074801 (2017). https://doi.org/10.1103/PhysRevLett.118.074801. .",
            "url": "https://austin-hoover.github.io/blog/accelerators/space%20charge/2021/05/27/painting_a_particle_beam.html",
            "relUrl": "/accelerators/space%20charge/2021/05/27/painting_a_particle_beam.html",
            "date": " • May 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Computation of the matched envelope of the Danilov distribution",
            "content": "My first peer-reviewed paper, titled Computation of the matched envelope of the Danilov distribution [1], was published in Physical Review Accelerators and Beams (PRAB) at the end of April. I thought I would summarize the results of the paper here. I probably ended up making this too long for anyone to be interested in reading all the way through (as usual), but oh well. The paper was on the topic of self-consistent beams; I&#39;ll start by defining this term. . Self-consistent beams . Definition and attractive properties . We define a beam to be self-consistent if it satisfies the following conditions: . It gives rise to linear space charge forces. | The above point remains true as long as all external forces are linear. | Let&#39;s discuss the meaning and relevance of these conditions. Space charge forces are the forces which arise between charged particles in a beam. Linear refers to the dependence of the magnitude of this force on the position in the beam. For example, consider the following two distributions of charged particles and the radial electric fields they produce. . import numpy as np import matplotlib.pyplot as plt import proplot as plot # Generate 2D uniform density distribution n = 10000 np.random.seed(6) rho = 2 * np.sqrt(np.random.uniform(size=n)) phi = np.random.uniform(0, 2*np.pi, size=n) X1 = np.vstack([rho * np.cos(phi), rho * np.sin(phi)]).T # Generate 2D Gaussian distribution X2 = np.random.normal(scale=1, size=(n, 2)) # Plot distributions fig, axes = plot.subplots(nrows=2, ncols=2, figsize=(5.4, 3.25), hspace=0, wspace=0.9, spany=False, spanx=False, aspect=1, height_ratios=[1, 0.3]) for ax, X in zip(axes[0, :], [X1, X2]): ax.scatter(X[:, 0], X[:, 1], s=0.05, c=&#39;pink9&#39;) for side in [&#39;top&#39;, &#39;bottom&#39;, &#39;left&#39;, &#39;right&#39;]: ax.spines[side].set_visible(False) axes[0, :].format(yticks=[], toplabels=[&#39;Uniform&#39;, &#39;Gaussian&#39;]) # Plot radial electric field def Efield(r, kind=&#39;gaussian&#39;): if r == 0: return 0 if kind == &#39;uniform&#39;: return r / 4 if abs(r) &lt;= 2 else 1 / r elif kind == &#39;gaussian&#39;: return (1 / r) * (1 - np.exp(-0.5 * r**2)) xvals = np.linspace(-4, 4, 100) axes[1, 0].plot(xvals, [Efield(x, &#39;uniform&#39;) for x in xvals], &#39;k-&#39;) axes[1, 1].plot(xvals, [Efield(x, &#39;gaussian&#39;) for x in xvals], &#39;k-&#39;); axes[1, :].format(ylabel=&#39;E field&#39;, yticks=[0], xlabel=r&#39;r / $ sigma$&#39;) for ax in axes[1, :]: for side in [&#39;top&#39;, &#39;right&#39;]: ax.spines[side].set_visible(False) . . The distribution on the left produces an electric field which is proportional to the radius, i.e., it gives rise to linear space charge forces (this isn&#39;t true outside $r = 2 sigma$, but no particles are in this region). On the other hand, the distribution on the right produces an electric field which depends nonlinearly on the radius, i.e., it gives rise to nonlinear space charge forces. . Nonlinear forces bring with them several negative effects. One of these is emittance growth. Emittance is a measure of the area occupied by the beam in phase space, so it measures both the spread in positions and the spread in velocities of the beam particles. Most of the time we are concerned with the horizontal emittance ($x$-$x&#39;$) and the vertical emittance ($y$-$y&#39;$). It&#39;s usually important to keep the emittance as small as possible; for instance, a smaller emittance leads to a larger luminosity in a collider. The key point here is that the emittance is only invariant if all forces are linear. Here is a simulation example showing emittance growth due to nonlinear space charge forces from [2]. . . Another effect of space charge in circular machines is called the tune shift. The single-particle tune is the number of oscillations a particle undergoes in phase space during one trip around the accelerator. Space charge decreases the tune of every particle, similar to how weakening a spring decreases the oscillation frequency of a mass attached to the spring. This is annoying because the tunes need to be precisely chosen to avoid resonances driven by nonlinear magnetic fields. A common strategy is to avoid any resonance lines up to a certain order defined by $M_x nu_x + M_y nu_y = N$, where $ nu_x$ and $ nu_y$ are the horizontal and vertical tunes, $M_x$, $M_y$, and $N$ are integers, and $|M_x| + |M_y|$ is the order of the resonance; if a particle lies on one of these lines in tune space, it increases the chances that it will be driven to large amplitudes and eventually lost. So there is some extra work to ensure that the tune doesn&#39;t cross any low-order resonance lines as it shifts. But nonlinear forces cause the magnitude of the tune shift to depend on the particle&#39;s position; the beam then spreads out in tune space, increasing the chance that some particles will cross resonance lines. This puts a fundamental limit on the beam intensity. Here is an old simulation example in the SNS from [3]. . . A third downside of nonlinear space charge forces is a loss of analytic tractability. Space charge introduces the self-consistency problem: the collective electric field causes each particle to move, which then changes the collective field, which then moves the particles, and so on. In general, there is no analytic solution to this problem and we must turn to expensive computer simulations. . At this point it&#39;s (hopefully) clear that a beam with linear space charge forces would alleviate several problems. In fact, we already showed an example of such a beam. But if we took this beam and transported it through an accelerator; would its uniform density be maintained? To answer this question, we would need to specify the initial distribution of particle velocities in addition to the initial positions. It turns out that there are only a few special cases, which we call self-consistent beams, for which the answer is yes. . KV distribution . The first self-consistent beam was derived by a pair of Russian scientists in 1959 and is known as the KV distribution. Particles in the KV distribution uniformly populate the boundary of an ellipsoid (or sphere in certain coordinates) in 4D phase space ($x$, $x&#39;$, $y$, $y&#39;$). I&#39;m not going into the details here, but it&#39;s simlar to the microcanonical distribution from statistical mechancis. Interestingly, any 2D projection of the distribution is a uniform density ellipse; in particular, the $x$-$y$ distribution is an upright ellipse. There are a few ways to generate a uniform density on a 4D sphere, but the nicest way I&#39;ve found is to first create a 4D Gaussian, then normalize all vectors to unit length. . X = np.random.normal(size=(100000, 4)) X = np.apply_along_axis(lambda x: x / np.linalg.norm(x), 1, X) # https://github.com/austin-hoover/accphys/blob/master/tools/plotting.py corner(X, figsize=5, units=None, text=&#39;KV ndistribution&#39;); . . The electric field within a uniform density, upright ellipse can (not easily) be shown to be . $$ mathbf{E}(x, y) propto frac{x}{c_x(c_x + c_y)} hat{x} + frac{y}{c_y(c_x + c_y)} hat{y}, tag{1}$$ . where $c_x$ and $c_y$ are the semi-axes of the ellipse (I&#39;ve left out a factor out front which contains the beam intensity). Notice that it&#39;s both linear and uncoupled, meaning that the $x$ component of the field is proportional to $x$. Using this field, a system of differential equations can be produced which evolve the beam envelope, the elliptical boundary containing the particles in the $x$-$y$ plane. The neat thing is that these equations close the feedback loop presented by the self-consistency problem; every change to the envelope is consistent with its own fields, even when the external fields are time-dependent. These envelope equations, as they&#39;re called, have been incredibly important for understanding space charge affects. In addition to providing a theoretical benchmark for computer simulations, the equations can capture the approximate behavior of more realistic beams in some cases (see the first figure in this post — the Gaussian has has an approximately uniform core). More details can be found in [4]. Let&#39;s try integrating these equations in a FODO lattice. . from scipy.integrate import odeint def fodo(s, quad_strength=0.556, cell_length=5.0): s = (s % cell_length) / cell_length delta = 0.125 if s &lt; delta or s &gt; 1 - delta: return +quad_strength elif 0.5 - delta &lt;= s &lt; 0.5 + delta: return -quad_strength return 0.0 def kv_derivs(params, s, Q, foc): cx, cxp, cy, cyp = params k0x = foc(s) k0y = -k0x w = np.zeros(4) w[0], w[2] = cxp, cyp w[1] = -k0x*cx + 2*Q/(cx+cy) + 16*epsx**2/(cx**3) w[3] = -k0y*cy + 2*Q/(cx+cy) + 16*epsy**2/(cy**3) return w def track_kv(params, positions, Q, foc): tracked = odeint(kv_derivs, params, positions, args=(Q, foc)) sizes = tracked[:, [0, 2]] return sizes * 1000 # convert to mm # Create KV envelope alphax, alphay, betax, betay = 0.0, 0.0, 8.017, 1.544 epsx = epsy = 10e-6 cx = 2 * np.sqrt(epsx * betax) cy = 2 * np.sqrt(epsy * betay) cxp = cyp = 0.0 params = [cx, cxp, cy, cyp] # Integrate envelope equations cell_length, periods, npts = 5.0, 4, 1000 positions = np.linspace(0, cell_length * periods, npts) Q = 1e-5 sizes = track_kv(params, positions, Q, fodo) sizes0 = track_kv(params, positions, 0.0, fodo) . . from matplotlib.patches import Ellipse fps = 12 stride = 10 umax, umin = np.max(sizes), np.min(sizes) umax_pad = 1.25 * umax fig, axes = plot.subplots(nrows=2, ncols=2, figsize=(7, 2.5), spany=False, aligny=True, sharey=False, sharex=False, hspace=0.2, height_ratios=[5, 1], width_ratios=[2.75, 1]) axes[0, 0].format(xlabel=&#39;&#39;, ylabel=&#39;Beam size [mm]&#39;, ylim=(umin - 5, umax + 5)) axes[1, 0].format(xlabel=&#39;s [m]&#39;, ylabel=r&#39;$k_x$&#39;, yticks=[0], ylim=(-0.6116, 0.6116)) axes[:, 0].format(xlim=positions[[0, -1]]) axes[1, 0].spines[&#39;top&#39;].set_visible(False) axes[0, 1].format(xticklabels=[], yticklabels=[], xlabel=&#39;x&#39;, ylabel=&#39;y&#39;, xlim=(-umax_pad, umax_pad), ylim=(-umax_pad, umax_pad)) for side in [&#39;top&#39;, &#39;right&#39;]: axes[0, 1].spines[side].set_visible(False) axes[1, 1].axis(&#39;off&#39;) axes[0, 0].format(xticklabels=[]) plt.close() line1, = axes[0, 0].plot([], []) line2, = axes[0, 0].plot([], []) axes[0, 0].format(cycle=&#39;colorblind&#39;) line3, = axes[0, 0].plot([], [], ls=&#39;--&#39;, lw=0.5) line4, = axes[0, 0].plot([], [], ls=&#39;--&#39;, lw=0.5) axes[1, 0].plot(positions, [fodo(s) for s in positions], color=&#39;k&#39;, lw=1) def update(i): i *= stride line1.set_data(positions[:i], sizes[:i, 0]) line2.set_data(positions[:i], sizes[:i, 1]) line3.set_data(positions[:i], sizes0[:i, 0]) line4.set_data(positions[:i], sizes0[:i, 1]) axes[0, 0].legend(labels=[&#39;x&#39;, &#39;y&#39;], ncols=1, loc=&#39;upper left&#39;, fontsize=&#39;small&#39;, handlelength=1.5) for patch in axes[0, 1].patches: patch.set_visible(False) axes[0, 1].add_patch(Ellipse((0, 0), 2*sizes[i, 0], 2*sizes[i, 1], fc=&#39;lightsteelblue&#39;, lw=0.75, ec=&#39;k&#39;)) axes[0, 1].add_patch(Ellipse((0, 0), 2*sizes0[i, 0], 2*sizes0[i, 1], fill=False, ls=&#39;--&#39;, color=&#39;k&#39;, lw=0.5, alpha=0.5)) animation.FuncAnimation(fig, update, frames=len(positions[::stride]), interval=1000/fps) . . &lt;/input&gt; Once Loop Reflect On the top left, I&#39;ve plotted the beam sizes with space charge (solid lines) and without space charge (dashed lines) as a function of the position $s$ in the lattice. The bottom plot shows the horizontal focusing strength; $k_x &gt; 0$ is a focusing quadrupole, $k_x &lt; 0$ is a defocusing quadrupole, and $k_x = 0$ is a drift. Notice, as I mentioned earlier, that the beam remains upright at all times. I should also mention that the horizontal and vertical emittances are invariants of the motion (more on that in a moment). . Danilov distribution . For a while, it was assumed that the KV distribution is the only self-consistent beam when the lattice focusing is time-dependent; recently, however, a larger class of self-consistent beams were derived. One of these is the so-called Danilov distribution, which was the focus of this work. This beam is similar to the KV distribution in that it has a uniform particle density, but the main difference is that it has a nonzero angular momentum. Each particle in the beam satisfies . $$ x&#39; = e_{11}x + e_{12}y, tag{2}$$ $$ y&#39; = e_{21}x + e_{22}y, $$ . where the $e_{ij}$ terms are constants. Suppose $e_{11} = e_{22} = 0$ and $e_{21} = -e_{12} = 0$; this gives $y&#39; = x$ and $x&#39; = -y$, which describes a rotating rigid disk. Here are the phase space projections of the beam. . X = np.random.normal(size=(100000, 4)) X = np.apply_along_axis(lambda x: x / np.linalg.norm(x), 1, X) X[:, 3] = +X[:, 0] X[:, 1] = -X[:, 2] corner(X, figsize=5, units=None, text=&#39;Danilov ndistribution&#39;); . . We can again derive equations for the elliptical beam envelope, but now they&#39;re going to include coupling between $x$ and $y$. This is because tilting the ellipse from Eq. (1) produces a term proportional to $xy$ for the electric field. More details can be found in [5]. . def get_tilt_angle(a, b, e, f): return -0.5 * np.arctan2(2*(a*e + b*f), a**2 + b**2 - e**2 - f**2) def get_radii(a, b, e, f): phi = get_tilt_angle(a, b, e, f) sin, cos = np.sin(phi), np.cos(phi) sin2, cos2 = sin**2, cos**2 xx = a**2 + b**2 yy = e**2 + f**2 xy = a*e + b*f cx = np.sqrt(abs(xx*cos2 + yy*sin2 - 2*xy*sin*cos)) cy = np.sqrt(abs(xx*sin2 + yy*cos2 + 2*xy*sin*cos)) return cx, cy def danilov_derivs(params, s, Q, foc): k0x = foc(s) k0y = -k0x a, b, ap, bp, e, f, ep, fp = params phi, (cx, cy) = get_tilt_angle(a, b, e, f), get_radii(a, b, e, f) cos, sin = np.cos(phi), np.sin(phi) cos2, sin2, sincos = cos**2, sin**2, sin * cos T = 2 * Q / (cx + cy) w = np.zeros(8) w[0], w[1], w[4], w[5] = ap, bp, ep, fp w[2] = -k0x*a + T*((a*cos2 - e*sincos)/cx + (a*sin2 + e*sincos)/cy) w[3] = -k0x*b + T*((e*sin2 - a*sincos)/cx + (e*cos2 + a*sincos)/cy) w[6] = -k0y*e + T*((b*cos2 - f*sincos)/cx + (b*sin2 + f*sincos)/cy) w[7] = -k0y*f + T*((f*sin2 - b*sincos)/cx + (f*cos2 + b*sincos)/cy) return w def track_danilov(params, positions, Q, foc): tracked = odeint(danilov_derivs, params, positions, args=(Q, foc)) a, b, ap, bp, e, f, ep, fp = tracked.T xsizes = np.sqrt(a**2 + b**2) ysizes = np.sqrt(e**2 + f**2) sizes = np.vstack([xsizes, ysizes]).T * 1000 cx, cy = get_radii(a, b, e, f) radii = np.vstack([cx, cy]).T * 1000 angles = np.degrees(get_tilt_angle(a, b, e, f)) return sizes, radii, angles # (Calculated starting params offline to save space) params = np.array([0.0179, 0.0, 0, 0.0022, 0, -0.0079, 0.0051, 0]) positions = np.linspace(0.0, 20.0, 1000) sizes, radii, angles = track_danilov(params, positions, Q, fodo) sizes0, radii0, angles0 = track_danilov(params, positions, 0.0, fodo) . . umax, umin = np.max(sizes), np.min(sizes) umax_pad = 1.25 * umax fig, axes = plot.subplots(nrows=2, ncols=2, figsize=(7, 2.5), spany=False, aligny=True, sharey=False, sharex=False, hspace=0.2, height_ratios=[5, 1], width_ratios=[2.75, 1]) axes[0, 0].format(xlabel=&#39;&#39;, ylabel=&#39;Beam size [mm]&#39;, ylim=(umin - 5, umax + 5)) axes[1, 0].format(xlabel=&#39;s [m]&#39;, ylabel=r&#39;$k_x$&#39;, yticks=[0], ylim=(-0.6116, 0.6116)) axes[:, 0].format(xlim=positions[[0, -1]]) axes[1, 0].spines[&#39;top&#39;].set_visible(False) axes[0, 1].format(xticklabels=[], yticklabels=[], xlabel=&#39;x&#39;, ylabel=&#39;y&#39;, xlim=(-umax_pad, umax_pad), ylim=(-umax_pad, umax_pad)) for side in [&#39;top&#39;, &#39;right&#39;]: axes[0, 1].spines[side].set_visible(False) axes[1, 1].axis(&#39;off&#39;) axes[0, 0].format(xticklabels=[]) plt.close() line1, = axes[0, 0].plot([], []) line2, = axes[0, 0].plot([], []) axes[0, 0].format(cycle=&#39;colorblind&#39;) line3, = axes[0, 0].plot([], [], ls=&#39;--&#39;, lw=0.5) line4, = axes[0, 0].plot([], [], ls=&#39;--&#39;, lw=0.5) axes[1, 0].plot(positions, [fodo(s) for s in positions], color=&#39;k&#39;, lw=1) def update(i): i *= stride line1.set_data(positions[:i], sizes[:i, 0]) line2.set_data(positions[:i], sizes[:i, 1]) line3.set_data(positions[:i], sizes0[:i, 0]) line4.set_data(positions[:i], sizes0[:i, 1]) axes[0, 0].legend(labels=[&#39;x&#39;, &#39;y&#39;], ncols=1, loc=&#39;upper left&#39;, fontsize=&#39;small&#39;, handlelength=1.5) for patch in axes[0, 1].patches: patch.set_visible(False) axes[0, 1].add_patch(Ellipse((0, 0), 2*radii[i, 0], 2*radii[i, 1], angles[i], fc=&#39;lightsteelblue&#39;, lw=0.75, ec=&#39;k&#39;)) axes[0, 1].add_patch(Ellipse((0, 0), 2*radii0[i, 0], 2*radii0[i, 1], angles0[i], fill=False, ls=&#39;--&#39;, color=&#39;k&#39;, lw=0.5, alpha=0.5)) animation.FuncAnimation(fig, update, frames=len(positions[::stride]), interval=1000/fps) . . &lt;/input&gt; Once Loop Reflect Notice that the beam tilts even without space charge. This has to do with the phase relationship between $x$ and $y$. When the beam is upright in the $x$-$y$ plane, $x$ and $y$ are 90 degrees out of phase (think of circular motion); on the other hand, a 0 or 180 degree phase difference would lead to a diagonal line in the $x$-$y$ plane and no correlations in $x$-$y&#39;$ or $y$-$x&#39;$. So, the beam is going to tilt any time the $x$ and $y$ phase are changed by unequal amounts. . What is a bit less obvious is how space charge space charge affects the beam when it tilts. Although these forces are still linear, the $x$ and $y$ emittances are no longer conserved. Below is an example of a turn-by-turn plot (1 turn = 1 period = 5 meters in the above example) in which there are two frequencies involved: a faster oscillation of the beam envelope, and a slower coupling oscilllation corresponding the emittance exchange. . . Finding the matched beam . I&#39;ll now move on to describing the problem that we addressed in the paper. Notice that the focusing in the above animations repeats itself after five meters; we call this the period length. A beam is matched to the lattice if its envelope repeats itself after one period length, or more precisely, its covariance matrix repeats itself: . $$ mathbf{ Sigma}(s + L) = Sigma(s) tag{3}$$ . for all $s$, where $s$ is the position in the lattice, $L$ is the period length, and $ Sigma$ is the covariance matrix given by . $$ mathbf{ Sigma} = begin{bmatrix} langle{x^2} rangle &amp; langle{xx&#39;} rangle &amp; langle{xy} rangle &amp; langle{xy&#39;} rangle langle{xx&#39;} rangle &amp; langle{x&#39;^2} rangle &amp; langle{yx&#39;} rangle &amp; langle{x&#39;y&#39;} rangle langle{xy} rangle &amp; langle{yx&#39;} rangle &amp; langle{y^2} rangle &amp; langle{yy&#39;} rangle langle{xy&#39;} rangle &amp; langle{x&#39;y&#39;} rangle &amp; langle{yy&#39;} rangle &amp; langle{y&#39;^2} rangle end{bmatrix} tag{4}$$ (assuming all means are zero). So the matched beam has not only the same shape and orientation in $x$-$y$ space, but also the same spread in velocities and correlations between the positions and velocites. Finding the matched beam amounts to choosing the correct initial $ mathbf{ Sigma}$ such that Eq. (3) is satisfied. As I&#39;ll explain in a moment, the task is trivial without space charge but difficult with space charge. . Motivation . There are a few reasons for computing this matched envelope. First, it&#39;s a sort of minimum energy solution, and this free energy can drive emittance growth in real beams. Second, it&#39;s the most radially compact solution for a given beam intensity, i.e., the maximum beam size is minimized. Multiple previous studies computed the matched envelope of the KV distribution as a first step in a stability analysis; the strategy is to check that perturbations around the matched solution are stable. There is also motivation related to an ongoing project to create an approximate Danilov distribution in the SNS, for which it would be nice to know the matched solution in the 250 meter long SNS storage ring. For all these reasons, we thought it was worthwhile to develop a method to compute the matched envelope of this beam. Our strategy was to do this in as simple a case as possible (FODO), but also to include some interesting effects such as splitting the lattice tunes or adding external coupling. . Challenges . There are two challenges to overcome. First, space charge causes the final beam to depend on the initial beam in a potentially complicated way which is unknown before tracking the beam; this is especially true for long lattices and large beam intensities. This just means we&#39;ll need to iterate to get the correct answer. The second challenge is knowing which beam parameters to vary. This is straightforward in the KV distribution since space charge doesn&#39;t couple $x$ and $y$, but it wasn&#39;t immediately clear how to do this when coupling is present. . Solution . Consider the equation of motion for a particle in a coupled lattice: . $$ x&#39;&#39; + k_{11}x + k_{13}y + k_{14}y&#39; = 0, $$ $$ y&#39;&#39; + k_{31}x + k_{33}y + k_{23}x&#39; = 0. $$ . Now, what does space charge do to these equations? For a tilted, uniform density ellipse, it simply modifies $k_{11}$, $k_{13}$, $k_{31}$, and $k_{33}$. Thus, we could replicate the effect of space charge by inserting a large number of linear defocusing elements into the lattice. We decided to call this new lattice the effective lattice. This is illustrated below. . . The figure on the left is just showing how space charge is like a (possibly tilted) defocusing quadrupole in both planes. Notice that space charge causes $k_{13}$ to be nonzero, which shows that the system is coupled. We can think of the matched beam as generating an effective lattice which is periodic. The question then becomes which parameters to use to describe a coupled lattice... if we can do that, then we just vary those parameters to find the solution. There is actually no universally agreed upon method to describe coupled motion in beam physics; we used the one I wrote about in a previous post. I&#39;ll briefly comment on the application to this problem. . The following figure shows the turn-by-turn trajectory of a single particle in a coupled lattice. . . The particle traces out a surface in 4D phase space consisting of points $ mathbf{x}_1$, $ mathbf{x}_2$, etc. We can create a matched beam by putting a particle at each of these points since the particles will just trade positions after a turn ($ mathbf{x}_i rightarrow mathbf{x}_{i+1}$). This can also be understood in terms of the eigenvectors of the one-turn transfer matrix; each particle can be written as a linear combination of these eigenvectors (shown in blue and red): . $$ mathbf{x} = sqrt{ varepsilon_1} mathbf{v}_1e^{-i psi_1} + sqrt{ varepsilon_2} mathbf{v}_2e^{-i psi_2}. $$ . The eigenvectors have amplitudes ($ varepsilon$) and phases ($ psi$), and a matched beam is formed by distributing particles along either or both of the eigenvectors with phases ranging uniformly between zero and $2 pi$. The ellipses traced out by the eigenvectors are described by the 4D Twiss parameters $ alpha_{1x}$, $ beta_{1x}$, etc. . A key insight was that particles in the Danilov distribution only depend on one eigenvector; in other words, all the particles oscillate at the same frequency. There are more details on this in the paper, but the short story is that this cuts the number of parameters to six. There is the beam size in each dimension relative to the emittance, the beam divergence in each dimension relative to the emittance, the phase difference between $x$ and $y$ (this could go from 0 for a flat line or 90 degrees for an upright ellipse), and the ratio between the $x$ and $y$ emittances. We can also choose which eigenvector to use, so there are actually two different solutions. We wrap up all these parameters into a vector $ mathbf{p}$. . We can frame this as an optimization problem in which we search for the $ mathbf{p}$ which minimizes the sum of the squared differences between the initial and final moments when tracking through one period. We utilized two different optimization methods. The first was SciPy&#39;s nonlinear least squares, which worked really well in most cases. The second strategy was to track the beam over a number of terms and compute the average of $ mathbf{p}$, then use that as the seed for the next round. This figure shows that method converging toward the solution after a few iterations. The relevant code is found here. . . Example application . We demonstrated the matching routine in a FODO lattice, just like the one in the earlier animations. We also tried some variations like making the horizontal focusing stronger than the vertical, tilting the quadrupoles, and adding a longitudinal magnetic field. In the paper I write an analysis of each case, but I guess that would be a bit boring to include here. I can just show what the matched beam looks like instead. I&#39;ll also only look at one case: the simple FODO lattice. Here are the 2D phase space projections as a function of position. . . The colors are for different beam intensities — the darkest curve is zero intensity and the brightest curve is the highest intensity. The main takeaway is that, although space charge seems to scale the size of the matched beam, the evolution looks quite similar. There are differences, however. Here are the x and y beam sizes, emittances, and the difference between the x and y phases ($ nu$). . . In the top two subplots, the dashed lines represent $y$ and the solid lines represent $x$. The difference that wasn&#39;t super evident in the animation is that the $x$ and $y$ emittances are oscillatory in the matched beam when space charge is turned on. The bottom plot sort of represents the roundness of the beam: $ nu$ = 0 or 180 degrees is a complete correlation between $x$ and $y$, while $ nu$ = 90 is no correlation. Space charge tends to pull these curves backtoward 90 degrees, which is apparent in the animation. Finally, I should note that there is another solution for the same lattice if we choose to use the other eigenvector; this solution rotates clockwise, but is otherwise the same. The scripts and analysis notebooks used to generate these plots, as well as the other plots in the paper, can be found on GitHub. . Conclusion . Hopefully this gives a flavor of what went into this paper. Of course I&#39;ve left out quite a bit, including a discussion of the relevance of the cases we tried. Although in the paper we argue that our findings could have an impact on future experiments (which is true), the paper can really stand on its own as a study of the envelope equations. . I&#39;m now working on a much more practical project — this is a relief because I can explain what I&#39;m doing to people outside the field without rambling for minutes on end. We want to create an approximate Danilov distribution in the SNS and measure its covariance matrix (Eq. (4)). I&#39;m specifically working on the measurement component. . References . [1] A. Hoover, N. J. Evans, &amp; J. A. Holmes, &quot;Computation of the matched envelope of the Danilov distribution,&quot; Phys. Rev. Accel. Beams 24:044201 (2021). https://doi.org/10.1103/PhysRevAccelBeams.24.044201. . [2] I. Hofmann &amp; O. Boine-Frankenheim, &quot;Parametric instabilities in 3D periodically focused beams with space charge,&quot; Phys. Rev. Accel. Beams 20:014202 (2017). https://doi.org/10.1103/PhysRevAccelBeams.20.014202. . [3] J. Galambos, S. Danilov, D. Jeon, J. Holmes, D. Olsen, J. Beebe-Wang, &amp; A. Luccio, &quot;ORBIT-a ring injection code with space charge,&quot; Proceedings of the 1999 Particle Accelerator Conference (Cat. No.99CH36366) (1999), pp. 3143–3145 vol.5. https://doi.org/10.1109/PAC.1999.792230. . [4] S. M. Lund &amp; B. Bukh, &quot;Stability properties of the transverse envelope equations describing intense ion beam transport,&quot; Phys. Rev. ST Accel. Beams 7:024801 (2004). https://doi.org/10.1103/PhysRevSTAB.7.024801. . [5] V. Danilov, S. Cousineau, S. Henderson, &amp; J. Holmes, &quot;Self-consistent time dependent two dimensional and three dimensional space charge distributions with linear force,&quot; Phys. Rev. ST Accel. Beams 6:094202 (2003). https://doi.org/10.1103/PhysRevSTAB.6.094202. .",
            "url": "https://austin-hoover.github.io/blog/accelerators/space%20charge/differential%20equations/publications/2021/05/13/matched_Danilov_dist.html",
            "relUrl": "/accelerators/space%20charge/differential%20equations/publications/2021/05/13/matched_Danilov_dist.html",
            "date": " • May 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Authorship identification",
            "content": "In this post, I&#39;ll summarize a paper by John Houvardas and Efstathios Stamatatos titled N-Gram Feature Selection for Authorship Identification [1]. The topic of the paper is authorship identification, that is, to identify the author of an unlabeled document given a list of possible authors and some sample of each author&#39;s writing. I&#39;ll first motivate the problem of authorship identification, then briefly introduce the relevant statistical methods, and finally summarize and implement the methods in the paper. My goal is to reproduce the authors&#39; results. . . Stylometry . Motivation: the Federalist Papers . The Federalist Papers are an important collection of 85 essays written by Hamilton, Madison, and Jay during 1787 and 1788. The essays were published under the alias &quot;Plubious&quot;, and although it became well-known that the three men were involved, the authorship of each individual paper was kept hidden for over a decade. This was actually in the interest of both Hamilton and Madison; both were politicians who had changed positions on a number of issues and didn&#39;t want their political opponents to use their own words against them. Days before his death, Hamilton allegedly wrote down who he believed to be the correct author of each essay, claiming over 60 for himself. Madison waited a number of years before publishing his own list, and in the end there were 12 essays claimed by both Madison and Hamilton. Many interesting details on the controversy can be found in [2]. . . Alexander Hamilton (left) and James Madison (right). Credit: Wikipedia. There are a few ways one might go about resolving this dispute. One approach is to analyze the actual content of the text. For example, perhaps an essay draws from a reference with which only Madison was intimately familiar, or maybe an essay is similar to Hamilton&#39;s previous work. This approach was used many times over the next 150 years, but perhaps the final word on the subject was by Adair, who in 1944 concluded that Madison likely wrote all 12 essays. An alternative approach is to analyze the style of the text. For example, maybe Madison used many more commas than Hamilton. The field of stylometry attempts to statistically quantify these stylistic differences. David Holmes writes the following about stylometry [3]: . At its heart lies an assumption that authors have an unconscious aspect to their style, an aspect which cannot consciously be manipulated but which possesses features which are quantifiable and which may be distinctive. . I think this a valid assumption. The question is which features best characterize the author&#39;s style and which methods are best to use in the analysis of these features. Let&#39;s go back in time a bit to see how stylometry has developed over the past 150 years. . History . The physicist Thomas Mendenhall is considered the first to statistically analyze large literary texts. He presented the following interesting idea in an 1887 paper titled The Characteristic Curves of Composition [4]: it is known that each chemical element emits light with a unique distribution of wavelengths when it is heated; perhaps each author has a unique distribution of word lengths in the texts they have written. It&#39;s a really cool idea, and I highly recommend reading his original paper. Mendenhall tallied word lengths by hand for various books, usually in batches of 1000 words or so. Here is Fig. 2 from his paper which shows the characteristic curves for a few excerpts of Oliver Twist. . . Distribution of word lengths in &quot;Oliver Twist&quot;. Each curve is for a different sample of 1000 words. From [4]. He showed that these curves are very interesting and that they do reveal similarities between different works by the same author. The use of these statistics for authorship identification was left for future work. . The next significant advance in the statistical analysis of text was made by Zipf in 1932. Zipf found an interesting relationship between an integer $k$ and the frequency $f(k)$ of the $k$th most frequent word. This is often called a rank-frequency relationship, where $k$ is the rank. The scaling law can be written as . $$ f(k) propto k^{-1}. tag{1} $$ . The idea expressed by this law is that short words are much more frequent than large words. Surprisingly, the law holds up very well, albeit not perfectly, for most texts. Why this is the case is still unknown; a comprehensive review of the current state of the law can be found in [5]. The law also shows up in other situations such as national GDP: . . National GDPs appear to be moving toward the prediction by Zipf&#39;s Law (red line). From [6]. The success of Zipf&#39;s Law was very encouraging and led to a flurry of new mathematical models. Stylometry reached a landmark case in the 1960&#39;s when researchers used the frequency distributions of short function words — words we don&#39;t think about too much like &quot;upon&quot; or &quot;therefore&quot; — to support Adair&#39;s conclusion that Madison wrote the 12 disputed Federalist Papers. At the end of the day, however, models created in the spirit of Zipf&#39;s Law are probably doomed to fail. The &quot;true&quot; underlying model must be very complex due to its dependence on human psychology. There are now many algorithms available which instead build predictive models directly from data, and these can be readily applied to the problem of authorship identification. Here we focus on the use of the Support Vector Machine (SVM). . Support Vector Machine (SVM) . I include here the basic idea behind the SVM approach. There are a huge number of resources which go into the details (such as [7]). I&#39;ll follow the Wikipedia page since it has a nice short summary. . Maximum margin hyperplane . Consider a linear binary classifier, i.e., a plane which splits the data into two classes. The equation for a plane in any number of dimensions is . $$ y( mathbf{x}) = mathbf{w}^T mathbf{x} + w_0 = 0 tag{2}. $$ . This plane is called the decision surface; points are assigned to class 1 if $y( mathbf{x}) &gt; 0$ or class 2 when $y( mathbf{x}) &lt; 0$. Suppose the data is linearly separable (able to be completely split in two) and that we&#39;ve found a plane which correctly splits the data. We could then scale the coordinates such that all points with $y( mathbf{x}) ge 1$ belong to class 1 and all points with $y( mathbf{x}) le -1$ belong to class 2. The separating plane then sits in the middle as in the following figure. . . Maximum margin separating plane. Credit: Wikipedia. Notice that the plane could be rotated while still correctly splitting the existing data; the SVM attempts to find the optimal plane by maximizing the orthogonal distance from the decision plane to the closest point. This is known as the margin, and it can be shown that it is inversely proportional to the magnitude of $ mathbf{w}$. Thus, the SVM tries to minimize $| mathbf{w}|^2$ subject to the constraint that all points are correctly categorized. New data is then assigned based on this optimal boundary. . Some datasets won&#39;t be linear separable, in which case we can add a penalty function in order to minimize the number of miscategorized points. So, for N samples we minimize . $$ frac{1}{2}| mathbf{w}|^2 + C sum_{i=1}^{N}{ max left[0, 1 - {t_i y( mathbf{x}_i)} right]} . tag{3}$$ . where $t_i$ is the true class of point $i$ ($ pm 1$) and $C$ is a positive constant. Correctly classified points don&#39;t contribute anything to the sum since $t_i y( mathbf{x}_i)$ will be greater than or equal to one. Let&#39;s try this on non-linearly separable data sampled from two Gaussian distributions in 2D space. The Python package scikit-learn has a user-friendly interface for the SVM implementation in LIBLINEAR which we use here. . import numpy as np from sklearn import svm import matplotlib.pyplot as plt import proplot as plot # Create two Gaussian distributions np.random.seed(0) n = 200 X = np.vstack([np.random.normal(size=(n, 2), loc=[0, 0], scale=2.0), np.random.normal(size=(n, 2), loc=[5, 5], scale=2.5)]) y = n * [1] + n * [-1] # Find SVM decision boundary clf = svm.LinearSVC(C=1) clf.fit(X, y) # Plot the data def despine(ax): ax.format(xticks=[], yticks=[]) for side in [&#39;left&#39;, &#39;right&#39;, &#39;top&#39;, &#39;bottom&#39;]: ax.spines[side].set_visible(False) def padded_ranges(X, pad=0.5): xmin, ymin = np.min(X, axis=0) - pad xmax, ymax = np.max(X, axis=0) + pad return (xmin, xmax), (ymin, ymax) def plot_dec_boundary(ax, clf, xlim=(-100, 100), i=0, **kws): w0 = clf.intercept_ if type(clf.intercept_) is float else clf.intercept_[i] (w1, w2) = clf.coef_[i] line_x = np.array(xlim) line_y = -(w1 / w2) * line_x - (w0 / w2) kws.setdefault(&#39;c&#39;, &#39;black&#39;) ax.plot(line_x, line_y, **kws) def plot_dec_regions(ax, clf, xlim, ylim, nsteps=500, **kws): (xmin, xmax), (ymin, ymax) = xlim, ylim xx, yy = np.meshgrid(np.linspace(xmin, xmax, nsteps), np.linspace(ymin, ymax, nsteps)) Z = np.c_[xx.ravel(), yy.ravel()] y_pred = clf.predict(Z) zz = y_pred.reshape(xx.shape) kws.setdefault(&#39;alpha&#39;, 0.05) kws.setdefault(&#39;zorder&#39;, 0) ax.contourf(xx, yy, zz, **kws) fig, ax = plot.subplots() despine(ax) ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y) xlim, ylim = padded_ranges(X, pad=0.5) plot_dec_boundary(ax, clf) plot_dec_regions(ax, clf, xlim, ylim) ax.format(xlim=xlim, ylim=ylim) ax.annotate(&#39;Decision nboundary&#39;, xy=(0.55, 0.02), xycoords=&#39;axes fraction&#39;); . . The points are colored by their true classes, and the background is shaded according to the SVM prediction at each point. It can be important to try at least a few different values of $C$, which determines the trade-off between correctly classifying all samples and maximizing the margin, and to observe the effect on the accuracy as well as the algorithm convergence. Parameters such as this one which change the algorithm behavior but aren&#39;t optimized by the algorithm itself are commonly known as hyperparameters. . Kernel trick . In some cases the linear model is going to be bad; a frequently used example is &quot;target&quot; dataset. . n = 400 r1 = np.sqrt(np.random.uniform(0.0, 0.2, size=(n,))) r2 = np.sqrt(np.random.uniform(0.5, 1.0, size=(n,))) t1 = np.random.uniform(0, 2*np.pi, size=(n,)) t2 = np.random.uniform(0, 2*np.pi, size=(n,)) X = np.vstack([np.vstack([r1*np.cos(t1), r1*np.sin(t1)]).T, np.vstack([r2*np.cos(t2), r2*np.sin(t2)]).T]) y = n * [1] + n * [-1] xlim, ylim = padded_ranges(X, pad=0.1) fig, ax = plot.subplots() despine(ax) ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y) ax.format(xlim=xlim, ylim=ylim) . . A line obviously won&#39;t work; ideally we would draw a circle around the inner cluster to split the data. The kernel trick can be used to alleviate this problem by performing a transformation to a higher dimensional space in which the data is linearly separable. For example, consider the transformation . $$ (x_1, x_2) rightarrow (x_1^2, x_2^2, sqrt{2} x_1 x_2) . tag{4}$$ . from plotly import graph_objects as go x1, x2 = X.T u = x1**2 v = np.sqrt(2) * x1 * x2 w = x2**2 fig = go.Figure(data=go.Scatter3d(x=u, y=v, z=w, mode=&#39;markers&#39;, marker=dict(color=y, size=3, opacity=0.5))) fig.update_scenes(xaxis_visible=False, yaxis_visible=False, zaxis_visible=False) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . It&#39;s clear from rotating this plot that the transformed data can be split with a 2D plane. This need not be the transformation used by the SVM — in fact, many transformations can be used — but it clearly demonstrates the idea. The linear boundary in the transformed space can then be transformed to a nonlinear boundary in the original space. One way to plot this boundary is to make a prediction on a grid of points, then make a contour plot (the boundary is shown in grey). . clf = svm.SVC(kernel=&#39;rbf&#39;) clf.fit(X, y) fig, ax = plot.subplots() despine(ax) ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y) ax.format(xlim=xlim, ylim=ylim) plot_dec_regions(ax, clf, xlim, ylim) . . There are still several advantages to the linear SVM. First, it is much faster to train, and second, the kernel trick may be unnecessary for high-dimensional data. As we&#39;ll see, text data can involve a large number of very high-dimensional samples, so we&#39;ll be sticking with linear kernels. . Multi-class . A binary classifier can also be used for multi-class problems. Here we use the one-versus-rest(OVR) approach. Suppose we had $N$ classes denoted by $c_1$, $c_2$ ... $c_N$. In the OVR approach we train $N$ different classifiers; the ith classifier $L_i$ tries to split the data into two parts: $c_i$ and not $c_i$. Then we observe a new point and ask each classifier $L_i$ how confident it is that the point belongs to $c_i$. The point is assigned to the class with the highest score. We can extend our previous example to three Gaussian distributions to get a sense of how the decision boundaries are formed. . # Create three Gaussian distributions np.random.seed(0) n = 200 X = np.vstack([np.random.normal(size=(n, 2), loc=[0, 0], scale=2.0), np.random.normal(size=(n, 2), loc=[5, 5], scale=2.5), np.random.normal(size=(n, 2), loc=[-6, 6], scale=2.5)]) y = n * [1] + n * [0] + n * [-1] # Find SVM decision boundary clf = svm.LinearSVC(C=1, multi_class=&#39;ovr&#39;, max_iter=10000) clf.fit(X, y) # Plot the data fig, ax = plot.subplots() despine(ax) ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y, cmap=plot.Colormap((&#39;pink9&#39;, &#39;grey&#39;, &#39;darkgreen&#39;))) # Plot decision boundary xlim, ylim = padded_ranges(X) for i in range(3): ls = [&#39;-&#39;, &#39;--&#39;, &#39;dotted&#39;][i] plot_dec_boundary(ax, clf, i=i, ls=ls) ax.format(xlim=xlim, ylim=ylim) ax.legend(labels=[&#39;class {} boundary&#39;.format(i) for i in range(1, 4)], ncols=1, loc=(1.1, 0.6)); plot_dec_regions(ax, clf, xlim, ylim, alpha=0.1) . . The same idea holds with more classes and dimensions. Notice that there are some regions which are claimed by multiple classifiers, so it&#39;s not a perfect method. . N-grams and feature selection methods . As I mentioned in the introduction, the paper I&#39;m following is called N-Gram Feature Selection for Authorship Identification. In short, the paper used n-gram frequencies (defined in a moment) as features in the classification task and developed a new method to select the most significant or &quot;dominant&quot; n-grams. This was tested on a collection of short news articles. Let&#39;s step through their method. . Data set description . The Reuters Corpus Volume 1 (RCV1) data set is a big collection of news articles labeled by topic. Around 100,000 of these have known authors, and there are around 2000 different authors. A specific topic was chosen, and only authors who wrote at least one article which fell under this topic were considered. From this subset of authors, the top 50 in terms of number of articles written were chosen. 100 articles from each author were selected — 5000 in total — and these were evenly split into a training and testing set. The resulting corpus is a good challenge for authorship identification because the genre is invariant across documents and because the authors write about similar topics. Hopefully this leaves the author&#39;s style as the primary distinguishing factor. The data set can be downloaded here. The files are organized like this: . . There are plenty of functions available to load the data and to extract features from it, but I&#39;ll do everything manually just for fun. To load the data, let&#39;s first create two lists of strings, texts_train and texts_test, corresponding to the 2500 training and testing documents. The class id and author name for each document are also stored. . from os import listdir from os.path import join def load_files(outer_path): texts, class_ids, class_names = [], [], [] for class_id, folder in enumerate(sorted(listdir(outer_path))): folder_path = join(outer_path, folder) for filename in listdir(folder_path): class_ids.append(class_id) class_names.append(folder) file = open(join(folder_path, filename), &#39;r&#39;) text = file.read().replace(&#39; &#39;, &#39;_&#39;) texts.append(text) file.close() return texts, class_ids, class_names texts_train, y_train, authors_train = load_files(&#39;reuters_data/train&#39;) texts_test, y_test, authors_test = load_files(&#39;reuters_data/test&#39;) . Author Name Author ID Training Text . 0 AaronPressman | 0 | A_group_of_leading_trademark_specialists_plans... | . 1 AaronPressman | 0 | Prospects_for_comprehensive_reform_of_U.S._ban... | . 2 AaronPressman | 0 | An_influential_economic_research_group_is_prep... | . 3 AaronPressman | 0 | The_Federal_Communications_Commission_proposed... | . 4 AaronPressman | 0 | An_international_task_force_charged_with_resol... | . ... ... | ... | ... | . 2495 WilliamKazer | 49 | China_could_list_more_railway_companies_and_is... | . 2496 WilliamKazer | 49 | The_choice_of_Singapore_for_the_listing_of_Chi... | . 2497 WilliamKazer | 49 | China_ushered_in_1997,_a_year_it_has_hailed_as... | . 2498 WilliamKazer | 49 | China_on_Tuesday_announced_a_ban_on_poultry_an... | . 2499 WilliamKazer | 49 | China&#39;s_leaders_have_agreed_on_a_need_to_stimu... | . 2500 rows × 3 columns . The following histogram shows the distribution of document lengths in the training set; it&#39;s expected that the short average document length will greatly increases the difficulty of the classification task relative to longer works such as books. . word_counts = [len(text) for text in texts_train] fig, ax = plot.subplots(figsize=(5, 1.5)) ax.hist(word_counts, bins=&#39;auto&#39;, color=&#39;k&#39;, density=True) ax.format(xlabel=&#39;Document length (characters)&#39;, ylabel=&#39;Num. docs&#39;, yticks=[], title=&#39;Distribution of document lengths in training set&#39;) ax.annotate(r&#39;mean = {:.0f}&#39;.format(np.mean(word_counts)), xy=(0.8, 0.5), xycoords=&#39;axes fraction&#39;) ax.annotate(&#39;std = {:.0f}&#39;.format(np.std(word_counts)), xy=(0.8, 0.3), xycoords=&#39;axes fraction&#39;); from scipy.stats import norm mean, std = norm.fit(word_counts) x = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 1000) y = norm.pdf(x, mean, std) ax.plot(x, y, c=&#39;red&#39;, alpha=0.4); ax.legend(labels=[&#39;Gaussian fit&#39;], frameon=False); . . N-grams . An obvious feature candidate is word frequency; a less obvious one is n-gram frequency. A character n-gram is a string of length n. For example, the 3-grams contained in red_bike! are red, ed_, d_b, _bi, bik, ike, ke!. These shorter strings may be useful because they capture different aspects of style such as the use of punctuation or certain prefixes/suffixes. They also remove any ambiguities in word extraction and work for all languages. In order to use these features in the SVM classifier, we need to create a feature matrix $X$ where $X_{ij}$ is the frequency of the jth n-gram in the ith document. Thus, each document is represented as a vector in $k$ dimensional space, where $k$ is the number of unique n-grams selected from the training documents. We&#39;ll also normalize each vector so that all points are mapped onto the surface of the $k$-dimensional unit sphere while preserving the angles between the vectors; this should help the SVM performance a bit. . from collections import Counter def get_ngrams(text, n): return [text[i - n : i] for i in range(n, len(text) + 1)] def get_ngrams_in_range(text, min_n, max_n): ngrams = [] for n in range(min_n, max_n + 1): ngrams.extend(get_ngrams(text, n)) return ngrams def sort_by_val(dictionary, max_items=None, reverse=True): n_items = len(dictionary) if max_items is None or max_items &gt; n_items: max_items = n_items sorted_key_val_list = sorted(dictionary.items(), key=lambda item: item[1], reverse=reverse) return {k: v for k, v in sorted_key_val_list[:max_items]} class NgramExtractor: def __init__(self, ngram_range=(3, 5)): self.vocab = {} self.set_ngram_range(ngram_range) def set_ngram_range(self, ngram_range): self.min_n, self.max_n = ngram_range def build_vocab(self, texts, max_features=None): self.vocab, index = {}, 0 for n in range(self.min_n, self.max_n + 1): ngrams = [] for text in texts: ngrams.extend(get_ngrams(text, n)) counts = sort_by_val(Counter(ngrams), max_features) for ngram, count in counts.items(): self.vocab[ngram] = (index, count) index += 1 def create_feature_matrix(self, texts, norm_rows=True): X = np.zeros((len(texts), len(self.vocab))) for text_index, text in enumerate(texts): ngrams = get_ngrams_in_range(text, self.min_n, self.max_n) for ngram, count in Counter(ngrams).items(): if ngram in self.vocab: term_index = self.vocab[ngram][0] X[text_index, term_index] = count if norm_rows: X = np.apply_along_axis(lambda row: row / np.linalg.norm(row), 1, X) return X . Now we need to decide which value(s) of n to use as features. Let&#39;s look at the distribution of n-grams in the training documents. . extractor = NgramExtractor(ngram_range=(1, 15)) extractor.build_vocab(texts_train) len_counts = Counter([len(ngram) for ngram in extractor.vocab.keys()]) fig, axes = plot.subplots(ncols=2, sharex=False) x, y = zip(*len_counts.items()) axes[0].barh(x, y, color=&#39;k&#39;) axes[1].barh(x, np.log10(y), color=&#39;k&#39;) axes.format(yticks=x, ytickminor=False, ylabel=&#39;n&#39;, suptitle=&#39;Distribution of character n-grams in training text&#39;) axes[0].format(xlabel=&#39;Counts&#39;, xformatter=&#39;sci&#39;) axes[1].format(xlabel=&#39;log$_{10}$(Counts)&#39;, xformatter=&#39;sci&#39;) for ax in axes: ax.grid(axis=&#39;x&#39;) . . The total number of n-grams with 1 $ le$n $ le$ 15 is about 31 million; training a classifier on data with this number of dimensions is probably infeasible, and even more so on a larger data set. Previous studies have apparently had success with fixing the value of n to be either 3, 4, or 5, so the authors chose to restrict their attention to these values. Their new idea was to use all n-grams in the range 3 $ le$n $ le$ 5. This leaves a few hundred thousand features. . The next section will discuss statistical methods to prune the features; for now, though, we&#39;ll implement the simple method of keeping the $k$ most frequent across all the training documents. As long as this doesn&#39;t affect the accuracy too much, we reap the benefits of a reduction in computational time and the ability to fix the feature space dimensionality for comparison of different feature types. To see why many low frequency terms may be unimportant, suppose one of the authors wrote a single article about sharks in the training set. The the term &quot;shark&quot; would have a small global frequency and be very useful in the training set since no other writers write about sharks, but it&#39;s probabably a good idea to discard it since its unlikely to appear in the testing set. We must be careful, however, because some low-frequency terms could be important. These are probably terms which an author uses rarely but consistently over time. Maybe they like to use &quot;incredible&quot; as an adjective; the global frequency of &quot;incred&quot; would be much less than, say, &quot;that_&quot;, but it&#39;s valuable because its frequency distribution will likely be the same in future writing. A quick test on our data set shows that $k$ = 15,000 is a good number. Let&#39;s try this out on the 15,000 most frequent 3-grams. . ngram_range = (3, 3) max_features = 15000 norm_rows = True extractor = NgramExtractor(ngram_range) extractor.build_vocab(texts_train, max_features) X_train = extractor.create_feature_matrix(texts_train, norm_rows) X_test = extractor.create_feature_matrix(texts_test, norm_rows) . Here are some of the values in X_train. The columns have been sorted by descending frequency from left to right. . _th he_ the _in ed_ _to ng_ ing to_ _of ... Agi gip L_I +12 n t t _VW VW_ ig. kw_ d-n . 0 0.129339 | 0.129339 | 0.086226 | 0.129339 | 0.086226 | 0.043113 | 0.129339 | 0.129339 | 0.043113 | 0.086226 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 0.278956 | 0.266276 | 0.272616 | 0.088759 | 0.088759 | 0.044379 | 0.145818 | 0.133138 | 0.044379 | 0.069739 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 0.308339 | 0.247880 | 0.266018 | 0.108825 | 0.102780 | 0.102780 | 0.120917 | 0.114871 | 0.102780 | 0.096734 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 0.244807 | 0.244807 | 0.231207 | 0.102003 | 0.108803 | 0.102003 | 0.061202 | 0.068002 | 0.108803 | 0.108803 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 0.231920 | 0.248486 | 0.207072 | 0.066263 | 0.149092 | 0.115960 | 0.066263 | 0.066263 | 0.074546 | 0.082829 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2495 0.148444 | 0.197926 | 0.141375 | 0.091894 | 0.077756 | 0.127238 | 0.247407 | 0.162582 | 0.106032 | 0.162582 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2496 0.264067 | 0.193649 | 0.211254 | 0.146704 | 0.158440 | 0.082154 | 0.146704 | 0.228858 | 0.082154 | 0.193649 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2497 0.346437 | 0.276067 | 0.276067 | 0.092022 | 0.151566 | 0.113675 | 0.276067 | 0.151566 | 0.102849 | 0.151566 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2498 0.286280 | 0.293262 | 0.251367 | 0.125684 | 0.181543 | 0.111719 | 0.104736 | 0.076807 | 0.104736 | 0.160596 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2499 0.334275 | 0.227915 | 0.205123 | 0.159540 | 0.151943 | 0.129152 | 0.091166 | 0.113957 | 0.121554 | 0.189929 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2500 rows × 15000 columns . We can now feed this array to the SVM and make predictions on the testing data. I&#39;ll keep the $C$ parameter fixed at $C = 1$ in all cases since this is what is done in the paper (I tried a few different values of $C$ and there wasn&#39;t a large effect on the accuracy). Here is the confusion matrix obtained after training and testing: . from sklearn.metrics import accuracy_score, confusion_matrix import seaborn as sns clf = svm.LinearSVC(C=1) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) acc = accuracy_score(y_test, y_pred) cmat = confusion_matrix(y_test, y_pred) fig, ax = plt.subplots(figsize=(4.5, 3.5)) sns.heatmap(cmat, ax=ax, cmap=&#39;binary&#39;, cbar_kws=dict(label=&#39;Number of documents&#39;)) ax.set_title(&#39;Confusion Matrix (accuracy = {:.3f})&#39;.format(acc)) ax.set_ylabel(&#39;True class&#39;); ax.set_xlabel(&#39;Predicted class&#39;); . . Feature selection . In the rest of this post, we&#39;ll study how to use statistical methods to further eliminate features from this initial set of 15,000. This process of selecting features which are &quot;best&quot; in a statistical sense is known as feature selection. . Information gain . A classical statistical measure of feature &quot;goodness&quot; is called information gain (IG). The idea is that knowing whether or not a term t is found in a document of a known class $c$ gives information about $c$, and that some terms will contribute more information than others. The information gain can be written as [8] . $$ IG(t) = p(t) sum_{i=1}^{m}p(c_i | t) log p(c_i | t) + p( bar{t}) sum_{i=1}^{m}p(c_i | bar{t}) log p(c_i | bar{t}) - sum_{i=1}^{m}p(c_i) log p(c_i). tag{5}$$ . The probability of choosing term $t$ out of all terms in the corpus is given by $p(t)$, and $p(t) + p( bar{t}) = 1$. Similarly, $p(c_i)$ is the probability that a randomly chosen document belongs to class $c_i$, and $p(c_i) + p( bar{c_i}) = 1$. The probability that a document belongs to $c_i$ given that it contains $t$ is $p(c_i | t)$, or $p(c_i | bar{t})$ if it doesn&#39;t contain $t$. The strategy is then to keep the terms with the highest information gain scores. . class InfoGainSelector: def __init__(self): self.idx = None def fit(self, X, y): # Compute probability distributions n_docs, n_terms = X.shape n_classes = len(np.unique(y)) P_c_and_t = np.zeros((n_classes, n_terms)) for doc_index, class_index in enumerate(y): P_c_and_t[class_index, :] += (X[doc_index, :] &gt; 0).astype(int) P_c_and_t /= np.sum(P_c_and_t) P_t = np.sum(P_c_and_t, axis=0) P_c = np.sum(P_c_and_t, axis=1) P_c_given_t = P_c_and_t / P_t P_c_given_tbar = 1 - (1 - P_c_and_t) / (1 - P_t) # Compute information gain for each feature def XlogX(X): return X * np.log2(X, out=np.zeros_like(X), where=(X &gt; 0)) scores = np.zeros(n_terms) scores += np.sum(P_t * XlogX(P_c_given_t), axis=0) scores += np.sum((1 - P_t) * XlogX(P_c_given_tbar), axis=0) scores -= np.sum(XlogX(P_c)) self.idx = np.argsort(scores) def select(self, X, k=-1): return X[:, self.idx[:k]] . We&#39;ll now compare 4 sets of 15,000 features: 3-grams, 4-grams, 5-grams, and equal parts 3/4/5-grams, each time using IG to select the best $k$ features and plotting the accuracy vs. $k$. I&#39;ll start from $k$ = 1 to 200. . extractor = NgramExtractor() selector = InfoGainSelector() clf = svm.LinearSVC(C=1) def compare_acc_ig(ngram_ranges, kmin, kmax, kstep): n_keep = np.arange(kmin, kmax + kstep, kstep).astype(int) accuracies = np.zeros((len(ngram_ranges), len(n_keep))) for i, ngram_range in enumerate(ngram_ranges): extractor.set_ngram_range(ngram_range) max_features = 5000 if ngram_range == (3, 5) else 15000 extractor.build_vocab(texts_train, max_features) X_train = extractor.create_feature_matrix(texts_train) X_test = extractor.create_feature_matrix(texts_test) selector.fit(X_train, y_train) for j, k in enumerate(n_keep): X_train_red = selector.select(X_train, k) X_test_red = selector.select(X_test, k) clf.fit(X_train_red, y_train) y_pred = clf.predict(X_test_red) accuracies[i, j] = accuracy_score(y_test, y_pred) return accuracies def plot_accs(accuracies, kmin, kmax, kstep): fig, ax = plot.subplots(figsize=(4, 3)) for i in range(4): m = [&#39;D&#39;, &#39;s&#39;, &#39;^&#39;, &#39;s&#39;][i] mfc = [None, None, &#39;w&#39;, &#39;w&#39;][i] ax.plot(np.arange(kmin, kmax + kstep, kstep), accuracies[i, :], marker=m, mew=1, mfc=mfc) ax.format(title=&#39;Information Gain feature selection&#39;) ax.format(xlabel=&#39;Number of features selected (k)&#39;) ax.format(ylabel=&#39;Accuracy&#39;) ax.legend(labels=[&#39;n = 3&#39;, &#39;n = 4&#39;, &#39;n = 5&#39;, &#39;n = (3, 4, 5)&#39;], ncols=1); return ax . . The accuracy at $k$ = 1 is 0.04, so using the feature with the highest IG score is actually twice as effective as random guessing! By the end of the plot 3-grams and variable length n-grams have taken a clear leaad, with 5-grams in last place. The performance gap between the different n-grams also appears to be growing with $k$. The next region we&#39;ll look at is $200 le k le 2000$. . Now the gap is decreasing as we approach an upper performance limit at higher $k$, especially for 3-grams. We&#39;ll now look at the region which is plotted in the paper: $2,000 le k le 10,000$. . One interesting thing is that 5-grams make a big jump from last place to first place. I&#39;m not sure if I have any deep insights into this behavior, but it&#39;s interesting that the best n-gram to choose depends on the number of features selected. Now, I should compare with Fig. 1 from the paper: . . The first difference is the maximum achieved accuracy which is a few percentage points higher. The second difference is that the authors found 3-grams to be worst at low $k$ and best at high $k$.and the opposite for 5-grams. I&#39;ll leave this as an open problem for now. . LocalMaxs algorithm . Let&#39;s look at the top IG scoring n-grams from from the variable-length feature set. . extractor.set_ngram_range((3, 5)) extractor.build_vocab(texts_train, max_features=5000) X_train = extractor.create_feature_matrix(texts_train) X_test = extractor.create_feature_matrix(texts_test) selector.fit(X_train, y_train) def get_term(i): for key, (idx, count) in extractor.vocab.items(): if idx == i: return key for rank, i in enumerate(selector.idx[:10], start=1): print(&#39;{:02}. {}&#39;.format(rank, get_term(i))) . . 01. _th 02. _the_ 03. the_ 04. _to 05. _the 06. _in 07. he_ 08. ed_ 09. the 10. on_ . Notice all the variants of the which were included. IG has no way of knowing that these are basically the same. This motivates the definition of something called &quot;glue&quot;. Consider the word bigram Amelia Earhart. These two words are very likely to be found next to each other and could probably be treated as a single multi-word unit; it is as if there is a glue holding the two words together. The amount of glue is probably higher than that between, say, window and Earhart. A technique has been developed to quantify this glue and extend its calculation to word n-grams instead of just word bi-grams [9]. The same idea can then be applied to character n-grams. . Let $g(C)$ be the glue of character n-gram $C = c_1 dots c_n$. Assuming we had a way to calculate the glue, how could this concept be used for feature selection? One solution is called the LocalMaxs algorithm. First define an antecedent $ant(C)$ as an (n-1)-gram which is contained in $C$, e.g., &quot;string&quot; $ rightarrow$ &quot;strin&quot; or &quot;tring&quot;. Then define a successor $succ(C)$ as an (n+1)-gram which contains $C$, e.g., &quot;string&quot; $ rightarrow$ &quot;strings&quot; or &quot;astring&quot;. C is selected as a feature if . $$ g(C) ge g(ant(C)) , , and , , g(C) &gt; g(succ(C)) tag{6}$$ . for all ant(C) and succ(C). Since we&#39;re dealing with 3 $ le$ n $ le$ 5, only the latter condition is checked if n = 3, and only the former condition is checked for n = 5. Eq. (6) says that the glue of a selected feature shouldn&#39;t increase by adding a character to or removing a character from the start or end of the n-gram, i.e., the glue is at a local maximum with respect to similar n-grams. Now that the selection criteria are established, we can move on to calculating the glue. Here there are several options, but the one used in the paper is called symmetrical conditional probability (SCP). If we have a bigram $C = c_1c_2$, then . $$ SCP(c_1c_2) = p(c_1|c_2) cdot p(c_2|c_1) = frac{p(c_1,c_2)^2}{p(c_1)p(c_2)}, tag{7}$$ . so SCP is a measure of how likely one character is given the other and vice versa. This formula can be applied to an n-gram $C = c_1 dots c_n$ by performing a pseudo bigram transformation, which means splitting the n-gram into two parts at a chosen dispersion point; for example, &quot;help&quot; could be split as &quot;h*elp&quot;, &quot;he*lp&quot;, or &quot;hel*p&quot;, where is the dispersion point. Splitting $C$ as $c_1 dots c_{n-1}$$c_n$ would give . $$ SCP((c_1 dots c_{n-1})c_n) = frac{p(c_1 dots c_n)^2}{p(c_1 dots c_{n-1})p(c_n)}. tag{8}$$ . Of course, the answer will depend on the dispersion point. We therefore introduce the FairSCP which averages over the possible dispersion points: . $$ FairSCP(c_1 dots c_n) = frac{p(c_1 dots c_n)^2}{ frac{1}{n-1} sum_{i=1}^{n-1} p(c_1 dots c_i)p(c_{i+1} dots c_n)}. tag{9}$$ . In summary, LocalMaxs loops through every n-gram in the vocabulary, computes the glue as $g(C) = FairSCP(C)$, and keeps the n-gram if Eq. (6) is satisfied. It differs from IG selection in that the features are not ranked, so the number of selected features is completely determined by the text. The method is implemented below. . import string def antecedents(ngram): return [ngram[:-1], ngram[1:]] def successors(ngram, characters=None): if characters is None: characters = string.printable successors = [] for character in characters: successors.append(character + ngram) successors.append(ngram + character) return successors . class LocalMaxsExtractor(NgramExtractor): def __init__(self, ngram_range=(3, 5)): super().__init__(ngram_range) self.counts_list = [] # ith element is dictionary of unique (i+1)-gram counts self.sum_counts_list = [] # ith element is the sum of `counts_list[i].values()` def build_vocab(self, texts, max_features=None): # Count all n-grams with n &lt;= self.max_n self.counts_list, self.sum_counts_list = [], [] candidate_ngrams = {} for n in range(1, self.max_n + 1): ngrams = [] for text in texts: ngrams.extend(get_ngrams(text, n)) counts = Counter(ngrams) self.counts_list.append(counts) self.sum_counts_list.append(sum(counts.values())) if self.min_n &lt;= n &lt;= self.max_n: candidate_ngrams.update(sort_by_val(counts, max_features)) self.available_characters = self.counts_list[0].keys() # Select candidate n-grams whose glue is at local maximum self.vocab, index = {}, 0 for ngram, count in candidate_ngrams.items(): if self.is_local_max(ngram): self.vocab[ngram] = (index, count) index += 1 def is_local_max(self, ngram): glue, n = self.glue(ngram), len(ngram) if n &lt; self.max_n: for succ in successors(ngram, self.available_characters): if self.glue(succ) &gt;= glue: return False if n &gt; self.min_n: for ant in antecedents(ngram): if self.glue(ant) &gt; glue: return False return True def glue(self, ngram): n = len(ngram) P = self.counts_list[n-1].get(ngram, 0) / self.sum_counts_list[n-1] if P == 0: return 0.0 Avp = 0.0 for disp_point in range(1, n): ngram_l, ngram_r = ngram[:disp_point], ngram[disp_point:] n_l, n_r = disp_point, n - disp_point P_l = self.counts_list[n_l-1].get(ngram_l, 0) / self.sum_counts_list[n_l-1] P_r = self.counts_list[n_r-1].get(ngram_r, 0) / self.sum_counts_list[n_r-1] Avp += P_l * P_r Avp /= (n - 1) return P**2 / Avp . The first thing we should do is check the the glue of the derivative n-grams the, _the, etc. . extractor = LocalMaxsExtractor(ngram_range=(3, 5)) extractor.build_vocab(texts_train) for ngram in [&#39;the&#39;, &#39;_the&#39;, &#39;the_&#39;, &#39;_the_&#39;]: glue = extractor.glue(ngram) selected = extractor.is_local_max(ngram) freq = extractor.counts_list[len(ngram) - 1][ngram] print(&#39;{:&lt;5}: glue = {:.4f}, selected = {}&#39;.format(ngram, glue, selected)) . the : glue = 0.0856, selected = True _the : glue = 0.0846, selected = False the_ : glue = 0.0748, selected = False _the_: glue = 0.0808, selected = False . It seems to be working correctly. Now we&#39;d like to compare the performance to IG. There&#39;s no way to directly compare since LocalMaxs doesn&#39;t rank features; however, it&#39;s possible to vary the size of the initial set of features from which LocalMaxs makes its selections. Below, this initial size is varied from 3,000 to 24,000 using equal parts 3/4/5 grams as features. . lm_extractor = LocalMaxsExtractor(ngram_range=(3, 5)) clf = svm.LinearSVC(C=1) max_features_list = np.arange(2000, 8000, 1000).astype(int) lm_accuracies, lm_vocabs = [], [] for max_features in max_features_list: lm_extractor.build_vocab(texts_train, max_features) X_train_red = lm_extractor.create_feature_matrix(texts_train) X_test_red = lm_extractor.create_feature_matrix(texts_test) clf.fit(X_train_red, y_train) y_pred = clf.predict(X_test_red) lm_accuracies.append(accuracy_score(y_test, y_pred)) lm_vocabs.append(lm_extractor.vocab) n_keep = [len(vocab) for vocab in lm_vocabs] ig_extractor = NgramExtractor(ngram_range=(3, 5)) ig_extractor.build_vocab(texts_train, max_features=5000) X_train = ig_extractor.create_feature_matrix(texts_train) X_test = ig_extractor.create_feature_matrix(texts_test) ig_selector = InfoGainSelector() ig_selector.fit(X_train, y_train) ig_accuracies, ig_vocabs = [], [] for lm_vocab in lm_vocabs: k = len(lm_vocab) X_train_red = ig_selector.select(X_train, k) X_test_red = ig_selector.select(X_test, k) clf.fit(X_train_red, y_train) y_pred = clf.predict(X_test_red) ig_accuracies.append(accuracy_score(y_test, y_pred)) ig_vocabs.append(extractor.vocab) fig, ax = plot.subplots(figsize=(4, 3)) ax.plot(n_keep, lm_accuracies, c=&#39;k&#39;, marker=&#39;D&#39;) ax.plot(n_keep, ig_accuracies, marker=&#39;D&#39;, c=&#39;red8&#39;) ax.format(xlabel=&#39;Number of features selected&#39;, ylabel=&#39;Accuracy&#39;, title=&#39;IG vs. LocalMaxs feature selection&#39;) ax.legend(labels=[&#39;n = (3, 4, 5) — LocalMaxs&#39;, &#39;n = (3, 4, 5) — IG&#39;], ncols=1); . . As you can see, LocalMaxs achieves a higher accuracy with the same number of features. The neat thing is that the vocabularies are totally different; for example, at the last data point, only about 15% of the n-grams are found in both sets! Let&#39;s count the number of related n-grams in the two sets, where x is related to y if x is an antecedent or successor of y. . def count_related(ngrams): count = 0 for n1 in ngrams: for n2 in ngrams: if n1 != n2 and n1 in n2: count += 1 break return count lm_ngrams = list(lm_vocabs[-1]) vocab_size = len(lm_ngrams) ig_vocab = list(ig_extractor.vocab) ig_ngrams = [ig_vocab[i] for i in ig_selector.idx[:vocab_size]] shared = len([ig_ngram for ig_ngram in ig_ngrams if ig_ngram in lm_ngrams]) print(&#39;Vocab size: {}&#39;.format(vocab_size)) print(&#39;n-grams selected by both IG and LM: {}&#39;.format(shared)) print(&#39;IG related n-grams: {}&#39;.format(count_related(ig_ngrams))) print(&#39;LM related n-grams: {}&#39;.format(count_related(lm_ngrams))) . . Vocab size: 2706 n-grams selected by both IG and LM: 426 IG related n-grams: 1413 LM related n-grams: 178 . As mentioned earlier, IG selects many related terms such as the and the_. The LocalMaxs vocabulary is much &quot;richer&quot;, as the authors put it. Here is the corresponding figure from the paper (ignore the white squares): . . For some reason, their implementation extracted way more features than mine did. I don&#39;t have access to the author&#39;s code, and I couldn&#39;t find any implementation of LocalMaxs online, so it&#39;s hard for me to say what&#39;s happening. I&#39;m happy with my implementation since it exhibits the expected behavior (less related terms, better performance at lower feature numbers). . Conclusion . This post summarized a research paper in the field of Natural Language Processing (NLP) which focused on feature selection techniques. I didn&#39;t exactly reproduce the authors&#39; results, so if anyone reads this (unlikely) and finds a mistake, I would love to know about it. . In a future post I may apply these methods to my own data set; I&#39;m particularly interested in what would happen with Chinese characters. There are, of course, a ton of different techniques and experiments to explore which involve NLP. A different problem I&#39;d like to examine is that of artist identification; the problem would be to match a collection of paintings with their painters. The Web Gallery of Art is a potential database that I found after a quick search, and I&#39;m sure there are others. This would give me the chance to learn about image classification techniques. . References . [1] J. Houvardas &amp; E. Stamatatos, &quot;N-Gram Feature Selection for Authorship Identification,&quot; In J. Euzenat, &amp; J. Domingue,eds., Artificial Intelligence: Methodology, Systems, and Applications (Berlin, Heidelberg: Springer Berlin Heidelberg, 2006), pp. 77–86. . [2] A. Douglass, &quot;The Authorship of the Disputed Federalist Papers,&quot; The William and Mary Quarterly 1:97–122 (1944). . [3] D. Holmes, &quot;The Evolution of Stylometry in Humanities Scholarship,&quot; Literary and Linguistic Computing 13:111–117 (1998). . [4] T. C. Mendenhall, &quot;THE CHARACTERISTIC CURVES OF COMPOSITION,&quot; Science ns-9:237–246 (1887). https://doi.org/10.1126/science.ns-9.214S.237. . [5] S. T. Piantadosi, &quot;Zipf’s word frequency law in natural language: A critical review and future directions,&quot; Psychon Bull Rev 21:1112–1130 (2014). https://doi.org/https://doi.org/10.3758/s13423-014-0585-6. . [6] M. Cristelli, M. Batty, &amp; L. Pietronero, &quot;There is More than a Power Law in Zipf,&quot; Sci Rep 2 (2012). https://doi.org/https://doi.org/10.1038/srep00812. . [7] C. M. Bishop, Pattern Recognition and Machine Learning (Springer, 2006). . [8] Y. Yang &amp; J. O. Pedersen, &quot;A Comparative Study on Feature Selection in Text Categorization,&quot; ICML (1997). . [9] J. Silva, &quot;A Local Maxima method and a Fair Dispersion Normalization for extracting multi-word units from corpora,&quot; (2009). .",
            "url": "https://austin-hoover.github.io/blog/stylometry/machine%20learning/natural%20language%20processing/2021/04/29/authorship_identification.html",
            "relUrl": "/stylometry/machine%20learning/natural%20language%20processing/2021/04/29/authorship_identification.html",
            "date": " • Apr 29, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Nonlinear resonances",
            "content": "Most of us are familiar with the experience of pushing someone else on a playground swing. We intuitively know that we should sync our pushes with the swing oscillation frequency, which appears to be independent of the swing amplitude. This strategy employs the idea of a resonance, which is an increase of the oscillation amplitude of a system for certain driving frequencies. In this post we first review the mathematics of this simple example, then extend the machinery to the nonlinear dynamics in a particle accelerator. My goal here is to write down the main results which are relevant to accelerators in order to improve my own understanding of the topic. . . Linear resonances . Consider a mass on a spring which, if left alone, oscillates at freqency $ omega_0^2$. . . The equation of motion for $x$ is . $$ frac{d^2}{dt^2}{x} + omega_0^2x = 0. tag{1}$$ . Now consider a sinusoidal driving force $f(t) = f_0 cos( omega t)$ as well as a damping term: . $$ frac{d^2}{dt^2}{x} + b dot{x} + omega_0^2x = f_0 cos( omega t).$$ . After doing some work it can be shown that the gravitational, damping, and driving forces initially fight against each other, but in the end the driving force dominates and the position oscillates as . $$ x(t) = A cos( omega t - delta) tag{2}$$ . where . $$A^2 = frac{f_0^2}{( omega - omega_0)^2 + b omega^2}. tag{3}$$ . The figure below shows the squared amplitude as the driving frequency is varied. The maximum amplitude approaches infinity as the damping term goes to zero. . The next step is to consider what happens when the driving force is not a pure sine wave. We&#39;ll only consider periodic driving forces, and any periodic function can be written as a sum of sines and cosines of different frequencies. Assuming $f(t)$ is an even function so that we can drop the sine terms in the Fourier expansion, the equation of motion becomes . $$ ddot{x} + b dot{x} + omega_0^2 x = sum_{n=0}^{ infty} {f_n cos(n omega t)}. tag{4}$$ . The long-term solution is found by just adding up the solutions to each term in the sum: . $$x(t) = sum_{n = 0}^{ infty}{A_n cos{(n omega t - delta_n)}}, tag{5}$$ . where $A_n$ is given by Eq. (3) for the frequency $n omega$. The resonance condition will apply to each of these amplitudes individually, which means that a resonance could be excited if any component of the driving force is near the natural frequency. . Sources of nonlinearity . We&#39;re now going to apply these ideas to a particle accelerator. We&#39;ll assume small transverse oscillations, no acceleration, no deviation from the design momentum, and no particle-particle interactions. Under these assumptions, the transverse equation of motion of a particle with charge $q$ and momentum $p$ in a magnetic field $ mathbf{B} = (B_x, B_y)^T$ is . $$ x&#39;&#39; = - frac{q}{p} B_y(s), tag{6}$$ $$ y&#39;&#39; = + frac{q}{p} B_x(s). $$ . Remember that $x&#39; = dx/ds$, and $s$ is the position in accelerator (from now on we&#39;ll assume a circular accelerator or &quot;ring&quot; of circumference $L$). Any 2D magnetic field can be expanded as the following infinite sum: . $$B_y - iB_x = sum_{n=1}^{ infty} left({b_n - ia_n} right) left( frac{x + iy}{r_0} right)^{n-1}, tag{7}$$ . where $r_0$ is a constant. The $b_n$ and $a_n$ terms are called the multipole coefficients and skew multipole coefficients, respectively. The $n^{th}$ term in the expansion is the field produced by $2n$ symmetrically arranged magnetic poles. . . We can see that terms with $n &gt; 2$ introduce nonlinear powers of $x$ and $y$ on the right side of Eq. (6), while terms with $n le 2$ introduce linear or constant terms. One may ask why we are considering a general magnetic field when in reality we use only dipoles and quadrupoles. The answer is two-fold. First, the best we can do in a real magnet is to make the $n &gt; 2$ terms as small as possible; they aren&#39;t zero and we need to know how they affect the motion. Second, sextupoles (and sometimes even octopoles) can be introduced intentionally. Their primary use is to correct for the fact that not all beam particles have the same momentum. . . An example of a sextupole electromagnet. Credit: CERN. Perturbation analysis . The nonlinear terms in Eq. (6) eliminate any hope of an analytic solution. There are two options in situations such as these: 1) use a computer, or 2) use perturbation theory. The strategy of option 2 is to make approximations until an exact solution can be found, then to add in small nonlinear terms and see how the solution changes. The process can be repeated to solve the problem up to a certain order of accuracy. Usually this is infeasible beyond a few iterations, but it is a helpful tool for gaining intuition and interpreting numerical results. In particular, we&#39;ll be looking for regions where the particle may encounter a resonance. Without many details, let&#39;s try out the perturbation approach. Later on we&#39;ll use a computer and see if our analysis was accurate. . Floquet coordinates . The first step is to find an exact solution under some approximation. We&#39;ll neglect coupling by setting $y = 0$ and focus on one dimension to make things easier. Let&#39;s denote the linear focusing from the lattice by $k$, with all other terms in the field expansion folded into $ Delta B$ (there are still $n = 1$ and $n = 2$ terms in $ Delta B$, but they represent deviations from the design values). We&#39;re also assuming that these variables are normalized by the ratio $q / p$. This results in the equation of motion . $$ x&#39;&#39; + k(s)x = Delta B. tag{8}$$ . This is Hill&#39;s equation with a nonlinear driving term. The stable solution when $ Delta B = 0$ is . $$x(s) = sqrt{ epsilon beta(s)} cos left({ mu(s) + delta} right), tag{9}$$ . with the phase advance is given by . $$ mu(s) = int_{0}^{s}{ frac{ds}{ beta(s)}}. tag{10}$$ . These pseudo-harmonic oscillations are still a bit difficult to visualize, so it&#39;s helpful to perform the Floquet transformation which scales the $x$ coordinate as . $$x(s) rightarrow u(s) = frac{x(s)}{ sqrt{ beta_x(s)}}. tag{11}$$ . Furthermore, it is convenient to replace the $s$ coordinate with . $$ phi(s) = frac{1}{ nu_0} int_{0}^{C}{ frac{ds}{ beta_x(s)}}. tag{12}$$ . Here $ nu_0$ is the number of phase space oscillations per trip around the ring. As a result, the unperturbed equation of motion becomes (with $ dot{x} = dx / d phi$) . $$ ddot{u} + nu_0^2 u = 0. tag{13}$$ . But this is just a harmonic oscillator! The trajectory in phase space is a circle, and the particle revolves once around this circle for every turn around the ring. Finally, we can write $ Delta B$ as a power series in $u$ and derive the equation of motion in Floquet coordinates: . $$ ddot{u} + nu_0^2 u = - nu_0^2 beta^{3/2} Delta B = - nu_0^2 sum_{n=0}^{ infty} left({ beta^{ frac{n + 3}{2}} b_{n+1}} right) u^n. tag{14}$$ . Fourier expansion . The tools to analyze driven harmonic oscillators are now available to us. Similar to Eq. (4), each term on the right hand side can be Fourier expanded, the reason being that $ beta$ (the oscillation amplitude of the unperturbed motion) and $b_n$ (a multipole coefficient) depend only on the position in the ring, so of course they are periodic in $ phi$. Grouping these terms together and performing the expansion gives . $$ ddot{u} + nu_0^2 u = - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} C_{n,k} , u^n , e^{ik phi}. tag{16}$$ . We&#39;re now going to linearize this equation. This means plugging in $u = u_0 + delta u$, where $u_0$ is the unperturbed solution and $ delta_u$ is small, and discarding all higher powers of $ delta_u$. This gives . $$ ddot{ delta u} + nu_0^2 delta u approx - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} C_{n,k} , u_0^n , e^{ik phi} . tag{17}$$ . This equation tells us how the perturbation evolves with time — ideally it remains finite, but at a resonant condition it will grow without bound. The final step is to write $u_0^n$ in a managable form. There is this trick involving the binomial expansion: . $$ u_0^n propto cos^n( nu phi) = frac{1}{2^n} sum_{m=0}^{n} binom{n}{m} e^{i(n-2m) nu_0 phi}. tag{17}$$ . So, we finally arrive at . $$ ddot{ delta u} + nu_0^2 delta u approx - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} sum_{m=0}^{n} {n choose m} frac{C_{n,k}}{2^n} e^{i left[(n - 2m) nu_0 + k right] phi}. tag{18} $$ . There are a lot of indices floating around; $n$ is one less than the multipole coefficient of the magnetic field, $k$ is for the Fourier expansion, and $m$ is just a dummy index we used to binomially expand $u_0^2$. . Resonance diagram . Eq. (18) describes a driven harmonic oscillator like Eq. (5), so we can expect a resonance condition to occur when any of the frequency components of the driving force are close to the natural frequency $ nu_0$. In other words, a resonance could occur when . $$ (n - 2m) nu_0 + k = pm nu_0. tag{19}$$ . If you write out the different cases ($n$ = 0, 1, 2, ...), you&#39;ll find that dipole terms ($n = 0$) forbid integer tunes, quadrupole terms forbid 1/2 integer tunes, sextupole terms forbid 1/3 integer tunes, and so on. The same thing can be done for the vertical dimension. Once coupling is included between $x$ and $y$, we&#39;re lead to the definition of resonance lines: . $$ M_x nu_x + M_y nu_y = N, tag{20}$$ . where $M_x$, $M_y$, and $N$ are integers and $|M_x| + |M_y|$ is the order of the resonance. The reason for calling these resonance lines is because they define lines in $ nu_x$-$ nu_y$ space (tune space). You can click through the following animation to see how the lines fill up the space as higher order resonances are included. . def plot_resonance_lines(ax, max_order, c=&#39;k&#39;): for N in range(-max_order, max_order + 1): for Mx in range(-max_order, max_order + 1): for My in range(-max_order, max_order + 1): order = abs(Mx) + abs(My) if order &gt; 1: factor = (1 - (order - 2)/5) lw = 1.0 * factor lw = 0.4 if lw &lt; 0 else lw alpha = 1.0 * factor alpha = 0.25 if alpha &lt; 0 else alpha if order &lt;= max_order: if My == 0: if Mx != 0: ax.axvline(N / Mx, c=c, alpha=alpha, lw=lw) else: ax.plot([0, 1], [N / My, (N - Mx) / My], c=c, alpha=alpha, lw=lw) fig, ax = plot.subplots(figsize=(3, 3)) ax.format(xlim=(0, 1), ylim=(0, 1), xlabel=r&#39;$ nu_x$&#39;, ylabel=r&#39;$ nu_y$&#39;, xlabel_kw={&#39;fontsize&#39;:9}, ylabel_kw={&#39;fontsize&#39;:9}, aspect=0.95) ax.tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=7.5) plt.close() def update(max_order): plot_resonance_lines(ax, max_order, c=&#39;k&#39;) ax.set_title(&#39;Max order = {}&#39;.format(max_order), fontsize=8) animation.FuncAnimation(fig, update, frames=11, interval=1000) . . &lt;/input&gt; Once Loop Reflect Resonance strengths tend to decrease with order number, so people generally don&#39;t consider anything beyond order 3 or 4. That being said, the machine tunes $ nu_x$ and $ nu_y$ need to be carefully chosen to avoid all low order resonance lines. Ideally all beam particles occupy this single point in tune space, but space charge complicates things by decreasing the tune by different amounts for each particle, possible placing them on one of the above resonance lines. This effect, called tune spread, places a fundamental limit on the number of particles in the beam. . Numerical exploration of the sextupole . Let&#39;s explore the behavior of a beam under the influence of a sextupole magnet. This section recreates some figures from the book Accelerator Physics by S. Y. Lee. The easiest way to do this is to approximate the multipole as an instantaneous change to the slope of the particle&#39;s trajectory. This is valid if the magnet isn&#39;t too long. . import numpy as np class Multipole: &quot;&quot;&quot;Class to apply multipole kick to particle. Adapted from PyORBIT tracking routine in `py-orbit/src/teapotbase.cc`. Attributes - order : int The order of the multipole term (dipole: 1, quadrupole: 2, ...). strength : float Integrated multipole strength [m^-(order - 1)]. skew : bool If True, rotate the magnet 45 degrees. &quot;&quot;&quot; def __init__(self, order, strength, skew=False): self.order, self.strength, self.skew = order, strength, skew def track_part(self, vec): &quot;&quot;&quot;Apply transverse kick to particle slopes. vec : ndarray, shape (4,) Transverse phase space coordinate vector [x, x&#39;, y, y&#39;]. &quot;&quot;&quot; x, xp, y, yp = vec k = self.strength / np.math.factorial(self.order - 1) zn = (x + 1j*y)**(self.order- 1) if self.skew: vec[1] += k * zn.imag vec[3] += k * zn.real else: vec[1] -= k * zn.real vec[3] += k * zn.imag return vec . order = 3 strength = 0.5 multipole = Multipole(order, strength, skew=False) . The situation we&#39;ll consider is a circular lattice which is made of linear uncoupled elements + one thin sextupole. We&#39;ll observe the beam at the location of the sextupole after each turn. A key result of the linear theory is that the details of the rest of the lattice are unimportant for this task. All we need to do is choose the Twiss parameters and tune in each dimension to form the transfer matrix, then we can just track using matrix multiplication. Recall that the transfer matrix is written as $ mathbf{M} = mathbf{V P V^{-1}}$, where $ mathbf{V} = mathbf{V}( alpha_x, alpha_y, beta_x, beta_y)$ performs the Floquet normalization and $ mathbf{P} = mathbf{P}( nu_x, nu_y)$ is a rotation in the $x$-$x&#39;$ and $y$-$y&#39;$ phase spaces by the angle $2 pi nu_x$ and $2 pi nu_y$, respectively. The following class implements this representation of the lattice. . def V_2D(alpha, beta): &quot;&quot;&quot;Floquet normalization matrix in 2D phase space.&quot;&quot;&quot; return np.array([[beta, 0.0], [-alpha, 1.0]]) / np.sqrt(beta) def P_2D(tune): &quot;&quot;&quot;Phase advance matrixmin 2D phase space.&quot;&quot;&quot; phase_advance = 2 * np.pi * tune cos, sin = np.cos(phase_advance), np.sin(phase_advance) return np.array([[cos, sin], [-sin, cos]]) class Lattice: &quot;&quot;&quot;Represents lattice as linear one-turn transfer matrix + multipole kick. Attributes - M : ndarray, shape (4, 4) Linear one-turn transfer matrix. aperture : float Radius of cylindical boundary containing the particles [m]. multipole : Multipole object Must implement `track_part(vec)`, where vec = [x, xp, y, yp]. &quot;&quot;&quot; def __init__(self, alpha_x, alpha_y, beta_x, beta_y, tune_x, tune_y, aperture=0.2): &quot;&quot;&quot;Constructor. Parameters - alpha_x, alpha_y, beta_x, beta_y : float Twiss parameters at the lattice entrance. tune_x, tune_y : float Number of phase space oscillations per turn. &quot;&quot;&quot; self.P = np.zeros((4, 4)) self.V = np.zeros((4, 4)) self.M = np.zeros((4, 4)) self.P[:2, :2] = P_2D(tune_x) self.P[2:, 2:] = P_2D(tune_y) self.V[:2, :2] = V_2D(alpha_x, beta_x) self.V[2:, 2:] = V_2D(alpha_y, beta_y) self.M = np.linalg.multi_dot([self.V, self.P, la.inv(self.V)]) self.aperture = aperture self.multipole = None def add_multipole(self, multipole): self.multipole = multipole def track_part(self, vec): &quot;&quot;&quot;Track a single particle through the lattice. vec : ndarray, shape (4,) Transverse phase space coordinate vector [x, x&#39;, y, y&#39;]. &quot;&quot;&quot; vec = np.matmul(self.M, vec) if self.multipole is not None: vec = self.multipole.track_part(vec) return vec def track_bunch(self, X): &quot;&quot;&quot;Track a particle bunch through the lattice. X : ndarray, shape (nparts, 4) Transverse phase space coordinate array. &quot;&quot;&quot; X = np.apply_along_axis(self.track_part, 1, X) return self.collimate(X) def collimate(self, X): &quot;&quot;&quot;Delete particles outside aperture.&quot;&quot;&quot; radii = np.sqrt(X[:, 0]**2 + X[:, 2]**2) return np.delete(X, np.where(radii &gt; self.aperture), axis=0) def get_matched_bunch(self, nparts=2000, emittance=10e-6, cut=3.0): &quot;&quot;&quot;Generate truncated Gaussian distribution matched to the lattice.&quot;&quot;&quot; from scipy.stats import truncnorm X = truncnorm.rvs(a=4*[-cut], b=4*[cut], size=(nparts, 4)) A = np.sqrt(emittance) * np.identity(4) V = self.V X = np.apply_along_axis(lambda vec: np.matmul(A, vec), 1, X) X = np.apply_along_axis(lambda vec: np.matmul(V, vec), 1, X) return X . 1/3 integer resonance . We focus first on the 1/3 integer resonance. Below, a particle is tracked over 100 turns starting from few different initial amplitudes. We set $y = y&#39; = 0$ in all cases. The $x$-$x&#39;$ trajectories should be upright ellipses in the absence of nonlinear elements. Some helper functions are defined in the collapsed cell. . # Define Twiss parameters at the observation point alpha_x = alpha_y = 0.0 beta_x = beta_y = 20.0 def create_lattice(tune_x, tune_y, multipole=None): lattice = Lattice(alpha_x, alpha_y, beta_x, beta_y, tune_x, tune_y) lattice.add_multipole(multipole) return lattice def get_traj(lattice, emittance, nturns=1): &quot;&quot;&quot;Return array of shape (nturns, 4) of tracked single particle coordinates. The vertical coordinate and slope are set to zero. &quot;&quot;&quot; X = np.array([[np.sqrt(emittance * beta_x), 0, 0, 0]]) tracked_vec = [X[0]] for _ in range(nturns): X = lattice.track_bunch(X) if X.shape[0] == 0: # particle was deleted break tracked_vec.append(X[0]) return 1000 * np.array(tracked_vec) # convert from m to mm def compare_traj(tunes_x, tune_y, emittances, nturns=1, multipole=None, limits=(45, 2.5), **kws): &quot;&quot;&quot;Compare trajectories w/ different emittances as horizontal tune is scaled.&quot;&quot;&quot; kws.setdefault(&#39;s&#39;, 1) kws.setdefault(&#39;c&#39;, &#39;pink8&#39;) fig, axes = plot.subplots(nrows=2, ncols=3, figsize=(7.25, 4)) xlim, ylim = (-limits[0], limits[0]), (-limits[1], limits[1]) axes.format(xlabel=&quot;x [mm]&quot;, ylabel=&quot;x&#39; [mrad]&quot;, xlim=xlim, ylim=ylim) for ax, tune_x in zip(axes, tunes_x): lattice = create_lattice(tune_x, tune_y, multipole) for emittance in emittances: tracked_vec = get_traj(lattice, emittance, nturns) ax.scatter(tracked_vec[:, 0], tracked_vec[:, 1], **kws) ax.annotate(r&#39;$ nu_x = {:.3f}$&#39;.format(tune_x), xy=(0.7, 0.9), xycoords=&#39;axes fraction&#39;, bbox=dict(fc=&#39;w&#39;, ec=&#39;k&#39;)) return axes def track_bunch(X, lattice, nturns=1): &quot;&quot;&quot;Track and return list of coordinate array after each turn. Also return the fraction of particles which were lost (exceeded aperture) at each frame.&quot;&quot;&quot; coords, nparts, frac_lost = [X], X.shape[0], [0.0] for _ in range(nturns): X = lattice.track_bunch(X) coords.append(X) frac_lost.append(1 - X.shape[0] / nparts) return [1000*X for X in coords], frac_lost def animate_phase_space(coords, frac_lost=None, limits=(55, 5)): &quot;&quot;&quot;Create animation of turn-by-turn x-x&#39; and y-y&#39; distributions.&quot;&quot;&quot; fig, axes = plot.subplots(ncols=2, figsize=(5, 2.5), wspace = 0.75, sharey=False, sharex=False) xlim, ylim = (-limits[0], limits[0]), (-limits[1], limits[1]) axes.format(xlim=xlim, ylim=ylim, xlabel_kw={&#39;size&#39;:8}, ylabel_kw={&#39;size&#39;:8}) axes[0].format(xlabel=&quot;x [mm]&quot;, ylabel=&quot;x&#39; [mrad]&quot;) axes[1].format(xlabel=&quot;y [mm]&quot;, ylabel=&quot;y&#39; [mrad]&quot;) for ax in axes: for side in [&#39;top&#39;, &#39;right&#39;]: ax.spines[side].set_visible(False) plt.close() kws = dict(marker=&#39;.&#39;, c=&#39;steelblue&#39;, ms=3, lw=0, markeredgewidth=0, fillstyle=&#39;full&#39;) line0, = axes[0].plot([], [], **kws) line1, = axes[1].plot([], [], **kws) def update(t): x, xp, y, yp = coords[t].T line0.set_data(x, xp) line1.set_data(y, yp) axes[0].set_title(&#39;Turn {}&#39;.format(t), fontsize=8) if frac_lost: axes[1].set_title(&#39;Frac. lost = {:.3f}&#39;.format(frac_lost[t]), fontsize=8) return animation.FuncAnimation(fig, update, frames=len(coords)) . . tunes_x = np.linspace(0.61, 0.66, 6) tune_y = 0.518 emittances = 1e-6 * np.array([10, 30, 60, 90]) nturns = 100 axes = compare_traj(tunes_x, tune_y, emittances, nturns) axes.format(suptitle=&#39;Linear lattice&#39;) . Now turn on the sextupole magnet. . axes = compare_traj(tunes_x, tune_y, emittances, nturns, multipole) axes.format(suptitle=&#39;Linear lattice + sextupole&#39;) . The initially elliptical orbits are morphed into a triangular shape as the tune approaches the resonance condition, and some of the larger orbits become unstable. It turns out that by looking at the Hamiltonian you can find a triangular region defining a separatrix between stable and unstable motion. Particles inside the triangle will oscillate forever, particles at the corner of the triangle are at unstable equilibrium points, and particles outside the triangle will eventually stream outward from the corners. This is easier to see by tracking a bunch of particles. The interesting stuff will be in the horizontal plane, but I&#39;ll plot the vertical plane as well for comparison. . lattice = create_lattice(0.66, tune_y, multipole) X = lattice.get_matched_bunch() coords, frac_lost = track_bunch(X, lattice, nturns=50) animate_phase_space(coords, frac_lost) . &lt;/input&gt; Once Loop Reflect The triangular region of stability is clearly visible at the end of 50 turns. Interestingly, the third order resonance can be used to extract a beam from an accelerator at a much slower rate than normal. To do this, the strength and spacing of sextupole magnets must be carefully chosen to control the shape and orientation of the stability triangle, then tune is slowly moved closer to the 1/3 integer resonance value. The result is that the triangle shrinks as the stable phase space area decreases, and that more and more particles will find themselves in the unstable area and eventually stream out along the vertices. . Integer resonance . The sextupole should also excite the integer resonance. . compare_traj(np.linspace(0.96, 0.976, 6), tune_y, emittances, nturns, multipole, limits=(60, 2.5)); . lattice = create_lattice(0.99, 0.18, multipole) animate_phase_space(*track_bunch(X, lattice, nturns=75)) . &lt;/input&gt; Once Loop Reflect Cool pattern! The separatrix is now shaped like a tear drop. It looks like it&#39;s evolving more slowly because the tune is close to an integer, so the particles almost return to the same location in phase space after a turn. . Higher order resonances . There are also higher order resonances which a sextupole can drive. You can actually find fourth and fifth order resonances if you perform perturbation theory up to second order (at least that&#39;s what I&#39;m told in a textbook... I&#39;d like to avoid carrying out such a procedure). Do these show up using our mapping equations? They are expected to be weaker, so we&#39;ll double the sextupole strength. . emittances = 1e-6 * np.array([1, 2, 7, 10, 25, 50, 100, 150, 200, 250, 350]) compare_traj(np.linspace(0.7496, 0.798, 6), 0.23, emittances, 1000, Multipole(3, 1.0), limits=(150, 6), s=0.1); . These are really interesting plots. The tune near 0.75 (it&#39;s actually 0.7496) is exciting a fourth order resonance, while the tune near 0.8 is exciting a fifth order resonance. In all the plots, the low amplitude orbits are stable ellipses. We then see the behavior change as the amplitude is increased, with the particle jumping between distinct &quot;islands&quot;. Eventually the trajectories once again form closed loops, but in deformed shapes. The motion is unstable at even larger amplitudes. Understanding exactly why the the plots look like they do would take more work. . Conclusion . This post outlined the theory of nonlinear resonances driven by magnetic multipoles. The effect of a sextupole-driven resonance on the phase space trajectory was then examined using mapping equations. Taking the time to write down the steps which lead to Eq. (20), an equation which is often referenced in accelerator physics, was a rewarding experience and helped make the topic less mysterious to me (although I&#39;m no expert). Here are a number of helpful references: . Lectures S. Lund, Transverse Particle Resonances with Application to Circular Accelerators | E. Prebys, Resonances and Coupling | . | Textbooks D. Edwards and M. Syphers, An introduction to the Physics of High Energy Accelerators | H. Wiedemann, Particle Accelerator Physics | S. Y. Lee, Accelerator Physics | L. Reichl, The Transition to Chaos — Conservative Classical Systems and Quantum Manifestations | J. Taylor, Classical Mechanics | H. Goldstein, Classical Mechanics | . | .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/nonlinear%20systems/resonances/2021/03/28/nonlinear_resonances.html",
            "relUrl": "/physics/accelerators/nonlinear%20systems/resonances/2021/03/28/nonlinear_resonances.html",
            "date": " • Mar 28, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Particle-in-cell simulation",
            "content": "Many simulation codes exist for beam physics (one example is PyORBIT). A key component of the these simulations is the inclusion of the electromagnetic interactions between particles in the beam, also known as space charge forces. One way to compute space charge forces is the particle-in-cell (PIC) method. This post implements the PIC method in Python. . Theoretical model . We&#39;ll use bunch to refer to a group of particles in three-dimensional (3D) space, and we&#39;ll use a local cartesian coordinate system whose origin moves with the center of the bunch as shown below: . . The $s$ coordinate specifies the position of the bunch in the accelerator, and the path can be curved. Now for a few assumptions and approximations. First, assume all particles in the bunch move at a constant velocity $ beta c$, where $c$ is the speed of light. We then make the paraxial approximation. It&#39;s conventional to use the slope $x&#39; = dx/ds$ instead of the velocity, and the paraxial approximation assumes this slope is very small. Usually we report this slope in milliradians since $tan theta approx theta$ for small angles. Next we assume that the transverse ($x$-$y$) size of the bunch varies slowly along the $s$ axis. If this is true and we look at the electric field in a transverse slice of the bunch, there won&#39;t be much difference between the true field and the field of an infinitely long, uniform density cylinder. Our focus will be on the transverse dynamics of such a slice, so we&#39;ll treat each &quot;particle&quot; as an infinite line of charge. The figure below illustrates this approximation. . . Credit: G. Franchetti Another approximation is to neglect any magnetic fields generated by the beam, which is again valid if the transverse velocities are very small relative to $ beta c$. All this being said, the equations of motion without any external forces, i.e., in free space, can be written as . $$ mathbf{x}&#39;&#39; = frac{q}{mc^2 beta^2 gamma^3} mathbf{E}, tag{1}$$ . where $ mathbf{x} = [x, y]^T$ is the coordinate vector, $ mathbf{E} = [E_x, E_y]^T$ is the self-generated electric field, $m$ is the particle mass, and $ gamma = left({1 - beta^2} right)^{-1/2}$. Let&#39;s first address the factor $ gamma^{-3}$ in the equation of motion, which means that the space charge force goes to zero as the velocity approaches the speed of light. This is because parallel moving charges generate an attractive magnetic force which grows with velocity, completely cancelling the electric force in the limit $v rightarrow c$. . . Credit: OpenStax University PhysicsOne may ask: what about the rest frame in which there is no magnetic field? But special relativity says that electrogmagnetic fields change with reference frame. Using the transformations defined here, you can quickly prove that . $$ mathbf{E}_{lab} = frac{ mathbf{E}_{rest}}{ gamma}. tag{2}$$ . This inverse relationship between velocity and the space charge force has real-life consequences. It tells us that space charge is important if 1) the beam is very intense, meaning there are many particles in a small area, or 2) the beam is very energetic, meaning it is moving extremely fast. For example, space charge can usually be ignored in electron beams, which move near the speed of light for very modest energies due to their tiny mass, but is significant in high-intensity, low-energy hadron accelerators such as FRIB, SNS, and ESS. . We should now address the difficulty in determining the evolution of this system: the force on a particle in an $n$-particle bunch depends on the positions of the other $n - 1$ particles. The approach of statistical mechanics to this problem is to introduce a distribution function $f( mathbf{x}, mathbf{x}&#39;, s)$ which gives the particle density at axial position $s$ and phase space coordinates $ mathbf{x}$, $ mathbf{x}&#39;$. The Vlasov-Poisson system of equations determines the evolution of $f$ as long as we ignore collisions between particles: . $$ frac{ partial{f}}{ partial{s}} + mathbf{x}&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}}} + mathbf{x}&#39;&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}&#39;}} = 0. tag{3}$$ . We know $ mathbf{x&#39;&#39;}$ from Eq. (1). The electric field is obtained from Poisson&#39;s equation: . $$ nabla cdot mathbf{E} = - nabla^2 phi = frac{ rho}{ varepsilon_0}. tag{4}$$ . Finally, the transverse charge density $ rho$ is determined by . $$ rho = q int{f dx&#39;dy&#39;}. tag{5}$$ . Although these equations are easy to write down, they are generally impossible to solve analytically. We need to turn to a computer for help. . Computational method . The Vlasov equation could be solved directly, but this is difficult, especially in 2D or 3D. On the other end of the spectrum, the notion of a fluid in phase space could be abandoned and each particle could be tracked individually, computing the forces using direct sums. But this is infeasible with current hardware; the time complexity would by $O(n^2)$, where $n$ is the number of particles, and $n$ may be on the order of $10^{14}$. The particle-in-cell (PIC) method is a sort of combination of these two approaches. The idea is to track a group of macroparticles according to Eq. (1), each of which represents a large number of real particles. The fields, however, are solved from Eq. (4). The key step is transforming back and forth between a discrete and continuous representation of the bunch. The simulation loop for the PIC method is shown below. . . In the next sections I will discuss each of these steps and implement them in Python. The hidden cell below shows all the imports needed to run the code. . import numpy as np from scipy.interpolate import RegularGridInterpolator from scipy.fft import fft2, ifft2 from scipy.integrate import odeint from scipy.stats import truncnorm from tqdm import trange import Cython %load_ext cython # Plotting from matplotlib import pyplot as plt, animation from matplotlib.lines import Line2D from matplotlib.patches import Ellipse import seaborn as sns import proplot as plot . . Let&#39;s first create a Bunch class, which is a simple container for the bunch coordinates. . class Bunch: &quot;&quot;&quot;Container for 2D distribution of positive elementary charges. Attributes - intensity : float Number of physical particles in the bunch. length : float Length of the bunch [m]. mass, kin_energy : float Mass [GeV/c^2], charge [C], and kinetic energy [GeV] per particle. nparts : float Number of macroparticles in the bunch. X : ndarray, shape (nparts, 4) Array of particle coordinates. Columns are [x, x&#39;, y, y&#39;]. Units are meters and radians. positions : ndarray, shape (nparts, 2): Just the x and y positions (for convenience). &quot;&quot;&quot; def __init__(self, intensity=1e14, length=250., mass=0.938, kin_energy=1.0): self.intensity, self.length = intensity, length self.mass, self.kin_energy = mass, kin_energy self.gamma = 1 + (kin_energy / mass) # Lorentz factor self.beta = np.sqrt(1 - (1 / self.gamma)**2) # v/c r0 = 1.53469e-18 # classical proton radius [m] self.perveance = 2 * r0 * intensity / (length * self.beta**2 * self.gamma**3) self.nparts = 0 self.compute_macrosize() self.X, self.positions = None, None def compute_macrosize(self): &quot;&quot;&quot;Update the macrosize and macrocharge.&quot;&quot;&quot; self.macrosize = self.intensity // self.nparts if self.nparts &gt; 0 else 0 def fill(self, X): &quot;&quot;&quot;Fill with particles.&quot;&quot;&quot; self.X = X if self.X is None else np.vstack([self.X, X]) self.positions = self.X[:, [0, 2]] self.nparts = self.X.shape[0] self.compute_macrosize() def compute_extremum(self): &quot;&quot;&quot;Get extreme x and y coorinates.&quot;&quot;&quot; self.xmin, self.ymin = np.min(self.positions, axis=0) self.xmax, self.ymax = np.max(self.positions, axis=0) self.xlim, self.ylim = (self.xmin, self.xmax), (self.ymin, self.ymax) . Weighting . Starting from a group of macroparticles, we need to produce a charge density $ rho_{i,j}$ on a grid. The most simple approach is the nearest grid point (NGP) method, which, as the name suggests, assigns the full particle charge to the closest grid point. This is commonly called zero-order weighting; although it is very fast and easy to implement, it is not commonly used because it can lead to significant noise. A better method called cloud-in-cell (CIC) treats each particle as a rectangular, uniform density cloud of charge with dimensions equal to the grid spacing. A fractional part of the charge is assigned based on the fraction of the cloud overlapping with a given cell. This can be thought of as first-order weighting. To get a sense of what these methods are doing (in 1D), we can slide a particle across a cell and plot the resulting density of the cell at each position, thus giving an effective particle shape. . def shape_func(u, v, cell_width, method=&#39;ngp&#39;): S, diff = 0, np.abs(u - v) if method == &#39;ngp&#39;: S = 1 if diff &lt; (0.5 * cell_width) else 0 elif method == &#39;cic&#39;: S = 1 - diff/cell_width if diff &lt; cell_width else 0 return S / cell_width fig, axes = plot.subplots(ncols=2, figsize=(5, 1.5)) xvals = np.linspace(-1, 1, 1000) for ax, method in zip(axes, [&#39;ngp&#39;, &#39;cic&#39;]): densities = [shape_func(x, 0, 1, method) for x in xvals] ax.plot(xvals, densities, &#39;k&#39;) axes.format( xlabel=&#39;($x - x_k) ,/ , Delta x$&#39;, ylabel=&#39;Density&#39;, grid=False, xlabel_kw={&#39;size&#39;:&#39;large&#39;}, ylabel_kw={&#39;size&#39;:&#39;large&#39;}, toplabels=[&#39;NGP&#39;, &#39;CIC&#39;] ) . . The NGP method leads to a discontinuous boundary while the CIC method leads to a continous boundary (but discontinous derivative). There are also higher order methods which lead to a smooth boundary, but I don&#39;t cover those here. . We also need to perform the inverse operation: given the electric field at each grid point, interpolate the value at each particle position. The same method applies here. NGP just uses the electric field at the nearest grid point, while CIC weights the four nearest grid points. The following Grid class implements the CIC method. Notice that Cython is used in the for-loop in the distribute method. I couldn&#39;t figure out a way to perform this operation with the loop, and in pure Python it took about 90% of the runtime for a single simulation step. Using Cython gave a significant performance boost. . %%cython import numpy as np from scipy.interpolate import RegularGridInterpolator class Grid: &quot;&quot;&quot;Class for 2D grid. Attributes - xmin, ymin, xmax, ymax : float Minimum and maximum coordinates. Nx, Ny : int Number of grid points. dx, dy : int Spacing between grid points. x, y : ndarray, shape (Nx,) or (Ny,) Positions of each grid point. cell_area : float Area of each cell. &quot;&quot;&quot; def __init__(self, xlim=(-1, 1), ylim=(-1, 1), size=(64, 64)): self.xlim, self.ylim = xlim, ylim (self.xmin, self.xmax), (self.ymin, self.ymax) = xlim, ylim self.size = size self.Nx, self.Ny = size self.dx = (self.xmax - self.xmin) / (self.Nx - 1) self.dy = (self.ymax - self.ymin) / (self.Ny - 1) self.cell_area = self.dx * self.dy self.x = np.linspace(self.xmin, self.xmax, self.Nx) self.y = np.linspace(self.ymin, self.ymax, self.Ny) def set_lims(self, xlim, ylim): &quot;&quot;&quot;Set the min and max grid coordinates.&quot;&quot;&quot; self.__init__(xlim, ylim, self.size) def zeros(self): &quot;&quot;&quot;Create array of zeros with same size as the grid.&quot;&quot;&quot; return np.zeros((self.size)) def distribute(self, positions): &quot;&quot;&quot;Distribute points on the grid using the cloud-in-cell (CIC) method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. Returns - rho : ndarray, shape (Nx, Ny) The value rho[i, j] gives the number of macroparticles in the i,j cell. &quot;&quot;&quot; # Compute area overlapping with 4 nearest neighbors (A1, A2, A3, A4) ivals = np.floor((positions[:, 0] - self.xmin) / self.dx).astype(int) jvals = np.floor((positions[:, 1] - self.ymin) / self.dy).astype(int) ivals[ivals &gt; self.Nx - 2] = self.Nx - 2 jvals[jvals &gt; self.Ny - 2] = self.Ny - 2 x_i, x_ip1 = self.x[ivals], self.x[ivals + 1] y_j, y_jp1 = self.y[jvals], self.y[jvals + 1] _A1 = (positions[:, 0] - x_i) * (positions[:, 1] - y_j) _A2 = (x_ip1 - positions[:, 0]) * (positions[:, 1] - y_j) _A3 = (positions[:, 0] - x_i) * (y_jp1 - positions[:, 1]) _A4 = (x_ip1 - positions[:, 0]) * (y_jp1 - positions[:, 1]) # Distribute fractional areas rho = self.zeros() cdef double[:, :] rho_view = rho cdef int i, j for i, j, A1, A2, A3, A4 in zip(ivals, jvals, _A1, _A2, _A3, _A4): rho_view[i, j] += A4 rho_view[i + 1, j] += A3 rho_view[i, j + 1] += A2 rho_view[i + 1, j + 1] += A1 return rho / self.cell_area def interpolate(self, grid_vals, positions): &quot;&quot;&quot;Interpolate values from the grid using the CIC method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. grid_vals : ndarray, shape (n, 2) Scalar value at each coordinate point. Returns - int_vals : ndarray, shape (nparts,) Interpolated value at each position. &quot;&quot;&quot; int_func = RegularGridInterpolator((self.x, self.y), grid_vals) return int_func(positions) def gradient(self, grid_vals): &quot;&quot;&quot;Compute gradient using 2nd order centered differencing. Parameters - grid_vals : ndarray, shape (Nx, Ny) Scalar values at each grid point. Returns - gradx, grady : ndarray, shape (Nx, Ny) The x and y gradient at each grid point. &quot;&quot;&quot; return np.gradient(grid_vals, self.dx, self.dy) . It should also be mentioned that the field interpolation method should be the same as the charge deposition method; if this is not true, it is possible for a particle to exert a force on itself! Let&#39;s test the method on a Gaussian distribution of 100,000 macroparticles in the $x$-$y$ plane, truncated at three standard devations. We&#39;ll choose the number of grid points to be $N_x = N_y = 64$. . # Create coordinate array nparts = 100000 cut = 3.0 X = truncnorm.rvs(a=4*[-cut], b=4*[cut], size=(nparts, 4)) X *= 10e-6 # Create bunch bunch = Bunch() bunch.fill(X) bunch.compute_extremum() # Distribute bunch particles on grid grid = Grid(bunch.xlim, bunch.ylim, size=(64, 64)) rho = grid.distribute(bunch.positions) # Plot fig, axes = plt.subplots(1, 2, figsize=(5.5, 2.25), sharey=True, sharex=True) X_samp = X[np.random.choice(X.shape[0], 2000, replace=False), :] x = (X_samp[:, 0] - grid.xmin) / grid.dx y = (X_samp[:, 2] - grid.ymin) / grid.dy axes[0].set_facecolor(&#39;k&#39;) axes[0].scatter(x, y, s=1, c=&#39;w&#39;, ec=&#39;none&#39;) sns.heatmap(rho.T, ax=axes[1], cmap=&#39;binary_r&#39;, cbar=False) axes[0].set_title(&#39;2000 random samples&#39;) axes[1].set_title(&#39;CIC weighting&#39;); . . Field solver . The workhorse in the simulation loop is the field solver. We need to solve Poisson&#39;s equation: . $$ left({ frac{ partial^2}{ partial x^2} + frac{ partial^2}{ partial y^2}} right) = - frac{ rho left(x, y right)}{ varepsilon_0}. tag{6}$$ . The discretized version of the equation reads . $$ frac{ phi_{i+1,j} -2 phi_{i,j} + phi_{i-1,j}}{{ Delta_x}^2} + frac{ phi_{i,j+1} -2 phi_{i,j} + phi_{i,j-1}}{{ Delta_y}^2} = - frac{ rho_{i,j}}{ varepsilon_0} tag{7}$$ . for a grid with spacing $ Delta_x$ and $ Delta_y$. There are multiple paths to a solution; we will focus on the method implemented in PyORBIT which utilizes the Fourier convolution theorem. Let&#39;s briefly go over this method. The potential from an infinite line of elementary charges at the origin with number density $ lambda$ is . $$ phi( mathbf{x}) = - frac{ lambda e}{2 pi varepsilon_0} ln{| mathbf{x}|} = - frac{ lambda e}{2 pi varepsilon_0} int{ ln{| mathbf{x} - mathbf{y}|} delta( mathbf{y})d mathbf{y}}. tag{8}$$ . Note that $ mathbf{y}$ is just a dummy variable. By letting $G( mathbf{x} - mathbf{y}) = - ln{| mathbf{x} - mathbf{y}|}$ and $ rho( mathbf{x}) = delta( mathbf{x})$, then up to a scaling factor we have . $$ phi( mathbf{x}) = int{G( mathbf{x} - mathbf{y}) rho( mathbf{y})d mathbf{y}} = G( mathbf{x}) * rho( mathbf{x}). tag{9}$$ . In this form the potential is a convolution (represented by $*$) of the charge density $ rho$ with $G$, which is called the Green&#39;s function. On the grid this will look like . $$ phi_{i, j} = sum_{k,l ne i,j}{G_{i-k, j-l} rho_{k, l}}. tag{11}$$ . This solves the problem in $O(N^2)$ time complexity for $N$ grid points. This is already much faster than a direct force calculation but could still get expensive for fine grids. We can speed things up by exploiting the convolution theorem, which says that the Fourier transform of a convolution of two functions is equal to the product of their Fourier transforms. The Fourier transform is defined by . $$ hat{ phi}( mathbf{k})= mathcal{F} left[ phi( mathbf{x}) right] = int_{- infty}^{ infty}{e^{-i mathbf{k} cdot mathbf{x}} phi( mathbf{x}) d mathbf{x}}. tag{12}$$ . The convolution theorem then says $$ mathcal{F} left[ rho * G right] = mathcal{F} left[ rho right] cdot mathcal{F} left[G right]. tag{13}$$ . For the discrete equation this gives . $$ hat{ phi}_{n, m} = hat{ rho}_{n, m} hat{G}_{n, m}, tag{14}$$ . where the hat represents the discrete Fourier transform. The time complexity can be reduced to $O left(N log N right)$ with the FFT algorithm at our disposal. . There is a caveat to this method: Eq. (11) must be a circular convolution in order to use the FFT algorithm, which means $G$ must be periodic. But the beam is in free space (we&#39;ve neglected any conducting boundary), so this is not true. We can make it true by doubling the grid size in each dimension. We then make $G$ a mirror reflection in the new quadrants so that it is periodic, and also set the charge density equal to zero in these regions. After running the method on this larger grid, the potential in the new quadrants will be unphysical; however, the potential in the original quadrant will be correct. There are also some tricks we can play to reduce the space complexity, and in the end doubling the grid size is not much of a price to pay for the gain in speed. The method is implemented in the PoissonSolver class. . class PoissonSolver: &quot;&quot;&quot;Class to solve Poisson&#39;s equation on a 2D grid. Attributes - rho, phi, G : ndarray, shape (2*Nx, 2*Ny) The density (rho), potential (phi), and Green&#39;s function (G) at each grid point on a doubled grid. Only one quadrant (i &lt; Nx, j &lt; Ny) corresponds to to the real potential. &quot;&quot;&quot; def __init__(self, grid, sign=-1.): self.grid = grid new_shape = (2 * self.grid.Nx, 2 * self.grid.Ny) self.rho, self.G = np.zeros(new_shape), np.zeros(new_shape) self.phi = np.zeros(new_shape) def set_grid(self, grid): self.__init__(grid) def compute_greens_function(self): &quot;&quot;&quot;Compute Green&#39;s function on doubled grid.&quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny Y, X = np.meshgrid(self.grid.x - self.grid.xmin, self.grid.y - self.grid.ymin) self.G[:Nx, :Ny] = -0.5 * np.log(X**2 + Y**2, out=np.zeros_like(X), where=(X + Y &gt; 0)) self.G[Nx:, :] = np.flip(self.G[:Nx, :], axis=0) self.G[:, Ny:] = np.flip(self.G[:, :Ny], axis=1) def get_potential(self, rho): &quot;&quot;&quot;Compute the scaled electric potential on the grid. Parameters - rho : ndarray, shape (Nx, Ny) Number of macroparticles at each grid point. Returns - phi : ndarray, shape (Nx, Ny) Scaled electric potential at each grid point. &quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny self.rho[:Nx, :Ny] = rho self.compute_greens_function() self.phi = ifft2(fft2(self.G) * fft2(self.rho)).real return self.phi[:Nx, :Ny] . Running the algorithm gives the following potential on the doubled grid: . solver = PoissonSolver(grid) phi = solver.get_potential(rho) fig, axes = plt.subplots(ncols=2, figsize=(7, 3), sharey=True) sns.heatmap(solver.rho.T, ax=axes[0], cmap=&#39;binary_r&#39;, cbar=False) sns.heatmap(solver.phi.T, ax=axes[1], cbar=False) for ax in axes: ax.axvline(grid.Nx - 0.5, c=&#39;w&#39;); ax.axhline(grid.Ny - 0.5, c=&#39;w&#39;) for xy in [(0.65, 0.75), (0.15, 0.25), (0.65, 0.25)]: ax.annotate(&#39;unphysical&#39;, xy=xy, xycoords=&#39;axes fraction&#39;, c=&#39;w&#39;) axes[0].set_title(r&#39;Density $ rho$&#39;) axes[1].set_title(r&#39;Potential $ phi$&#39;); . . We can then approximate the gradient of the potential using second-order centered differencing. This gives . $$( nabla phi)_{i,j} = frac{ phi_{i+1,j} - phi_{i-1,j}}{2 Delta_x} hat{x} + frac{ phi_{i,j+1} - phi_{i,j-1}}{2 Delta_y} hat{y}. tag{15}$$ . Ex, Ey = grid.gradient(-phi) fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(7, 2.5), gridspec_kw=dict(width_ratios=[1.04, 1, 0.025])) sns.heatmap(np.rot90(Ex), ax=ax1, cmap=&#39;RdBu&#39;, cbar=False); sns.heatmap(np.rot90(Ey), ax=ax2, cmap=&#39;RdBu&#39;, cbar_ax=ax3); ax1.set_title(&#39;$E_x$&#39;) ax2.set_title(&#39;$E_y$&#39;) ax2.set_yticklabels([]); . . Finally, the value of the electric field at each particle position can be interpolated from the grid. . Ex_int = grid.interpolate(Ex, bunch.positions) Ey_int = grid.interpolate(Ey, bunch.positions) . Particle mover . All we need to do in this step is integrate the equations of motion. A common method is leapfrog integration in which the position and velocity are integrated out of phase as follows: . $$ m left( frac{ mathbf{v}_{i+1/2} - mathbf{v}_{i-1/2}}{ Delta_t} right) = mathbf{F}( mathbf{x}_i), tag{16}$$ . $$ frac{ mathbf{x}_{i+1} - mathbf{x}_i}{ Delta_t} = mathbf{v}_{i+1/2} tag{17}$$ . . Credit: S. LundA different scheme must be used when velocity-dependent forces are present. This is a symplectic integrator, which means it conserves energy. It is also second-order accurate, meaning that its error is proportional to the square of the $ Delta_t$. Finally, it is time-reversible. The only complication is that, because the velocity and position are out of phase, we need to push the velocity back one half-step before starting the simulation, and push it one half-step forward when taking a measurement. . Putting it all together . Simulation loop . We have all the tools to implement the simulation loop. While $s &lt; s_{max}$ we: . Compute the charge density on the grid. | Find the electric potential on the grid. | Interpolate the electric field at the particle positions. | Update the particle positions. | We&#39;ll first create a History class which stores the beam moments or phase space coordinates. . class History: &quot;&quot;&quot;Class to store bunch data over time. Atributes moments : list Second-order bunch moments. Each element is ndarray of shape (10,). coords : list Bunch coordinate arrays. Each element is ndarray of shape (nparts, 4) moment_positions, coord_positions : list Positions corresponding to each element of `moments` or `coords`. &quot;&quot;&quot; def __init__(self, bunch, samples=&#39;all&#39;): self.X = bunch.X self.moments, self.coords = [], [] self.moment_positions, self.coord_positions = [], [] if samples == &#39;all&#39; or samples &gt;= bunch.nparts: self.idx = np.arange(bunch.nparts) else: self.idx = np.random.choice(bunch.nparts, samples, replace=False) def store_moments(self, s): Sigma = np.cov(self.X.T) self.moments.append(Sigma[np.triu_indices(4)]) self.moment_positions.append(s) def store_coords(self, s): self.coords.append(np.copy(self.X[self.idx, :])) self.coord_positions.append(s) def package(self): self.moments = np.array(self.moments) self.coords = np.array(self.coords) . Now we&#39;ll create a Simulation class. . class Simulation: &quot;&quot;&quot;Class to simulate the evolution of a charged particle bunch in free space. Attributes - bunch : Bunch: The bunch to track. distance : float Total tracking distance [m]. step_size : float Distance between force calculations [m]. nsteps : float Total number of steps = int(length / ds). steps_performed : int Number of steps performed so far. s : float Current bunch position. history : History object Object storing historic bunch data. meas_every : dict Dictionary with keys: &#39;moments&#39; and &#39;coords&#39;. Values correspond to the number of simulations steps between storing these quantities. For example, `meas_every = {&#39;coords&#39;:4, &#39;moments&#39;:2}` will store the moments every 4 steps and the moments every other step. Defaults to storing only the initial and final positions. samples : int Number of bunch particles to store when measuring phase space coordinates. Defaults to the entire coordinate array. &quot;&quot;&quot; def __init__(self, bunch, distance, step_size, grid_size, meas_every={}, samples=&#39;all&#39;): self.bunch = bunch self.distance, self.step_size = distance, step_size self.nsteps = int(distance / step_size) self.grid = Grid(size=grid_size) self.solver = PoissonSolver(self.grid) self.fields = np.zeros((bunch.nparts, 2)) self.history = History(bunch, samples) self.s, self.steps_performed = 0.0, 0 self.meas_every = meas_every self.meas_every.setdefault(&#39;moments&#39;, self.nsteps) self.meas_every.setdefault(&#39;coords&#39;, self.nsteps) self.sc_factor = bunch.perveance / bunch.nparts def set_grid(self): &quot;&quot;&quot;Set grid limits from bunch size.&quot;&quot;&quot; self.bunch.compute_extremum() self.grid.set_lims(self.bunch.xlim, self.bunch.ylim) self.solver.set_grid(self.grid) def compute_electric_field(self): &quot;&quot;&quot;Compute self-generated electric field.&quot;&quot;&quot; self.set_grid() rho = self.grid.distribute(self.bunch.positions) phi = self.solver.get_potential(rho) Ex, Ey = self.grid.gradient(-phi) self.fields[:, 0] = self.grid.interpolate(Ex, self.bunch.positions) self.fields[:, 1] = self.grid.interpolate(Ey, self.bunch.positions) def kick(self, step_size): &quot;&quot;&quot;Update particle slopes.&quot;&quot;&quot; self.bunch.X[:, 1] += self.sc_factor * self.fields[:, 0] * step_size self.bunch.X[:, 3] += self.sc_factor * self.fields[:, 1] * step_size def push(self, step_size): &quot;&quot;&quot;Update particle positions.&quot;&quot;&quot; self.bunch.X[:, 0] += self.bunch.X[:, 1] * step_size self.bunch.X[:, 2] += self.bunch.X[:, 3] * step_size def store(self): &quot;&quot;&quot;Store bunch data.&quot;&quot;&quot; store_moments = self.steps_performed % self.meas_every[&#39;moments&#39;] == 0 store_coords = self.steps_performed % self.meas_every[&#39;coords&#39;] == 0 if not (store_moments or store_coords): return Xp = np.copy(self.bunch.X[:, [1, 3]]) self.kick(+0.5 * self.step_size) # sync positions/slopes if store_moments: self.history.store_moments(self.s) if store_coords: self.history.store_coords(self.s) self.bunch.X[:, [1, 3]] = Xp def run(self, meas_every={}): &quot;&quot;&quot;Run the simulation.&quot;&quot;&quot; self.store() self.compute_electric_field() self.kick(-0.5 * self.step_size) # desync positions/slopes for i in trange(self.nsteps): self.compute_electric_field() self.kick(self.step_size) self.push(self.step_size) self.s += self.step_size self.steps_performed += 1 self.store() self.history.package() . Demonstration . We need some way of checking our method&#39;s accuracy. Luckily there is an analytic benchmark available: the Kapchinskij-Vladimirskij (KV) distribution. Without going into any detail, the beam projects to a uniform density ellipse in the $x$-$y$ plane, and the space charge forces produced within this ellipse are linear (in general space charge forces are nonlinear). If we plug the KV distribution into the Vlasov equation, it can be seen that these forces will remain linear for all time if the external focusing forces are also linear. As a consequence, a set of self-consistent differential equations describing the evolution of the ellipse boundary can be written down. If we consider the beam to be an upright ellipse with semi-axis $a$ along the $x$ axis and $b$ along the $y$ axis, then without external fields the equations read: . $$ a&#39;&#39; = frac{2Q}{a + b} + frac{ varepsilon_x}{a^3}, $$ $$ b&#39;&#39; = frac{2Q}{a + b} + frac{ varepsilon_y}{b^3}. tag{18}$$ . These are known as the KV envelope equations or simply envelope equations. $Q$, called the perveance, is a dimensionless number which is proportional to the beam intensity but reduced by the beam energy. We can think of this constant as a measure of the space charge strength. The $ varepsilon_x$ and $ varepsilon_y$ terms are called the emittances and determine the area occupied by the beam in $x$-$x&#39;$ and $y$-$y&#39;$ phase space. For example, a beam with all particles sitting perfectly still in the $x$-$y$ plane has no emittance, but a beam which is instead spreading out has a nonzero emittance. These emittances will also be conserved for the KV distribution. The following function integrates the envelope equations. . def track_env(X, positions, perveance=0.0): &quot;&quot;&quot;Track beam moments (assuming KV distribution) through free space. Parameters - X : ndarray, shape (nparts, 4) Transverse bunch coordinate array. positions : list List of positions at which to evaluate the equations. perveance : float The dimensionless space charge perveance. Returns - ndarray, shape (len(positions), 4) Each row gives [a, a&#39;, b, b&#39;], where a and b are the beam radii in the x and y dimension, respectively. &quot;&quot;&quot; Sigma = np.cov(X.T) a, b = np.sqrt(Sigma[0, 0]), np.sqrt(Sigma[2, 2]) ap, bp = Sigma[0, 1] / a, Sigma[2, 3] / b epsx = np.sqrt(np.linalg.det(Sigma[:2, :2])) epsy = np.sqrt(np.linalg.det(Sigma[2:, 2:])) def derivs(env, s): a, ap, b, bp = env envp = np.zeros(4) envp[0], envp[2] = ap, bp envp[1] = 0.5 * perveance/(a + b) + epsx**2 / a**3 envp[3] = 0.5 * perveance/(a + b) + epsy**2 / b**3 return envp return odeint(derivs, [a, ap, b, bp], positions, atol=1e-14) . Some care must be taken in the choice of simulation parameters; we need a fine enough grid to resolve the hard edge of the beam and enough macroparticles per grid cell to collect good statistics. I chose what I thought was reasonable: 128,000 macroparticles, a step size of 2.5 cm, and a $128 times 128$ grid. Let&#39;s create and track four identical KV distributions, each with a different intensity. . nparts = 128000 bunch_length = 250.0 # [m] intensities = [0.0, 10e14, 20e14, 40e14] # Simulation parameters distance = 10.0 # [m] step_size = 0.025 # [m] grid_size = (128, 128) samples = 10000 meas_every = {&#39;moments&#39;: int(0.1 * distance/step_size), &#39;coords&#39;: 4} # Create KV bunch in normalized coordinates (surface of 4D unit sphere) X = np.random.normal(size=(nparts, 4)) # 4D Gaussian X = np.apply_along_axis(lambda row: row/np.linalg.norm(row), 1, X) # normalize rows # Scale by emittance eps_x, eps_y = 10e-6, 10e-6 A = 2 * np.sqrt(np.diag([eps_x, eps_x, eps_y, eps_y])) X = np.apply_along_axis(lambda row: np.matmul(A, row), 1, X) # Scale beam size and divergence relative to emittance alpha_x, alpha_y = 0.0, 0.0 beta_x, beta_y = 20.0, 20.0 V = np.zeros((4, 4)) V[:2, :2] = np.sqrt(1/beta_x)* np.array([[beta_x, 0], [alpha_x, 1]]) V[2:, 2:] = np.sqrt(1/beta_y)* np.array([[beta_y, 0], [alpha_y, 1]]) X = np.apply_along_axis(lambda row: np.matmul(V, row), 1, X) # Create and track bunches sims = [] for intensity in intensities: bunch = Bunch(intensity, bunch_length) bunch.fill(np.copy(X)) sim = Simulation(bunch, distance, step_size, grid_size, meas_every=meas_every, samples=samples) sim.run() sims.append(sim) . 100%|██████████| 400/400 [00:53&lt;00:00, 7.45it/s] 100%|██████████| 400/400 [00:52&lt;00:00, 7.58it/s] 100%|██████████| 400/400 [00:52&lt;00:00, 7.59it/s] 100%|██████████| 400/400 [00:52&lt;00:00, 7.55it/s] . bunch_positions = sims[0].history.moment_positions env_positions = np.linspace(0, distance, 400) rms_sizes_lists = {&#39;bunch&#39;:[], &#39;env&#39;:[]} for sim in sims: rms_sizes_lists[&#39;bunch&#39;].append(np.sqrt(sim.history.moments[:, [0, 7]])) rms_sizes_lists[&#39;env&#39;].append( track_env(X, env_positions, sim.bunch.perveance)[:, [0, 2]]) fig, axes = plot.subplots(ncols=2, figsize=(5.5, 2.5), spany=False) alphas = np.linspace(0.4, 1.0, 4) for key, rms_sizes_list in rms_sizes_lists.items(): for rms_sizes, alpha in zip(rms_sizes_list, alphas): x, y = 1000 * rms_sizes.T if key == &#39;env&#39;: c = &#39;steelblue&#39; axes[0].plot(env_positions, x, c=c, alpha=alpha) axes[1].plot(env_positions, y, c=c, alpha=alpha) elif key == &#39;bunch&#39;: c = &#39;k&#39; axes[0].scatter(bunch_positions, x, s=8, c=c, zorder=99, alpha=alpha-0.1) axes[1].scatter(bunch_positions, y, s=8, c=c, zorder=99, alpha=alpha-0.1) lines = [Line2D([0], [0], color=c, alpha=alphas[0]), Line2D([0], [0], color=c, alpha=alphas[1]), Line2D([0], [0], color=c, alpha=alphas[2]), Line2D([0], [0], color=c, alpha=alphas[3]), Line2D([0], [0], color=&#39;k&#39;, marker=&#39;o&#39;, lw=0, ms=2)] axes[1].legend(lines, [&#39;I = {:.0e}&#39;.format(I) for I in intensities] + [&#39;PIC&#39;], ncols=1, fontsize=7) axes[0].set_title(&#39;Horizontal&#39;) axes[1].set_title(&#39;Vertical&#39;) axes.format(ylabel=&#39;rms beam size [$mm$]&#39;, xlabel=&#39;Distance [m]&#39;, suptitle=&#39;KV benchmark: drift&#39;, grid=False); . . This plot shows the horizontal and vertical beam size over time for each of the four chosen beam intensities. The solid lines are the result of integrating the envelope equations, while the black dots are the result of the PIC calculation. Notice that the beam expands on its own due to the nonzero emittance and that the effect of space charge is to increase the expansion rate. It seems to be quite accurate over this distance, and the runtime is acceptable for my purposes. Here is the evolution of a sample of 10,000 of the macroparicles as well as an ellipse showing the KV envelope. . # Get coordinates coords_list = [sim.history.coords for sim in sims] positions = sims[0].history.coord_positions umax = 1.25 * 1000 * max([np.max(np.max(coords, axis=1)[:, [0, 2]]) for coords in coords_list]) # Create figure fig, axes = plot.subplots(nrows=2, ncols=2, figsize=(5, 4.5)) for ax in axes: for side in [&#39;top&#39;, &#39;right&#39;]: ax.spines[side].set_visible(False) axes.format(xlim=(-umax, umax), ylim=(-umax, umax), xlabel=&#39;x [mm]&#39;, ylabel=&#39;y [mm]&#39;, grid=False) axes[1].legend([Line2D([0], [0], color=&#39;k&#39;)], [&#39;KV envelope&#39;], frameon=False, loc=(0.5, 0.95)) for ax, intensity in zip(axes, intensities): ax.annotate(&#39;I = {:.2e}&#39;.format(intensity), xy=(0.05, 0.9), xycoords=&#39;axes fraction&#39;) plt.close() # Create lines plt_kws = dict(ms=2, c=&#39;steelblue&#39;, marker=&#39;.&#39;, lw=0, markeredgewidth=0, fillstyle=&#39;full&#39;) lines = [] for ax in axes: line, = ax.plot([], [], **plt_kws) lines.append(line) def update(t): for coords, line, ax, rms_sizes_list in zip(coords_list, lines, axes, rms_sizes_lists[&#39;env&#39;]): line.set_data(1000 * coords[t, :, 0], 1000 * coords[t, :, 2]) rms_sizes_list = rms_sizes_list[::4] a, b = 1000 * rms_sizes_list[t] ax.patches = [] ell = Ellipse((0, 0), 4*a, 4*b, color=&#39;k&#39;, fill=False, zorder=100, lw=1) ax.add_patch(ell) axes[0].set_title(&#39;s = {:.2f} m&#39;.format(positions[t])) fps = 20 frames = len(coords_list[0]) - 1 animation.FuncAnimation(fig, update, frames=frames, interval=1000/fps) . . &lt;/input&gt; Once Loop Reflect Conclusion . This post implemented an electrostatic PIC solver in Python. I learned quite a bit from doing this and was happy to see my calculations agree with the theoretical benchmark. One extension of this code would be to consider the velocity-dependent force from magnetic fields. It would also be straightforward to extend the code to 3D. Finally, all the methods used here are applicable to gravitational simulations. Here are some helpful references: . USPAS course | Hockney &amp; Eastwood | Birdsall &amp; Langdon | .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/simulation/space%20charge/2021/02/22/PIC.html",
            "relUrl": "/physics/accelerators/simulation/space%20charge/2021/02/22/PIC.html",
            "date": " • Feb 22, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Coupled parametric oscillators",
            "content": "Introduction . A previous post examined the analytic solutions to the equation of motion describing a parametric oscillator, which is a harmonic oscillator whose physical properties are time-dependent (but not dependent on the state of the oscillator). This problem was motivated by describing the transverse oscillations of a particle in an accelerator. In this post, the treatment will be extended to a coupled parametric oscillator. Basically, we are trying to solve the following equation of motion: . $$x&#39;&#39; + k_{11}(s)x + k_{13}(s)y + k_{14}(s)y&#39;,$$ $$y&#39;&#39; + k_{33}(s)y + k_{31}(s)x + k_{32}(s)x&#39;,$$ . where the prime denotes differentiation with respect to $s$. We also assume that each of the $k_{ij}$ coefficients are periodic, so $k_{ij}(s + L) = k_{ij}(s)$ for some $L$. . Motivation . The previous post discussed dipole and quadrupole magnetic fields, which have the special property that their fields depend linearly on $x$ and $y$, and are also uncoupled. Of course there are many other configurations possible. First, consider a solenoid magnet: . . Credit: brilliant.org The field within the coils points in the longitudinal direction and is approximatly constant ($ mathbf{B}_{sol} = B_0 hat{s}$). Plugging this into the Lorentz force equation we find: . $$ dot{ mathbf{v}} = frac{q}{m} mathbf{v} times mathbf{B} = frac{qB_0}{m} left({v_y hat{x} - v_x hat{y}} right).$$ . This means the motion in $x$ depends on the velocity in $y$, and vice versa, so this will contribute to $k_{14}$ and $k_{32}$. Coupling can also be produced from transverse magnetic fields. We again write the multipole expansion of this field: . $$B_y + iB_x = B_{ref} sum_{n=1}^{ infty} left({B_n + iA_n} right) left( frac{x + iy}{R_{ref}} right)^{n-1}.$$ . . Credit: Jeff Holmes . There will be nonlinear coupling (terms proportional to $x^j y^k$, where $j,k &gt; 1$ when $n &gt; 2$, but we are interested in linear coupling. This occurs when the skew quadrupole term $A_2$ is nonzero, which is true anytime a quadrupole is tilted in the transverse plane. The field couples the motion in one plane to the displacement in the other, contributing to the $k_{13}$ and $k_{31}$ terms. . Approach . Let&#39;s review the approach we took in analyzing the 1D parametric oscillator. We wrote the solution in pseudo-harmonic form, with an amplitude and phase which depended on time. We then found that particles travel along the boundary of an ellipse in 2D phase space, the area of which is a constant of the motion (we will denote this area by $ epsilon_x$). To understand the motion, we just need to know the dimensions and orientation of this ellipse, for which we proposed the parameters $ alpha_x$ and $ beta_x$, as well as the location of the particle on the ellipse boundary, which is determined by the phase $ mu_x$. All the subscripts can be replaced by $y$ to handle the vertical motion. We also wrote a transfer matrix $ mathbf{M}$, which connects the initial and final phase space coordinates after tracking through one period, from the parameters in the following form: . $$ mathbf{M} = mathbf{V} mathbf{P} mathbf{V}^{-1},$$ . where $ mathbf{V}^{-1}$ is a function of $ alpha_x$ and $ beta_x$ and transforms the ellipse into a circle while preserving the area, and $ mathbf{P}$ is a rotation in phase space according to the phase advance $ mu_x$. Basically, $ mathbf{V}$ turns the parametric oscillator into a harmonic oscillator. . This is a very elegant way to describe the motion with a minimal set of parameters. The question is: can we do something similar for coupled motion, in which the phase space is 4D, not 2D? To start, let&#39;s track a particle in a lattice with a nonzero skew quadrupole coefficient, plotting its phase space coordinates at one position after every period. . &lt;/input&gt; Once Loop Reflect The particle traces interesting donut-like shapes in horizontal ($x$-$x&#39;$) and vertical ($y$-$y&#39;$) phase space instead of ellipses. Below shows the shapes after 1000 periods. . There is definitely more than one frequency present, which we see if we plot the $x$ and $y$ position vs period number and take the FFT. . This is typical of a coupled oscillator. Such systems are typically understood as the superposition of normal modes, each of which corresponds to a single frequency. For example, consider two masses connected with a spring. There are two possible ways for the masses to oscillate at the same frequency. The first is a breathing mode in which they move in opposite directions, and the second is a sloshing mode in which they move in the same direction. The motion is simply the sum of these two modes. We will try to do something similar for a coupled parameteric oscillator. . Solution . Transfer matrix eigenvectors . If the phase space coordinate vector $ mathbf{x} = (x, x&#39;, y, y&#39;)^T$ evolves according to . $$ mathbf{x} rightarrow mathbf{Mx},$$ . where $ rightarrow$ represents tracking through one period, it can be shown that $ mathbf{M}$ is symplectic due to the Hamiltonian mechanics of the system. A consequence of the symplecticity condition is that $ mathbf{M}$ is fully described by 10 numbers instead of 16. Our method examines the eigenvectors of $ mathbf{M}$: . $$ mathbf{Mv} = e^{-i mu} mathbf{v}.$$ . The symplecticity condition also causes the eigenvalues and eigenvectors come in two complex conjugate pairs; this gives $ mathbf{v}_1$, $ mathbf{v}_2$, $ mu_1$, $ mu_2$ and their complex conjugates. The seemingly complex motion seen in the last animation is greatly simplified when written in terms of the eigenvectors. We can write any cooridinate vector as a linear combination of the real and imaginary components of $ mathbf{v}_1$ and $ mathbf{v}_2$: . $$ mathbf{x} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i psi_1} + sqrt{ epsilon_2} mathbf{v}_2e^{-i psi_2} right).$$ . We&#39;ve introduced two initial amplitudes ($ epsilon_1$ and $ epsilon_2$) as well as two initial phases ($ psi_1$ and $ psi_2$). Applying the transfer matrix then simply tacks on a phase. Thus, what we are observing are the 2D projections of the real components of these eigenvectors as they rotate in the complex plane. . $$ mathbf{Mx} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i left( psi_1 + mu_1 right)} + sqrt{ epsilon_2} mathbf{v}_2e^{-i( psi_2 + mu_2)} right).$$ . Let&#39;s replay the animation, but this time draw a red arrow for $ mathbf{v}_1$ and a blue arrow for $ mathbf{v}_2$. We&#39;ve chosen $ epsilon_1 = 4 epsilon_2$ and $ psi_2 - psi_1 = pi/2$. . &lt;/input&gt; Once Loop Reflect That really simplifies things! Each eigenvector simply rotates at its frequency $ mu_l$. It also explains why the amplitude in the $x$-$x&#39;$ and $y$-$y&#39;$ planes trade back and forth: it is because the projections of the eigenvectors rotate at different frequencies, sometimes aligning and sometimes anti-aligning. Because of this, the previous invariants $ epsilon_x$ and $ epsilon_y$ are replaced by $ epsilon_1$ and $ epsilon_2$ as the invariants. It is helpful to think of a torus (shown below). The two amplitudes would determine the inner and outer radii of the torus, and the two phases determine the location of a particle on the surface. . . Credit: Wikipedia Parameterization of eigenvectors . We are now going to introduce a set of parameters for these eigenvectors, and in turn the transfer matrix. We already have two phases, so that leaves 8 parameters. Our strategy is to observe that each eigenvector traces an ellipse in both horizontal ($x$-$x&#39;$) and vertical ($y$-$y&#39;$) phase space. Then, we will simply assign an $ alpha$ function and $ beta$ function to each of these ellipses. So, for the ellipse traced by $ mathbf{v}_1$ in the $x$-$x&#39;$ plane, we have $ beta_{1x}$ and $ alpha_{1x}$, and then for the second eigenvector we have $ beta_{2x}$ and $ alpha_{2x}$. The same thing goes for the vertical dimension with $x$ replaced by $y$. . . The actual eigenvectors written in terms of the parameters are . $$ vec{v}_1 = begin{bmatrix} sqrt{ beta_{1x}} - frac{ alpha_{1x} + i(1-u)}{ sqrt{ beta_{1x}}} sqrt{ beta_{1y}}e^{i nu_1} - frac{ alpha_{1y} + iu}{ sqrt{ beta_{1y}}} e^{i nu_1} end{bmatrix}, quad vec{v}_2 = begin{bmatrix} sqrt{ beta_{2x}}e^{i nu_2} - frac{ alpha_{2x} + iu}{ sqrt{ beta_{2x}}}e^{i nu_2} sqrt{ beta_{2y}} - frac{ alpha_{2y} + i(1-u)}{ sqrt{ beta_{2y}}} end{bmatrix}$$ . So in addition to the phases $ mu_1$ and $ mu_2$ we have $ alpha_{1x}$, $ alpha_{2x}$, $ alpha_{1y}$, $ alpha_{2y}$, $ beta_{1x}$, $ beta_{2x}$, $ beta_{1y}$, and $ beta_{2y}$. That&#39;s pretty much it. There are a few other parameters we need to introduce to simplify the notation, but they are not independent. The first is $u$, which, as noted in the figure, determines the areas of the ellipses in one plane relative to the other. The second and third are $ nu_1$ and $ nu_2$, which are phase differences between the $x$ and $y$ components of the eigenvectors (in the animation they are either $0$ or $ pi$). I won&#39;t discuss these here. The last thing to note is that the parameters reduce to their 1D definitions when there is no coupling in the lattice. So we would have $ beta_{1x}, beta_{2y} rightarrow beta_{x}, beta_{y}$ and $ beta_{2x}, beta_{1y} rightarrow 0$, and similar for $ alpha$. The invariants and phase advances would also revert back to their original values: $ epsilon_{1,2} rightarrow epsilon_{x,y}$ and $ mu_{1,2} rightarrow mu_{x,y}$. . Floquet transformation . These eigenvectors can also be used to construct a transformation which removes both the variance in the focusing strength and the coupling between the planes, turning the coupled parametric oscillator into an uncoupled harmonic oscillator. In other words, we seek a matrix $ mathbf{V}$ such that . $$ mathbf{V^{-1} M V} = mathbf{P} = begin{bmatrix} cos{ mu_1} &amp; sin{ mu_1} &amp; 0 &amp; 0 - sin{ mu_1} &amp; cos{ mu_1} &amp; 0 &amp; 0 0 &amp; 0 &amp; cos{ mu_2} &amp; sin{ mu_2} 0 &amp; 0 &amp; - sin{ mu_2} &amp; cos{ mu_2} end{bmatrix} $$We can do this simply by rewriting the following equation (I haven&#39;t yet figured out how to number equations in Jupyter): . $$ mathbf{x} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i psi_1} + sqrt{ epsilon_2} mathbf{v}_2e^{-i psi_2} right)$$ . in matrix form as $ mathbf{x} = mathbf{V} mathbf{x}_n$ with . $$ mathbf{x}_n = begin{bmatrix} sqrt{ epsilon_1} cos{ psi_1} - sqrt{ epsilon_1} sin{ psi_1} sqrt{ epsilon_2} cos{ psi_2} - sqrt{ epsilon_2} sin{ psi_2} end{bmatrix} $$ $$ mathbf{V} = left[{Re( mathbf{v}_1), -Im( mathbf{v}_1), Re( mathbf{v}_2), -Im( mathbf{v}_2)} right]$$ . Let&#39;s observe the motion in these new coordinates $ mathbf{x}_n$. . &lt;/input&gt; Once Loop Reflect The motion is uncoupled after this transformation; i.e., particles move in a circle of area $ varepsilon_1$ in the $x_n$-$x_n&#39;$ plane at frequency $ mu_1$, and in a circle of area $ varepsilon_2$ in the $y_n$-$y_n&#39;$ plane at frequency $ mu_2$. . Conclusion . The method introduced here allows us to describe the evolution of a parametric oscillator using the minimum number of parameters. Our physical motivation was an accelerator lattice with linear, coupled forces, such as when skew quadrupole terms are present in the magnetic fields. There is no agreed upon method to do this among accelerator physicists, but I like (and know) this method the best, and have used it in my research. I&#39;ve left out many details which can be found in the paper by Lebedev and Bogacz. The paper by Ripken is also very helpful. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/differential%20equations/2021/01/25/coupled_parametric_oscillators.html",
            "relUrl": "/physics/accelerators/differential%20equations/2021/01/25/coupled_parametric_oscillators.html",
            "date": " • Jan 25, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Parametric oscillators",
            "content": "This post presents the solution to a general problem: what is the motion of a particle in one dimension (1D) in the presence of time-dependent, linear, periodic forces? This amounts to solving the following equation of motion: . $$ frac{d^2x}{dt^2} + k(t)x = 0,$$ . where $k(t + T) = k(t)$ for some $T$. This is a parametric oscillator, a harmonic oscillator whose physical properties are not static. For example, the oscillations of a pendulum (in the small angle approximation) on the surface of a planet whose gravitational pull varies periodically would be described by the above equation. The solution to this equation was derived by George William Hill in 1886 to study lunar motion, and for this reason it is known as Hill&#39;s equation. It also finds application in areas such as condensed matter physics, quantum optics, and accelerator physics. After setting up the physical problem, we will examine the solutions and discuss their relevance to the last application, accelerator physics. . Problem motivation . Accelerator physics . Particle accelerators are machines which produce groups of charged particles (known as beams), increase their kinetic energy, and guide them to a target. These machines are invaluable to modern scientific research. The most famous examples are colliders, such as the LHC, in which two beams are smashed together to generate fundamental particles. A lesser known fact is that the fields of condensed matter physics, material science, chemistry, and biology also benefit tremendously from accelerators; this is due to the effectiveness of scattering experiments in which the deflection of a beam after colliding with a target is used to learn information about the target. The scattered beam is composed of neutrons in spallation neutron sources such as SNS, electrons in electron scattering facilities such as CEBAF, or photons in synchrotron light sources such as APS. In addition to scientific research, accelerators find use in medicine, particularly for cancer treatment, and also in various industrial applications. . . A large detector at an interaction point in the LHC. There are generally a few beam properties which are very important to experimentalists; in colliders it is the energy and luminosity, in spallation sources it is the intensity, and in light sources it is the brightness. There is thus a constant need to push these parameters to new regions. For example, below is the famous Livingston plot which shows the energy achieved by various machines over the past century. . . Note: vertical axis scale is beam energy needed to produce the center of mass energy by collision with a resting proton (credit: Rasmus Ischebeck). There are many physics issues associated with the optimization of these beam parameters. Accelerator physics is a field of applied physics which studies these issues. The task of the accelerator physicist is to understand, control, and measure the journey of the beam from its creation to its final destination. The difficulty of this task has grown over time; the improvement accelerator performance has brought with it a staggering increase in size and complexity. The construction and operation of modern accelerators generally requires years of planning, thousands of scientists and engineers, and hundreds of millions or even billions of dollars. Despite this complexity, the underlying physics principles are quite simple, and the single particle motion in one of these machines can be understood analytically if a few approximations are made. In the end we will arrive at Hill&#39;s equation. . How to build an accelerator . There are three basic tasks an accelerator has to accomplish. First, it must increase the beam energy (acceleration). Second, it must guide the beam along a predetermined path (steering). Third, it must ensure the beam particles remain close together (focusing). It is helpful to use a coordinate system in which the $s$ axis points along the design trajectory, and the $x$ and $y$ axes defined in the plane transverse to $s$. In this way the motion is broken up into transverse and longitudinal dynamics. . . How are these tasks accomplished? Well, particles are charged, and the force on a point charge in an electromagnetic field is given by . $$ mathbf{F} = q left({ mathbf{E} + mathbf{v} times mathbf{B}} right),$$ . where $q$ is the particle charge, $ mathbf{v}$ is the particle velocity, $ mathbf{E}$ is the electric field, and $ mathbf{B}$ is the magnetic field. An accelerator consists of a series of elements, each with their own $ mathbf{E}$ and $ mathbf{B}$; the collection of these elements is called a lattice. We need to determine which electric and magnetic fields to use. . The first task, acceleration, is not the focus of this post. The remaining tasks, steering and focusing, concern the motion in the transverse plane. $ mathbf{B}$ fields, not $ mathbf{E}$ fields, are used since their effect grows with increased particle velocity. Any transverse magnetic field $ mathbf{B} = (B_x, B_y)^T$ can be written using a multipole expansion . $$B_y + iB_x = B_{ref} sum_{n=1}^{ infty} left({B_n + iA_n} right) left( frac{x + iy}{R_{ref}} right)^{n-1}.$$ . $B_{ref}$ and $R_{ref}$ are a reference field strength and radius, respectively; just consider them to be constants. We then have the normal multiple coefficients $B_n$, and the skew multipole coefficients $A_n$. The field lines corresponding to the first few normal multipole coefficients are shown below. . . Credit: Jeff Holmes The dipole term is perfect for steering. The field is constant in magnitude and direction: . $$ mathbf{B}_{dipole} propto hat{y},$$ . producing a force which is proportional to the $x$ position: . $$ mathbf{F}_{dipole} propto - hat{x}.$$ . The quadrupole term is used for focusing. The field takes the following form: . $$ mathbf{B}_{quad} propto y hat{x} + x hat{y},$$ . with the resulting force: . $$ mathbf{F}_{quad} propto -x hat{x} + y hat{y}.$$ . The force from the quadrupole is focusing in the horizontal direction, but defocusing in the vertical direction; however, net focusing is still achieved by alternating the direction of the quadrupoles. This is analogous to a beam of light passing through a series of converging and diverging lenses. If the spacing and curvature of the lenses is correctly chosen, a net focusing can be achieved. . . Focusing (QF) and defocusing (QD) quadrupoles modeled as magnetic lenses. The forces which result from these fields are linear, meaning they are proportional the $x$ or $y$ but not $x^2$, $y^3$, etc., and they are uncoupled, meaning the dynamics in the $x$ and $y$ dimensions are independent. Now, we may ask, can we really produce a perfect dipole or quadrupole field? The answer is no. In reality there will always be higher order multipoles present in the field, but people work very hard to ensure these are much smaller than the desired multipole. This video shows a bit of the construction process for these magnets. . Linearized equation of motion . Making the above approximation of perfect dipole and quadrupole magnets, and ignoring all other elements in the machine, we arrive at the equation of motion for a single particle in the transverse plane: . $$x&#39;&#39; + k(s)x = 0,$$ . where $x&#39; = dx/ds$ and $k(s + L) = k(s)$ for some distance $L$. We could also write a similar equation for $y$. It is conventional to use the slope $x&#39;$ instead of the velocity; this allows us to talk about the position of the particle in the lattice instead of the amount of time which has passed. The period length $L$ could be the entire circumference of a circular machine, or could be a smaller repeated subsection. . Solution . Envelope function . The general solution to Hill&#39;s equation is given by . $$x(s) = sqrt{ epsilon} ,w(s) cos left({ mu(s) + delta} right).$$ . This introduces an amplitude $w(s) = w(s + L)$ which we call the envelope function, as well as a phase $ mu$, both of which depend on $s$. The constants $ epsilon$ and $ delta$ are determined by the initial conditions. Let&#39;s plot this trajectory in a FODO (focus-off-defocus-off) lattice, which consists of evenly spaced focusing and defocusing quadrupoles. Here is the focusing strength within the lattice (QF is the focusing quadrupole and QD is the defocusing quadrupole): . . For now we can think of the lattice as repeating itself forever in the $s$ direction. Each black line below is represents the trajectory for a different initial position and slope; although the individual trajectories look rather complicated, the envelope function has a very simple form. . . Phase space . The particle motion becomes much easier to interpret if we observe it in position-momentum space, aka phase space. The following animation shows the evolution of the particle phase space coordinates at a single position in the lattice. The position shown is $s = nL/4$, where $n$ is the period number, which corresponds to the midpoint between the focusing and defocusing quadrupoles. . . &lt;/input&gt; Once Loop Reflect We see that the particle jumps along the boundary of an ellipse in phase space. The shape and orientation of the ellipse will change if we look at a different position in the lattice, but its area will be the same. So, the motion is determined by the dimensions and oriention of this ellipse throughout the lattice, as well as the location of the paricle on the ellipse boundary. This motivates the definition of the so-called Twiss parameters, which were first introduced by Courant and Snyder in 1958: . $$ beta = w^2, quad alpha = - frac{1}{2} beta&#39;, quad gamma = frac{1 + alpha^2}{ beta}.$$ . The dimensions of the phase space ellipse are nicely described by these parameters: . . The maximum extent of the ellipse is determined by $ beta$ in the $x$ direction and $ gamma$ in the $y$ direction. $ alpha$ is proportional to the slope of the $ beta$ function, and so determines the tilt angle of the ellipse. The position of a particle on the ellipse is given by the phase $ mu$. Finally, the invariant of the motion corresponding to the ellipse area is constructed from the Twiss parameters as . $$ epsilon = beta {x&#39;}^2 + 2 alpha xx&#39; + gamma x^2$$ . for any $x$ and $x&#39;$. The $ beta$ functions and phase advances in both dimensions are extremely important to measure and control in a real machine. Here is an example of the horizontal and vertical $ beta$ functions in the SNS accumulator ring. . . Transfer matrices . A helpful tool to pair with the parameterization we just introduced is the transfer matrix, a matrix which connects the phase space coordinates at two different positions: . $$ begin{bmatrix} x x&#39; end{bmatrix}_{s + L} = mathbf{M} begin{bmatrix} x x&#39; end{bmatrix}_{s}$$ . The transfer matrix can be written as $ mathbf{M} = mathbf{V} mathbf{P} mathbf{V}^{-1}$, where . $$ mathbf{V} = frac{1}{ sqrt{ beta}} begin{bmatrix} beta &amp; 0 - alpha &amp; 1 end{bmatrix}$$ and $$ mathbf{P} = begin{bmatrix} cos mu &amp; sin mu - sin mu &amp; cos mu end{bmatrix} $$ . The effect of $ mathbf{V}^{-1}$ is to deform the phase space ellipse into a circle while preserving its area. $ mathbf{P}$ is then just a rotation in phase space, and $ mathbf{V}$ then transforms back into a tilted ellipse. This is illustrated below. . . $ mathbf{V}$ can be thought of as a time-dependent transformation which removes the variance in the focusing strength, turning the parametric oscillator into a simple harmonic oscillator. Often it is called the Floquet transformation. . Conclusion . We&#39;ve presented the solution to Hill&#39;s equation, which describes a parameteric oscillator. The equation pops up in multiple areas, but we focused on its application in accelerator physics, in which Hill&#39;s equation describes the transverse motion of a single particle in an accelerator with perfectly linear magnetic fields. . The solution is best understood geometrically: particles move around the surface of an ellipse in phase space, the area of which is an invariant of the motion. The dimensions and orientation of the ellipse are determined by $ alpha$ and $ beta$, and the location of the paricle on the ellipse boundary is determined by $ mu$. These parameters can be used to construct a time-dependent transformation ($ mathbf{V}$) which turns the parametric oscillator into a simple harmonic oscillator. . The next post will examine how this treatment can be extended to include coupling between the horizontal and vertical dimensions. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/differential%20equations/2021/01/21/parametric_oscillators.html",
            "relUrl": "/physics/accelerators/differential%20equations/2021/01/21/parametric_oscillators.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "References",
            "content": "The following are some references which I think are helpful. Any suggestions are encouraged in the comments. . Accelerator physics . Accelerator Physics — Lee | Introduction to the Physics of High Energy Accelerators — Edwards/Syphers | Measurement and Control of Charged Particle Beams — Minty/Zimmerman | Particle Accelerator Physics — Wiedemann | Space Charge Physics for Particle Accelerators — Hofmann | . Machine learning . Neural Networks and Deep Learning — Nielsen | Pattern Recognition and Machine Learning — Bishop | . Mathematics . Mathematics of Classical and Quantum Physics — Byron/Fuller | Mathematical Methods for Physicists — Arfken/Weber | . Physics core . Classical Mechanics — Taylor | Classical Mechanics — Goldstein | Introduction to Electrodynamics — Griffiths | Introduction to Thermal Physics — Schroeder | Quantum Mechanics: Concepts and Applications — Zettili | Statistical Physics of Particles — Kardar | . Programming . Algorithms — Sedgewick/Wayne | Elements of Programming Interviews in Python — Aziz/Lee/Prakash | .",
            "url": "https://austin-hoover.github.io/blog/1900/01/01/references.html",
            "relUrl": "/1900/01/01/references.html",
            "date": " • Jan 1, 1900"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This blog is a space for me to write about technical things. Most of the time, I’ll focus on my research in accelerator physics, but I’m also interested in other topics such as machine learning. Please see my homepage for my bio, publications, etc. .",
          "url": "https://austin-hoover.github.io/blog/_pages/about.html",
          "relUrl": "/_pages/about.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://austin-hoover.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}