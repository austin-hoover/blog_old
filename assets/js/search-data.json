{
  
    
        "post0": {
            "title": "Particle-in-cell simulation",
            "content": "In my research I utilize PyORBIT, one of many simulation codes for beam physics. One of the key components of the these simulations is the inclusion of the electromagnetic interactions between particles in the beam; these are known as space charge forces, which refers to the charge density of the beam in free space. This post examines the particle-in-cell (PIC) algorithm used to compute space charge forces by building a simple simulation engine in Python. Links to relevant books and lectures are included at the end. . Theoretical model . We&#39;ll use bunch to refer to a group of particles in three-dimensional (3D) space, and we&#39;ll use a local cartesian coordinate system whose origin moves with the center of the bunch as shown below: . . The $s$ coordinate specifies the position of the bunch in the accelerator, and the path can be curved. Now for a few assumptions and approximations. First, the center of the bunch is moving at a constant velocity $ beta c$, where $c$ is the speed of light, so the time variable can be replaced by the position $s = beta c t$. The slope $x&#39; = dx/ds$ can be used instead of the velocity, and is measured in milliradians based on the small angle approximation. Second, we&#39;ll assume the transverse ($x$-$y$) size of the bunch is much smaller than its length, so that for a given transverse slice it is as if the bunch had a uniform density and infinite length in the longitudinal direction. Our focus will be on the transverse dynamics of this slice, as shown below, so each tracked &quot;particle&quot; will really be an infinite line of charge; any dynamics in the longitudinal direction will be ignored. . . Credit: G. Franchetti Another approximation is to neglect any magnetic fields generated by the beam, again valid if the transverse momenta are very small compared to the bunch kinetic energy. All this being said, the equations of motion without any external foces, i.e., in free space, can be written as . $$ mathbf{x}&#39;&#39; = frac{q}{mv_s gamma^3} mathbf{E}, tag{1}$$ . where $ mathbf{x} = [x, y]^T$ is the coordinate vector, $ mathbf{E} = [E_x, E_y]^T$ is the self-generated electric field, $m$ is the particle mass, and $v_s$ is the velocity in the $s$ direction. Let&#39;s first address the factor $ gamma^{-3}$ in the equation of motion, which means that the space charge force goes to zero as the velocity approaches the speed of light. This is because parallel moving charges generate an attractive magnetic force which grows with velocity, completely cancelling the electric force in the limit $v rightarrow c$. . . Credit: OpenStax University PhysicsOne may ask: what about the rest frame, in which there is no magnetic field? But special relativity says that electrogmagnetic fields change with reference frame. Using the transformations defined here, you can quickly prove that . $$ mathbf{E}_{lab} = frac{ mathbf{E}_{rest}}{ gamma}. tag{2}$$ . This inverse relationship between velocity and the space charge force has real-life consequences. It tells us that space charge is important if 1) the beam is very intense, meaning there are many particles in a small area, or 2) the beam is very energetic, meaning it is moving extremely fast. For example, space charge can usually be ignored in electron beams, which move near the speed of light for very modest energies due to their tiny mass, but is significant in high-intensity, low-energy hadron accelerators such as FRIB, SNS, and ESS. . We should now address the difficulty in determining the evolution of this system: the force on a particle in an $n$-particle bunch depends on the positions of the other $n - 1$ particles. The approach of statistical mechanics to this problem is to introduce a distribution function $f( mathbf{x}, mathbf{x}&#39;, t)$ which gives the number of particles in an infinitesimal volume of phase space. The Vlasov-Poisson system of equations determines the evolution of $f$ (as long as we ignore collisions between particles and any magnetic fields): . $$ frac{ partial{f}}{ partial{s}} + mathbf{x}&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}}} + mathbf{x}&#39;&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}&#39;}}. tag{3}$$ . We know $ mathbf{x&#39;&#39;}$ from Eq. (1). The electric field is obtained from Poisson&#39;s equation: . $$ nabla cdot mathbf{E} = - nabla^2 phi = frac{ rho}{ varepsilon_0}. tag{4}$$ . Finally, the transverse charge density $ rho$ is determined by . $$ rho = q int{f dx&#39;dy&#39;}. tag{5}$$ . Although these equations are easy to write down, they are generally impossible to solve analytically. We need to turn to a computer for help. . Computational method . The Vlasov equation could be solved directly, but this is difficult, especially in 2D or 3D. On the other end of the spectrum, the notion of a fluid in phase space could be abandonded and each particle could be tracked individually, computing the forces using direct sums. But this is infeasible with current hardware; the time complexity would by $O(n^2)$, where $n$ is the number of particles, and $n$ may be on the order of $10^{14}$. The particle-in-cell (PIC) method is a sort of combination of these two approaches. The idea is to track a group of macroparticles according to Eq. (1), each of which represents a large number of real particles. The fields, however, are solved from Eq. (4). The key step is transforming back and forth between a discrete and continuous representation of the bunch. The simulation loop for the PIC method is shown below. . . In the next sections I will discuss each of these steps and implement them in Python code. Let&#39;s first create a Bunch class, which is a simple container for the bunch coordinates. (Note: there are some constants and helper functions I&#39;ve imported that I don&#39;t define. The whole package is posted on GitHub.) . import numpy as np class Bunch: &quot;&quot;&quot;Container for 2D distribution of particles. Attributes - intensity : float Number of physical particles in the bunch. Default 1.5e14. length : float Length of the bunch [m]. Default: 250. mass, charge, kin_energy : float Mass [GeV/c^2], charge [C], and kinetic energy [GeV] per particle. line_density : float Longitudinal particle density [1 / m]. Default 1.5e14 / 250. line_charge_density : float Longitudinal charge density [C / m]. nparts : float Number of macroparticles in the bunch. macrosize : float Number of physical particles represented by each macroparticle. macrocharge : float Charge represented by each macroparticle [C]. perveance : float Dimensionless space charge perveance. sc_factor : float Factor for space charge kicks such that that x&#39;&#39; = factor * Ex. X : ndarray, shape (nparts, 4) Array of particle coordinates. Columns are [x, x&#39;, y, y&#39;]. Units are meters and radians. positions : ndarray, shape (nparts, 2): Just the x and y positions (for convenience). &quot;&quot;&quot; def __init__(self, intensity=1e14, length=250., mass=0.938, kin_energy=1., charge=elementary_charge): self.intensity, self.length = intensity, length self.mass, self.kin_energy, self.charge = mass, kin_energy, charge self.gamma = 1 + (self.kin_energy / self.mass) self.beta = np.sqrt(1 - (1 / self.gamma)**2) self.nparts = 0 self.line_density = intensity / length self.line_charge_density = charge * self.line_density self.perveance = get_perveance(self.line_density, self.beta, self.gamma) self.sc_factor = get_sc_factor(charge, mass, self.beta, self.gamma) self.compute_macrosize() self.X, self.positions = None, None def compute_macrosize(self): &quot;&quot;&quot;Update the macrosize and macrocharge.&quot;&quot;&quot; if self.nparts &gt; 0: self.macrosize = self.intensity // self.nparts else: self.macrosize = 0 self.macrocharge = self.charge * self.macrosize def fill(self, X): &quot;&quot;&quot;Fill with particles.&quot;&quot;&quot; self.X = X if self.X is None else np.vstack([self.X, X]) self.positions = self.X[:, [0, 2]] self.nparts = self.X.shape[0] self.compute_macrosize() def compute_extremum(self): &quot;&quot;&quot;Get extreme x and y coorinates.&quot;&quot;&quot; self.xmin, self.ymin = np.min(self.positions, axis=0) self.xmax, self.ymax = np.max(self.positions, axis=0) self.xlim, self.ylim = (self.xmin, self.xmax), (self.ymin, self.ymax) . Weighting . Starting from a group of macroparticles, we need to produce a charge density $ rho_{i,j}$ on a grid. The most simple approach is the nearest grid point (NGP) method, which, as the name suggests, assigns the full particle charge to the closest grid point. This is commonly called zero-order weighting; although it is very fast and easy to implement, it is not commonly used because it can lead to significant noise. A better method called cloud-in-cell (CIC) treats each particle as a rectangular, uniform density cloud of charge with dimensions equal to the grid spacing. A fractional part of the charge is assigned based on the fraction of the cloud overlapping with a given cell. This can be thought of as first-order weighting. To get a sense of what these methods are doing (in 1D), we can slide a particle across a cell and plot the resulting density of the cell at each position, thus giving an effective particle shape. . The NGP method leads to a discontinuous boundary, while the CIC method leads to a continous boundary (but discontinous derivative). There are also higher order methods which lead to a smooth boundary, but I don&#39;t cover those here. . We also need to perform the inverse operation: given the electric field at each grid point, interpolate the value at each particle position. The same method applies here. NGP just uses the electric field at the nearest grid point, while CIC weights the four nearest grid points. The following Grid class implements the CIC method. Notice that I utilized Cython in the for-loop in the distribute method. I couldn&#39;t figure out a way to perform the operation with this loop, and in pure Python it took about 90% of the runtime for a single simulation step. Using Cython gave a significant performance boost. . %%cython import numpy as np from scipy.interpolate import RegularGridInterpolator class Grid: &quot;&quot;&quot;Class for 2D grid. Attributes - xmin, ymin, xmax, ymax : float Minimum and maximum coordinates. Nx, Ny : int Number of grid points. dx, dy : int Spacing between grid points. x, y : ndarray, shape (Nx,) or (Ny,) Positions of each grid point. cell_area : float Area of each cell. &quot;&quot;&quot; def __init__(self, xlim=(-1, 1), ylim=(-1, 1), size=(64, 64)): self.xlim, self.ylim = xlim, ylim (self.xmin, self.xmax), (self.ymin, self.ymax) = xlim, ylim self.size = size self.Nx, self.Ny = size self.dx = (self.xmax - self.xmin) / (self.Nx - 1) self.dy = (self.ymax - self.ymin) / (self.Ny - 1) self.cell_area = self.dx * self.dy self.x = np.linspace(self.xmin, self.xmax, self.Nx) self.y = np.linspace(self.ymin, self.ymax, self.Ny) def set_lims(self, xlim, ylim): &quot;&quot;&quot;Set the min and max grid coordinates.&quot;&quot;&quot; self.__init__(xlim, ylim, self.size) def zeros(self): &quot;&quot;&quot;Create array of zeros with same size as the grid.&quot;&quot;&quot; return np.zeros((self.size)) def distribute(self, positions): &quot;&quot;&quot;Distribute points on the grid using the cloud-in-cell (CIC) method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. Returns - rho : ndarray, shape (Nx, Ny) Density at each grid point. &quot;&quot;&quot; # Compute area overlapping with 4 nearest neighbors ivals = np.floor((positions[:, 0] - self.xmin) / self.dx).astype(int) jvals = np.floor((positions[:, 1] - self.ymin) / self.dy).astype(int) ivals[ivals &gt; self.Nx - 2] = self.Nx - 2 jvals[jvals &gt; self.Ny - 2] = self.Ny - 2 x_i, x_ip1 = self.x[ivals], self.x[ivals + 1] y_j, y_jp1 = self.y[jvals], self.y[jvals + 1] _A1 = (positions[:, 0] - x_i) * (positions[:, 1] - y_j) _A2 = (x_ip1 - positions[:, 0]) * (positions[:, 1] - y_j) _A3 = (positions[:, 0] - x_i) * (y_jp1 - positions[:, 1]) _A4 = (x_ip1 - positions[:, 0]) * (y_jp1 - positions[:, 1]) # Distribute areas for each point rho = self.zeros() cdef double[:, :] rho_view = rho cdef int i, j for i, j, A1, A2, A3, A4 in zip(ivals, jvals, _A1, _A2, _A3, _A4): rho_view[i, j] += A4 rho_view[i + 1, j] += A3 rho_view[i, j + 1] += A2 rho_view[i + 1, j + 1] += A1 return rho / self.cell_area def interpolate(self, grid_vals, positions): &quot;&quot;&quot;Interpolate values from the grid using the CIC method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. grid_vals : ndarray, shape (n, 2) Scalar value at each coordinate point. Returns - int_vals : ndarray, shape (nparts,) Interpolated value at each position. &quot;&quot;&quot; int_func = RegularGridInterpolator((self.x, self.y), grid_vals) return int_func(positions) def gradient(self, grid_vals): &quot;&quot;&quot;Compute gradient using 2nd order centered differencing. Parameters - grid_vals : ndarray, shape (Nx, Ny) Scalar values at each grid point. neg : Bool If True, return the negative of the gradient. Returns - gradx, grady : ndarray, shape (Nx, Ny) The x and y gradient at each grid point. &quot;&quot;&quot; return np.gradient(grid_vals, self.dx, self.dy) . It should be mentioned that the field interpolation method should be the same as the charge deposition method; if this is not true, it is possible for a particle to exert a force on itself! Let&#39;s test the method on a gaussian distribution of 100,000 macroparticles in the $x$-$y$ plane, truncated at three standard devations. We&#39;ll choose the number of grid points to be $N_x = N_y = 64$. I call a DistGenerator class that I don&#39;t show here. . dg = DistGenerator() bunch = Bunch() bunch.fill(dg.generate(kind=&#39;gauss&#39;, nparts=int(1e5), cut=3.)) bunch.compute_extremum() grid = Grid(bunch.xlim, bunch.ylim, size=(64, 64)) rho = bunch.line_density * grid.distribute(bunch.positions) . Field solver . The workhorse in the simulation loop is the field solver. We need to solve Poisson&#39;s equation: . $$ left({ frac{ partial^2}{ partial x^2} + frac{ partial^2}{ partial y^2}} right) = - frac{ rho left(x, y right)}{ varepsilon_0}. tag{6}$$ . The discretized version of the equation reads . $$ frac{ phi_{i+1,j} + -2 phi_{i,j} + phi_{i-1,j}}{{ Delta_x}^2} + frac{ phi_{i,j+1} + -2 phi_{i,j} + phi_{i,j-1}}{{ Delta_y}^2} = - frac{ rho_{i,j}}{ varepsilon_0} tag{7}$$ . for a grid with spacing $ Delta_x$ and $ Delta_y$. There are multiple paths to a solution; we will focus on the method implemented in PyORBIT, which utilizes the Fourier convolution theorem. . Convolution theorem . The potential from an infinite line charge at the origin with charge density $ lambda$ is . $$ phi( mathbf{x}) = - frac{ lambda}{2 pi varepsilon_0} ln{| mathbf{x}|} = - frac{ lambda}{2 pi varepsilon_0} int{ ln{| mathbf{x} - mathbf{q}|} delta( mathbf{q})d mathbf{q}}. tag{8}$$ . Note that $ mathbf{q}$ is just a dummy variable; usually a prime is used, but we already assigned physical meaning to $ mathbf{x}&#39;$. By defining $G( mathbf{x} - mathbf{q}) = - frac{ lambda}{2 pi varepsilon_0} ln{| mathbf{x} - mathbf{q}|}$ and $ rho( mathbf{x}) = delta( mathbf{x})$, we have . $$ phi( mathbf{x}) = int{G( mathbf{x} - mathbf{q}) rho( mathbf{q})d mathbf{q}} = G( mathbf{x}) * rho( mathbf{x}). tag{9}$$ . In this form the potential is a convolution (represented by $*$) of the charge density $ rho$ with $G$, which is called the Green&#39;s function. On the grid this will look like . $$ phi_{i, j} = sum_{k,l ne i,j}{G_{i-k, j-l} rho_{k, l}}. tag{11}$$ . This solves the problem in $O(N^2)$ time complexity for $N$ grid points. This is already much faster than a direct force calculation but could still get expensive for fine grids. We can speed things up by exploiting the convolution theorem, which says that the Fourier transform of a convolution of two functions is equal to the product of their Fourier transforms. The Fourier transform is defined by . $$ hat{ phi}( mathbf{k})= mathcal{F} left[ phi( mathbf{x}) right] = int_{- infty}^{ infty}{e^{- mathbf{k} cdot mathbf{x}} phi( mathbf{x}) d mathbf{x}}. tag{12}$$ . The convolution theorem then says $$ mathcal{F} left[ rho * G right] = mathcal{F} left[ rho right] cdot mathcal{F} left[G right]. tag{13}$$ . For the discrete equation this gives . $$ hat{ phi}_{n, m} = hat{ rho}_{n, m} hat{G}_{n, m}, tag{14}$$ . where the hat represents the discrete Fourier transform. With the FFT algorithm at our disposal, the time complexity can be reduced to $O left(N log N right)$. . Implementation . There is a caveat to this method: to use the FFT algorithm, Eq. (11) must be a circular convolution, which means $G$ must be periodic. But the beam is in free space (we&#39;ve neglected any conducting boundary), so this is not true. We can make it true by doubling the grid size in each dimension. We then make $G$ a mirror reflection in the new quadrants so that it is periodic, and also set the charge density equal to zero in these regions. After running the method on this larger grid, the potential in the new quadrants will be unphysical; however, the potential in the original quadrant will be correct. There are also some tricks we can play to reduce the space complexity, and in the end doubling the grid size is not much of a price to pay for the gain in speed. The method is implemented in the PoissonSolver class. . from scipy.fft import fft2, ifft2 class PoissonSolver: &quot;&quot;&quot;Class to solve Poisson&#39;s equation on a 2D grid. Attributes - rho, phi, G : ndarray, shape (2*Nx, 2*Ny) Charge density (rho), potential (phi), and Green&#39;s function (G) at each grid point on a doubled grid. Only one quadrant (i &lt; Nx, j &lt; Ny) corresponds to to the real potential. &quot;&quot;&quot; def __init__(self, grid, sign=-1.): self.grid = grid new_shape = (2 * self.grid.Nx, 2 * self.grid.Ny) self.rho, self.G = np.zeros(new_shape), np.zeros(new_shape) self.phi = np.zeros(new_shape) def set_grid(self, grid): self.__init__(grid) def compute_greens_function(self, line_charge_density): &quot;&quot;&quot;Compute Green&#39;s function on doubled grid.&quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny Y, X = np.meshgrid(self.grid.x - self.grid.xmin, self.grid.y - self.grid.ymin) self.G[:Nx, :Ny] = np.log(X**2 + Y**2, out=np.zeros_like(X), where=(X + Y &gt; 0)) self.G *= -0.5 * line_charge_density / (2 * pi * epsilon_0) self.G[Nx:, :] = np.flip(self.G[:Nx, :], axis=0) self.G[:, Ny:] = np.flip(self.G[:, :Ny], axis=1) def get_potential(self, rho, line_charge_density): &quot;&quot;&quot;Compute the electric potential on the grid. Parameters - rho : ndarray, shape (Nx, Ny) Charge density at each grid point. line_charge_density : float Longitudinal charge density. Returns - phi : ndarray, shape (Nx, Ny) Electric potential at each grid point. &quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny self.rho[:Nx, :Ny] = rho self.compute_greens_function(line_charge_density) self.phi = ifft2(fft2(self.G) * fft2(self.rho)).real return self.phi[:Nx, :Ny] . Running the algorithm gives the following potential on the doubled grid: . solver = PoissonSolver(grid) phi = solver.get_potential(rho, bunch.line_charge_density) . We can then compute the electric field at every grid point using second-order centered differencing. This gives . $$(E_x)_{i,j} = - frac{ phi_{i+1,j} - phi_{i-1,j}}{2 Delta_x}, tag{15}$$ . $$(E_y)_{i,j} = - frac{ phi_{i,j+1} - phi_{i,j-1}}{2 Delta_y}. tag{16}$$ . Ex, Ey = grid.gradient(-phi) . Finally, the value of the electric field at each particle position can be interpolated from the grid. . Ex_int = grid.interpolate(Ex, bunch.positions) Ey_int = grid.interpolate(Ey, bunch.positions) . Particle mover . All we need to do in this step is integrate the equations of motion. A common method is leapfrog integration in which the position and velocity are integrated out of phase as follows: . $$ m left( frac{ mathbf{v}_{i+1/2} - mathbf{v}_{i-1/2}}{ Delta_t} right) = mathbf{F}( mathbf{x}_i),$$ . $$ frac{ mathbf{x}_{i+1} - mathbf{x}_i}{ Delta_t} = mathbf{v}_{i+1/2}$$ . . Credit: S. LundA different scheme must be used when velocity-dependent forces are present. This is a symplectic integrator, which means it conserves energy. It is also second-order accurate, meaning that its error is proportional to the square of the $ Delta_t$. Finally, it is time-reversible. The only complication is that, because the velocity and position are out of phase, we need to push the velocity back one half-step before starting the simulation, and push it one half-step forward when taking a measurement. . Putting it all together . Simulation loop . We have all the tools to implement the simulation loop. While $s &lt; s_{max}$ we: . Compute the charge density on the grid. | Find the electric potential on the grid. | Interpolate the electric field at the particle positions. | Update the particle positions. | We&#39;ll first create a History class which stores the beam moments or phase space coordinates. . class History: &quot;&quot;&quot;Class to store bunch data over time. Atributes moments : list Second-order bunch moments. Each element is ndarray of shape (10,). coords : list Bunch coordinate arrays. Each element is ndarray of shape (nparts, 4) moment_positions, coord_positions : list Positions corresponding to each element of `moments` or `coords`. &quot;&quot;&quot; def __init__(self, bunch, samples=None): self.X = bunch.X self.moments, self.coords = [], [] self.moment_positions, self.coord_positions = [], [] if samples is None or samples &gt;= bunch.nparts: self.idx = np.arange(bunch.nparts) else: self.idx = np.random.choice(bunch.nparts, samples, replace=False) def store_moments(self, s): Sigma = np.cov(self.X.T) self.moments.append(Sigma[np.triu_indices(4)]) self.moment_positions.append(s) def store_coords(self, s): self.coords.append(np.copy(self.X[self.idx, :])) self.coord_positions.append(s) def package(self, mm_mrad): self.moments = np.array(self.moments) self.coords = np.array(self.coords) if mm_mrad: self.moments *= 1e6 self.coords *= 1e3 . Now we&#39;ll create a Simulation class. . class Simulation: &quot;&quot;&quot;Class to simulate the evolution of a charged particle bunch. Attributes - bunch : Bunch: The bunch to track. length : float Total tracking distance [m]. step_size : float Distance between force calculations [m]. nsteps : float Total number of steps = int(length / ds). steps_performed : int Number of steps performed so far. s : float Current bunch position. positions : ndarray, shape (nsteps + 1,) Positions at which coordinates are updated. history : History object Object storing bunch data at each position. meas_every : dict or int Keys should be &#39;moments&#39; and &#39;coords&#39;. Values correspond to the number of simulations steps between storing these quantities. For example, `meas_every = {&#39;coords&#39;:4, &#39;moments&#39;:2}` will store the moments every 4 steps and the moments every other step. If an int is provided, this will be applied to both. Defaults to start and end of simulation. samples : int Number of bunch particles to store when measuring phase space coordinates. Defaults to the entire coordinate array. mm_mrad : bool If True, use units of mm-mrad. Otherwise use m-rad. ext_foc : callable Function returning external kx and ky at the current position such that u&#39;&#39; = -ku. Call signature is `kx, ky = ext_foc(s)`. &quot;&quot;&quot; def __init__(self, bunch, length, step_size, grid_size, meas_every={}, samples=None, mm_mrad=True, ext_foc=None): self.bunch = bunch self.length, self.ds = length, step_size self.nsteps = int(length / step_size) self.positions = np.linspace(0, length, self.nsteps + 1) self.grid = Grid(size=grid_size) self.solver = PoissonSolver(self.grid) self.fields = np.zeros((bunch.nparts, 2)) self.history = History(bunch, samples) self.ext_foc = ext_foc if type(meas_every) is int: meas_every = {&#39;moments&#39;: meas_every, &#39;coords&#39;:meas_every} meas_every.setdefault(&#39;moments&#39;, self.nsteps) meas_every.setdefault(&#39;coords&#39;, self.nsteps) for key in meas_every.keys(): if meas_every[key] is None: meas_every[key] = self.nsteps self.meas_every = (meas_every[&#39;moments&#39;], meas_every[&#39;coords&#39;]) self.mm_mrad = mm_mrad self.s, self.steps_performed = 0.0, 0 def set_grid(self): &quot;&quot;&quot;Determine grid limits.&quot;&quot;&quot; self.bunch.compute_extremum() self.grid.set_lims(self.bunch.xlim, self.bunch.ylim) self.solver.set_grid(self.grid) def compute_electric_field(self): &quot;&quot;&quot;Compute the self-generated electric field.&quot;&quot;&quot; self.set_grid() rho = self.grid.distribute(self.bunch.positions) rho *= self.bunch.line_charge_density * 4 # unknown origin phi = self.solver.get_potential(rho, self.bunch.line_charge_density) Ex, Ey = self.grid.gradient(-phi) self.fields[:, 0] = self.grid.interpolate(Ex, self.bunch.positions) self.fields[:, 1] = self.grid.interpolate(Ey, self.bunch.positions) def kick(self, ds): &quot;&quot;&quot;Update particle slopes.&quot;&quot;&quot; # Space charge dxp_ds = self.bunch.sc_factor * self.fields[:, 0] dyp_ds = self.bunch.sc_factor * self.fields[:, 1] # External forces if self.ext_foc is not None: kx, ky = self.ext_foc(self.s) dxp_ds -= kx * self.bunch.X[:, 0] dyp_ds -= ky * self.bunch.X[:, 2] self.bunch.X[:, 1] += dxp_ds * ds self.bunch.X[:, 3] += dyp_ds * ds def push(self, ds): &quot;&quot;&quot;Update particle positions.&quot;&quot;&quot; self.bunch.X[:, 0] += self.bunch.X[:, 1] * ds self.bunch.X[:, 2] += self.bunch.X[:, 3] * ds def store(self): &quot;&quot;&quot;Store bunch coordinates or statistics.&quot;&quot;&quot; store_moments = self.steps_performed % self.meas_every[0] == 0 store_coords = self.steps_performed % self.meas_every[1] == 0 if not (store_moments or store_coords): return Xp = np.copy(self.bunch.X[:, [1, 3]]) self.kick(+0.5 * self.ds) # sync positions/slopes if store_moments: self.history.store_moments(self.s) if store_coords: self.history.store_coords(self.s) self.bunch.X[:, [1, 3]] = Xp def run(self): &quot;&quot;&quot;Run the simulation.&quot;&quot;&quot; self.compute_electric_field() self.store() self.kick(-0.5 * self.ds) # desync positions/slopes for i in trange(self.nsteps): self.compute_electric_field() self.kick(self.ds) self.push(self.ds) self.s += self.ds self.steps_performed += 1 self.store() self.history.package(self.mm_mrad) . Demonstration . To demonstrate our method, we need some way of checking its accuracy. Real bunch measurements will be noisy and infrequent along the accelerator, so this could be a pain. Probably the way to go is to compare with other well-tested codes. Alternatively, there is an analytic benchmark available: the Kapchinskij-Vladimirskij (KV) distribution. Without going into any detail, the beam projects to a uniform density ellipse in the $x$-$y$ plane, and the space charge forces produced within this ellipse are linear (in general space charge forces are nonlinear). If we plug the KV distribution into the Vlasov equation, it can be seen that these forces will remain linear for all time if the external focusing forces are also linear. As a consequence, a set of self-consistent differential equations describing the ellipse boundary can be written down. This is a remarkable fact. For now I won&#39;t show these equations; I&#39;ll just integrate them behind the scenes and compare with the calculations. . Now for the simulation. We&#39;ll look at two situations: 1) drift space and 2) FODO lattice. 1) is a region in which there are no external fields, so only the bunch&#39;s electric field is present. 2) is one of the basic building blocks of an accelerator, consisting of a series of alternating focusing and defocusing quadrupoles. This was covered in a previous post. Some care must be taken in the choice of simulation parameters; we need a fine enough grid to resolve the hard edge of the beam, enough macroparticles per grid cell to collect good statistics. I found the accuracy did depend on the choices here, and I sort of just played around with the numbers until things looked okay. Below are the simulation parameters we will use. . nparts = int(1e5) bunch_length = 250.0 # [m], circumference of SNS accumulator ring intensity = 40e14 # 25 times that of SNS to show change over short distance step_size = 0.025 # [m] grid_size = (128, 128) . Let&#39;s first track through a 10 meter drift. We&#39;ll compare the KV distribution to the envelope calculation and also to the Gaussian distribution. . def create_bunches(intensity, bunch_length, nparts): bunches = {} for kind in [&#39;kv&#39;, &#39;gauss&#39;]: bunches[kind] = Bunch(intensity=intensity, length=bunch_length) bunches[kind].fill(dg.generate(kind, nparts, eps=25e-6, cut=3)) return bunches bunches = create_bunches(intensity, bunch_length, nparts) tracking_distance = 10.0 # [m] emittance = 25e-6 # [m*rad] meas_every = {&#39;moments&#39;: int(0.1 * tracking_distance/step_size), &#39;coords&#39;: 2} sims = {} for kind, bunch in bunches.items(): sims[kind] = Simulation(bunch, tracking_distance, step_size, grid_size, meas_every=meas_every, samples=10000) sims[kind].run() . 100%|██████████| 400/400 [00:41&lt;00:00, 9.54it/s] 100%|██████████| 400/400 [00:40&lt;00:00, 9.90it/s] . Notice that the beam expands even without space charge; this is because there is a distribution of transverse particle velocities in the initial bunch. The accuracy seems fairly good. The runtime is also okay; the equivalent PyORBIT simulation is about 10 times faster (it directly calls C++ routines and doesn&#39;t store the bunch coordinates). Below shows the evolution of a sample of 10,000 macroparicles. . &lt;/input&gt; Once Loop Reflect Now we&#39;ll track through a FODO lattice. The external fields simply add a term proportional to $x$ or $y$ in Eq. (1), so that . $$ x&#39;&#39; + k_x(s)x = frac{q}{mv_s^2 gamma^3} E_x,$$ . and similar for $y$. These fields cause the beam to periodically breath in and out, expanding in one dimension while contracting in the other. We&#39;ll add a quadrupole every 2.5 meters in our 10 meter drift space. . The disagreement with the KV envelope model, although small, seems to grow with time, specifically in the horizontal direction. I&#39;m not totally sure why this is; it could be that we need more macroparticles, or a different grid spacing, or maybe there is a bug in my code. I don&#39;t have the the time or need to investigate further. It is at least somewhat accurate. Here again is the evolution of the bunch in the $x$-$y$ plane. . &lt;/input&gt; Once Loop Reflect Conclusion . This post implemented an electrostatic PIC simulation in Python. If I were to continue development of this code, the next step would be to consider the $ mathbf{v} times mathbf{B}$ rotation caused by magnetic fields. It would also be straightforward to extend the code to 3D. Finally, all the methods used here are applicable to gravitational simulations. Here are some helpful references: . USPAS course | Hockney &amp; Eastwood | Birdsall &amp; Langdon | .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/simulation/2021/02/22/PIC.html",
            "relUrl": "/physics/accelerators/simulation/2021/02/22/PIC.html",
            "date": " • Feb 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Particle-in-cell simulation",
            "content": "In my research I utilize PyORBIT, one of many simulation codes for beam physics. One of the key components of the these simulations is the inclusion of the electromagnetic interactions between particles in the beam; these are known as space charge forces, which refers to the charge density of the beam in free space. This post examines the particle-in-cell (PIC) algorithm used to compute space charge forces by building a simple simulation engine in Python. Links to relevant books and lectures are included at the end. . Theoretical model . We&#39;ll use bunch to refer to a group of particles in three-dimensional (3D) space, and we&#39;ll use a local cartesian coordinate system whose origin moves with the center of the bunch as shown below: . . The $s$ coordinate specifies the position of the bunch in the accelerator, and the path can be curved. Now for a few assumptions and approximations. First, the center of the bunch is moving at a constant velocity $ beta c$, where $c$ is the speed of light, so the time variable can be replaced by the position $s = beta c t$. The slope $x&#39; = dx/ds$ can be used instead of the velocity, and is measured in milliradians based on the small angle approximation. Second, we&#39;ll assume the transverse ($x$-$y$) size of the bunch is much smaller than its length, so that for a given transverse slice it is as if the bunch had a uniform density and infinite length in the longitudinal direction. Our focus will be on the transverse dynamics of this slice, as shown below, so each tracked &quot;particle&quot; will really be an infinite line of charge; any dynamics in the longitudinal direction will be ignored. . . Credit: G. Franchetti Another approximation is to neglect any magnetic fields generated by the beam, again valid if the transverse momenta are very small compared to the bunch kinetic energy. All this being said, the equations of motion without any external foces, i.e., in free space, can be written as . $$ mathbf{x}&#39;&#39; = frac{q}{mv_s gamma^3} mathbf{E}, tag{1}$$ . where $ mathbf{x} = [x, y]^T$ is the coordinate vector, $ mathbf{E} = [E_x, E_y]^T$ is the self-generated electric field, $m$ is the particle mass, and $v_s$ is the velocity in the $s$ direction. Let&#39;s first address the factor $ gamma^{-3}$ in the equation of motion, which means that the space charge force goes to zero as the velocity approaches the speed of light. This is because parallel moving charges generate an attractive magnetic force which grows with velocity, completely cancelling the electric force in the limit $v rightarrow c$. . . Credit: OpenStax University PhysicsOne may ask: what about the rest frame, in which there is no magnetic field? But special relativity says that electrogmagnetic fields change with reference frame. Using the transformations defined here, you can quickly prove that . $$ mathbf{E}_{lab} = frac{ mathbf{E}_{rest}}{ gamma}. tag{2}$$ . This inverse relationship between velocity and the space charge force has real-life consequences. It tells us that space charge is important if 1) the beam is very intense, meaning there are many particles in a small area, or 2) the beam is very energetic, meaning it is moving extremely fast. For example, space charge can usually be ignored in electron beams, which move near the speed of light for very modest energies due to their tiny mass, but is significant in high-intensity, low-energy hadron accelerators such as FRIB, SNS, and ESS. . We should now address the difficulty in determining the evolution of this system: the force on a particle in an $n$-particle bunch depends on the positions of the other $n - 1$ particles. The approach of statistical mechanics to this problem is to introduce a distribution function $f( mathbf{x}, mathbf{x}&#39;, t)$ which gives the number of particles in an infinitesimal volume of phase space. The Vlasov-Poisson system of equations determines the evolution of $f$ (as long as we ignore collisions between particles and any magnetic fields): . $$ frac{ partial{f}}{ partial{s}} + mathbf{x}&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}}} + mathbf{x}&#39;&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}&#39;}}. tag{3}$$ . We know $ mathbf{x&#39;&#39;}$ from Eq. (1). The electric field is obtained from Poisson&#39;s equation: . $$ nabla cdot mathbf{E} = - nabla^2 phi = frac{ rho}{ varepsilon_0}. tag{4}$$ . Finally, the transverse charge density $ rho$ is determined by . $$ rho = q int{f dx&#39;dy&#39;}. tag{5}$$ . Although these equations are easy to write down, they are generally impossible to solve analytically. We need to turn to a computer for help. . Computational method . The Vlasov equation could be solved directly, but this is difficult, especially in 2D or 3D. On the other end of the spectrum, the notion of a fluid in phase space could be abandonded and each particle could be tracked individually, computing the forces using direct sums. But this is infeasible with current hardware; the time complexity would by $O(n^2)$, where $n$ is the number of particles, and $n$ may be on the order of $10^{14}$. The particle-in-cell (PIC) method is a sort of combination of these two approaches. The idea is to track a group of macroparticles according to Eq. (1), each of which represents a large number of real particles. The fields, however, are solved from Eq. (4). The key step is transforming back and forth between a discrete and continuous representation of the bunch. The simulation loop for the PIC method is shown below. . . In the next sections I will discuss each of these steps and implement them in Python code. Let&#39;s first create a Bunch class, which is a simple container for the bunch coordinates. (Note: there are some constants and helper functions I&#39;ve imported that I don&#39;t define. The whole package is posted on GitHub.) . import numpy as np class Bunch: &quot;&quot;&quot;Container for 2D distribution of particles. Attributes - intensity : float Number of physical particles in the bunch. Default 1.5e14. length : float Length of the bunch [m]. Default: 250. mass, charge, kin_energy : float Mass [GeV/c^2], charge [C], and kinetic energy [GeV] per particle. line_density : float Longitudinal particle density [1 / m]. Default 1.5e14 / 250. line_charge_density : float Longitudinal charge density [C / m]. nparts : float Number of macroparticles in the bunch. macrosize : float Number of physical particles represented by each macroparticle. macrocharge : float Charge represented by each macroparticle [C]. perveance : float Dimensionless space charge perveance. sc_factor : float Factor for space charge kicks such that that x&#39;&#39; = factor * Ex. X : ndarray, shape (nparts, 4) Array of particle coordinates. Columns are [x, x&#39;, y, y&#39;]. Units are meters and radians. positions : ndarray, shape (nparts, 2): Just the x and y positions (for convenience). &quot;&quot;&quot; def __init__(self, intensity=1e14, length=250., mass=0.938, kin_energy=1., charge=elementary_charge): self.intensity, self.length = intensity, length self.mass, self.kin_energy, self.charge = mass, kin_energy, charge self.gamma = 1 + (self.kin_energy / self.mass) self.beta = np.sqrt(1 - (1 / self.gamma)**2) self.nparts = 0 self.line_density = intensity / length self.line_charge_density = charge * self.line_density self.perveance = get_perveance(self.line_density, self.beta, self.gamma) self.sc_factor = get_sc_factor(charge, mass, self.beta, self.gamma) self.compute_macrosize() self.X, self.positions = None, None def compute_macrosize(self): &quot;&quot;&quot;Update the macrosize and macrocharge.&quot;&quot;&quot; if self.nparts &gt; 0: self.macrosize = self.intensity // self.nparts else: self.macrosize = 0 self.macrocharge = self.charge * self.macrosize def fill(self, X): &quot;&quot;&quot;Fill with particles.&quot;&quot;&quot; self.X = X if self.X is None else np.vstack([self.X, X]) self.positions = self.X[:, [0, 2]] self.nparts = self.X.shape[0] self.compute_macrosize() def compute_extremum(self): &quot;&quot;&quot;Get extreme x and y coorinates.&quot;&quot;&quot; self.xmin, self.ymin = np.min(self.positions, axis=0) self.xmax, self.ymax = np.max(self.positions, axis=0) self.xlim, self.ylim = (self.xmin, self.xmax), (self.ymin, self.ymax) . Weighting . Starting from a group of macroparticles, we need to produce a charge density $ rho_{i,j}$ on a grid. The most simple approach is the nearest grid point (NGP) method, which, as the name suggests, assigns the full particle charge to the closest grid point. This is commonly called zero-order weighting; although it is very fast and easy to implement, it is not commonly used because it can lead to significant noise. A better method called cloud-in-cell (CIC) treats each particle as a rectangular, uniform density cloud of charge with dimensions equal to the grid spacing. A fractional part of the charge is assigned based on the fraction of the cloud overlapping with a given cell. This can be thought of as first-order weighting. To get a sense of what these methods are doing (in 1D), we can slide a particle across a cell and plot the resulting density of the cell at each position, thus giving an effective particle shape. . The NGP method leads to a discontinuous boundary, while the CIC method leads to a continous boundary (but discontinous derivative). There are also higher order methods which lead to a smooth boundary, but I don&#39;t cover those here. . We also need to perform the inverse operation: given the electric field at each grid point, interpolate the value at each particle position. The same method applies here. NGP just uses the electric field at the nearest grid point, while CIC weights the four nearest grid points. The following Grid class implements the CIC method. Notice that I utilized Cython in the for-loop in the distribute method. I couldn&#39;t figure out a way to perform the operation with this loop, and in pure Python it took about 90% of the runtime for a single simulation step. Using Cython gave a significant performance boost. . %%cython import numpy as np from scipy.interpolate import RegularGridInterpolator class Grid: &quot;&quot;&quot;Class for 2D grid. Attributes - xmin, ymin, xmax, ymax : float Minimum and maximum coordinates. Nx, Ny : int Number of grid points. dx, dy : int Spacing between grid points. x, y : ndarray, shape (Nx,) or (Ny,) Positions of each grid point. cell_area : float Area of each cell. &quot;&quot;&quot; def __init__(self, xlim=(-1, 1), ylim=(-1, 1), size=(64, 64)): self.xlim, self.ylim = xlim, ylim (self.xmin, self.xmax), (self.ymin, self.ymax) = xlim, ylim self.size = size self.Nx, self.Ny = size self.dx = (self.xmax - self.xmin) / (self.Nx - 1) self.dy = (self.ymax - self.ymin) / (self.Ny - 1) self.cell_area = self.dx * self.dy self.x = np.linspace(self.xmin, self.xmax, self.Nx) self.y = np.linspace(self.ymin, self.ymax, self.Ny) def set_lims(self, xlim, ylim): &quot;&quot;&quot;Set the min and max grid coordinates.&quot;&quot;&quot; self.__init__(xlim, ylim, self.size) def zeros(self): &quot;&quot;&quot;Create array of zeros with same size as the grid.&quot;&quot;&quot; return np.zeros((self.size)) def distribute(self, positions): &quot;&quot;&quot;Distribute points on the grid using the cloud-in-cell (CIC) method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. Returns - rho : ndarray, shape (Nx, Ny) Density at each grid point. &quot;&quot;&quot; # Compute area overlapping with 4 nearest neighbors ivals = np.floor((positions[:, 0] - self.xmin) / self.dx).astype(int) jvals = np.floor((positions[:, 1] - self.ymin) / self.dy).astype(int) ivals[ivals &gt; self.Nx - 2] = self.Nx - 2 jvals[jvals &gt; self.Ny - 2] = self.Ny - 2 x_i, x_ip1 = self.x[ivals], self.x[ivals + 1] y_j, y_jp1 = self.y[jvals], self.y[jvals + 1] _A1 = (positions[:, 0] - x_i) * (positions[:, 1] - y_j) _A2 = (x_ip1 - positions[:, 0]) * (positions[:, 1] - y_j) _A3 = (positions[:, 0] - x_i) * (y_jp1 - positions[:, 1]) _A4 = (x_ip1 - positions[:, 0]) * (y_jp1 - positions[:, 1]) # Distribute areas for each point rho = self.zeros() cdef double[:, :] rho_view = rho cdef int i, j for i, j, A1, A2, A3, A4 in zip(ivals, jvals, _A1, _A2, _A3, _A4): rho_view[i, j] += A4 rho_view[i + 1, j] += A3 rho_view[i, j + 1] += A2 rho_view[i + 1, j + 1] += A1 return rho / self.cell_area def interpolate(self, grid_vals, positions): &quot;&quot;&quot;Interpolate values from the grid using the CIC method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. grid_vals : ndarray, shape (n, 2) Scalar value at each coordinate point. Returns - int_vals : ndarray, shape (nparts,) Interpolated value at each position. &quot;&quot;&quot; int_func = RegularGridInterpolator((self.x, self.y), grid_vals) return int_func(positions) def gradient(self, grid_vals): &quot;&quot;&quot;Compute gradient using 2nd order centered differencing. Parameters - grid_vals : ndarray, shape (Nx, Ny) Scalar values at each grid point. neg : Bool If True, return the negative of the gradient. Returns - gradx, grady : ndarray, shape (Nx, Ny) The x and y gradient at each grid point. &quot;&quot;&quot; return np.gradient(grid_vals, self.dx, self.dy) . It should be mentioned that the field interpolation method should be the same as the charge deposition method; if this is not true, it is possible for a particle to exert a force on itself! Let&#39;s test the method on a gaussian distribution of 100,000 macroparticles in the $x$-$y$ plane, truncated at three standard devations. We&#39;ll choose the number of grid points to be $N_x = N_y = 64$. I call a DistGenerator class that I don&#39;t show here. . dg = DistGenerator() bunch = Bunch() bunch.fill(dg.generate(kind=&#39;gauss&#39;, nparts=int(1e5), cut=3.)) bunch.compute_extremum() grid = Grid(bunch.xlim, bunch.ylim, size=(64, 64)) rho = bunch.line_density * grid.distribute(bunch.positions) . Field solver . The workhorse in the simulation loop is the field solver. We need to solve Poisson&#39;s equation: . $$ left({ frac{ partial^2}{ partial x^2} + frac{ partial^2}{ partial y^2}} right) = - frac{ rho left(x, y right)}{ varepsilon_0}. tag{6}$$ . The discretized version of the equation reads . $$ frac{ phi_{i+1,j} + -2 phi_{i,j} + phi_{i-1,j}}{{ Delta_x}^2} + frac{ phi_{i,j+1} + -2 phi_{i,j} + phi_{i,j-1}}{{ Delta_y}^2} = - frac{ rho_{i,j}}{ varepsilon_0} tag{7}$$ . for a grid with spacing $ Delta_x$ and $ Delta_y$. There are multiple paths to a solution; we will focus on the method implemented in PyORBIT, which utilizes the Fourier convolution theorem. . Convolution theorem . The potential from an infinite line charge at the origin with charge density $ lambda$ is . $$ phi( mathbf{x}) = - frac{ lambda}{2 pi varepsilon_0} ln{| mathbf{x}|} = - frac{ lambda}{2 pi varepsilon_0} int{ ln{| mathbf{x} - mathbf{q}|} delta( mathbf{q})d mathbf{q}}. tag{8}$$ . Note that $ mathbf{q}$ is just a dummy variable; usually a prime is used, but we already assigned physical meaning to $ mathbf{x}&#39;$. By defining $G( mathbf{x} - mathbf{q}) = - frac{ lambda}{2 pi varepsilon_0} ln{| mathbf{x} - mathbf{q}|}$ and $ rho( mathbf{x}) = delta( mathbf{x})$, we have . $$ phi( mathbf{x}) = int{G( mathbf{x} - mathbf{q}) rho( mathbf{q})d mathbf{q}} = G( mathbf{x}) * rho( mathbf{x}). tag{9}$$ . In this form the potential is a convolution (represented by $*$) of the charge density $ rho$ with $G$, which is called the Green&#39;s function. On the grid this will look like . $$ phi_{i, j} = sum_{k,l ne i,j}{G_{i-k, j-l} rho_{k, l}}. tag{11}$$ . This solves the problem in $O(N^2)$ time complexity for $N$ grid points. This is already much faster than a direct force calculation but could still get expensive for fine grids. We can speed things up by exploiting the convolution theorem, which says that the Fourier transform of a convolution of two functions is equal to the product of their Fourier transforms. The Fourier transform is defined by . $$ hat{ phi}( mathbf{k})= mathcal{F} left[ phi( mathbf{x}) right] = int_{- infty}^{ infty}{e^{- mathbf{k} cdot mathbf{x}} phi( mathbf{x}) d mathbf{x}}. tag{12}$$ . The convolution theorem then says $$ mathcal{F} left[ rho * G right] = mathcal{F} left[ rho right] cdot mathcal{F} left[G right]. tag{13}$$ . For the discrete equation this gives . $$ hat{ phi}_{n, m} = hat{ rho}_{n, m} hat{G}_{n, m}, tag{14}$$ . where the hat represents the discrete Fourier transform. With the FFT algorithm at our disposal, the time complexity can be reduced to $O left(N log N right)$. . Implementation . There is a caveat to this method: to use the FFT algorithm, Eq. (11) must be a circular convolution, which means $G$ must be periodic. But the beam is in free space (we&#39;ve neglected any conducting boundary), so this is not true. We can make it true by doubling the grid size in each dimension. We then make $G$ a mirror reflection in the new quadrants so that it is periodic, and also set the charge density equal to zero in these regions. After running the method on this larger grid, the potential in the new quadrants will be unphysical; however, the potential in the original quadrant will be correct. There are also some tricks we can play to reduce the space complexity, and in the end doubling the grid size is not much of a price to pay for the gain in speed. The method is implemented in the PoissonSolver class. . from scipy.fft import fft2, ifft2 class PoissonSolver: &quot;&quot;&quot;Class to solve Poisson&#39;s equation on a 2D grid. Attributes - rho, phi, G : ndarray, shape (2*Nx, 2*Ny) Charge density (rho), potential (phi), and Green&#39;s function (G) at each grid point on a doubled grid. Only one quadrant (i &lt; Nx, j &lt; Ny) corresponds to to the real potential. &quot;&quot;&quot; def __init__(self, grid, sign=-1.): self.grid = grid new_shape = (2 * self.grid.Nx, 2 * self.grid.Ny) self.rho, self.G = np.zeros(new_shape), np.zeros(new_shape) self.phi = np.zeros(new_shape) def set_grid(self, grid): self.__init__(grid) def compute_greens_function(self, line_charge_density): &quot;&quot;&quot;Compute Green&#39;s function on doubled grid.&quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny Y, X = np.meshgrid(self.grid.x - self.grid.xmin, self.grid.y - self.grid.ymin) self.G[:Nx, :Ny] = np.log(X**2 + Y**2, out=np.zeros_like(X), where=(X + Y &gt; 0)) self.G *= -0.5 * line_charge_density / (2 * pi * epsilon_0) self.G[Nx:, :] = np.flip(self.G[:Nx, :], axis=0) self.G[:, Ny:] = np.flip(self.G[:, :Ny], axis=1) def get_potential(self, rho, line_charge_density): &quot;&quot;&quot;Compute the electric potential on the grid. Parameters - rho : ndarray, shape (Nx, Ny) Charge density at each grid point. line_charge_density : float Longitudinal charge density. Returns - phi : ndarray, shape (Nx, Ny) Electric potential at each grid point. &quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny self.rho[:Nx, :Ny] = rho self.compute_greens_function(line_charge_density) self.phi = ifft2(fft2(self.G) * fft2(self.rho)).real return self.phi[:Nx, :Ny] . Running the algorithm gives the following potential on the doubled grid: . solver = PoissonSolver(grid) phi = solver.get_potential(rho, bunch.line_charge_density) . We can then compute the electric field at every grid point using second-order centered differencing. This gives . $$(E_x)_{i,j} = - frac{ phi_{i+1,j} - phi_{i-1,j}}{2 Delta_x}, tag{15}$$ . $$(E_y)_{i,j} = - frac{ phi_{i,j+1} - phi_{i,j-1}}{2 Delta_y}. tag{16}$$ . Ex, Ey = grid.gradient(-phi) . Finally, the value of the electric field at each particle position can be interpolated from the grid. . Ex_int = grid.interpolate(Ex, bunch.positions) Ey_int = grid.interpolate(Ey, bunch.positions) . Particle mover . All we need to do in this step is integrate the equations of motion. A common method is leapfrog integration in which the position and velocity are integrated out of phase as follows: . $$ m left( frac{ mathbf{v}_{i+1/2} - mathbf{v}_{i-1/2}}{ Delta_t} right) = mathbf{F}( mathbf{x}_i),$$ . $$ frac{ mathbf{x}_{i+1} - mathbf{x}_i}{ Delta_t} = mathbf{v}_{i+1/2}$$ . . Credit: S. LundA different scheme must be used when velocity-dependent forces are present. This is a symplectic integrator, which means it conserves energy. It is also second-order accurate, meaning that its error is proportional to the square of the $ Delta_t$. Finally, it is time-reversible. The only complication is that, because the velocity and position are out of phase, we need to push the velocity back one half-step before starting the simulation, and push it one half-step forward when taking a measurement. . Putting it all together . Simulation loop . We have all the tools to implement the simulation loop. While $s &lt; s_{max}$ we: . Compute the charge density on the grid. | Find the electric potential on the grid. | Interpolate the electric field at the particle positions. | Update the particle positions. | We&#39;ll first create a History class which stores the beam moments or phase space coordinates. . class History: &quot;&quot;&quot;Class to store bunch data over time. Atributes moments : list Second-order bunch moments. Each element is ndarray of shape (10,). coords : list Bunch coordinate arrays. Each element is ndarray of shape (nparts, 4) moment_positions, coord_positions : list Positions corresponding to each element of `moments` or `coords`. &quot;&quot;&quot; def __init__(self, bunch, samples=None): self.X = bunch.X self.moments, self.coords = [], [] self.moment_positions, self.coord_positions = [], [] if samples is None or samples &gt;= bunch.nparts: self.idx = np.arange(bunch.nparts) else: self.idx = np.random.choice(bunch.nparts, samples, replace=False) def store_moments(self, s): Sigma = np.cov(self.X.T) self.moments.append(Sigma[np.triu_indices(4)]) self.moment_positions.append(s) def store_coords(self, s): self.coords.append(np.copy(self.X[self.idx, :])) self.coord_positions.append(s) def package(self, mm_mrad): self.moments = np.array(self.moments) self.coords = np.array(self.coords) if mm_mrad: self.moments *= 1e6 self.coords *= 1e3 . Now we&#39;ll create a Simulation class. . class Simulation: &quot;&quot;&quot;Class to simulate the evolution of a charged particle bunch. Attributes - bunch : Bunch: The bunch to track. length : float Total tracking distance [m]. step_size : float Distance between force calculations [m]. nsteps : float Total number of steps = int(length / ds). steps_performed : int Number of steps performed so far. s : float Current bunch position. positions : ndarray, shape (nsteps + 1,) Positions at which coordinates are updated. history : History object Object storing bunch data at each position. meas_every : dict or int Keys should be &#39;moments&#39; and &#39;coords&#39;. Values correspond to the number of simulations steps between storing these quantities. For example, `meas_every = {&#39;coords&#39;:4, &#39;moments&#39;:2}` will store the moments every 4 steps and the moments every other step. If an int is provided, this will be applied to both. Defaults to start and end of simulation. samples : int Number of bunch particles to store when measuring phase space coordinates. Defaults to the entire coordinate array. mm_mrad : bool If True, use units of mm-mrad. Otherwise use m-rad. ext_foc : callable Function returning external kx and ky at the current position such that u&#39;&#39; = -ku. Call signature is `kx, ky = ext_foc(s)`. &quot;&quot;&quot; def __init__(self, bunch, length, step_size, grid_size, meas_every={}, samples=None, mm_mrad=True, ext_foc=None): self.bunch = bunch self.length, self.ds = length, step_size self.nsteps = int(length / step_size) self.positions = np.linspace(0, length, self.nsteps + 1) self.grid = Grid(size=grid_size) self.solver = PoissonSolver(self.grid) self.fields = np.zeros((bunch.nparts, 2)) self.history = History(bunch, samples) self.ext_foc = ext_foc if type(meas_every) is int: meas_every = {&#39;moments&#39;: meas_every, &#39;coords&#39;:meas_every} meas_every.setdefault(&#39;moments&#39;, self.nsteps) meas_every.setdefault(&#39;coords&#39;, self.nsteps) for key in meas_every.keys(): if meas_every[key] is None: meas_every[key] = self.nsteps self.meas_every = (meas_every[&#39;moments&#39;], meas_every[&#39;coords&#39;]) self.mm_mrad = mm_mrad self.s, self.steps_performed = 0.0, 0 def set_grid(self): &quot;&quot;&quot;Determine grid limits.&quot;&quot;&quot; self.bunch.compute_extremum() self.grid.set_lims(self.bunch.xlim, self.bunch.ylim) self.solver.set_grid(self.grid) def compute_electric_field(self): &quot;&quot;&quot;Compute the self-generated electric field.&quot;&quot;&quot; self.set_grid() rho = self.grid.distribute(self.bunch.positions) rho *= self.bunch.line_charge_density * 4 # unknown origin phi = self.solver.get_potential(rho, self.bunch.line_charge_density) Ex, Ey = self.grid.gradient(-phi) self.fields[:, 0] = self.grid.interpolate(Ex, self.bunch.positions) self.fields[:, 1] = self.grid.interpolate(Ey, self.bunch.positions) def kick(self, ds): &quot;&quot;&quot;Update particle slopes.&quot;&quot;&quot; # Space charge dxp_ds = self.bunch.sc_factor * self.fields[:, 0] dyp_ds = self.bunch.sc_factor * self.fields[:, 1] # External forces if self.ext_foc is not None: kx, ky = self.ext_foc(self.s) dxp_ds -= kx * self.bunch.X[:, 0] dyp_ds -= ky * self.bunch.X[:, 2] self.bunch.X[:, 1] += dxp_ds * ds self.bunch.X[:, 3] += dyp_ds * ds def push(self, ds): &quot;&quot;&quot;Update particle positions.&quot;&quot;&quot; self.bunch.X[:, 0] += self.bunch.X[:, 1] * ds self.bunch.X[:, 2] += self.bunch.X[:, 3] * ds def store(self): &quot;&quot;&quot;Store bunch coordinates or statistics.&quot;&quot;&quot; store_moments = self.steps_performed % self.meas_every[0] == 0 store_coords = self.steps_performed % self.meas_every[1] == 0 if not (store_moments or store_coords): return Xp = np.copy(self.bunch.X[:, [1, 3]]) self.kick(+0.5 * self.ds) # sync positions/slopes if store_moments: self.history.store_moments(self.s) if store_coords: self.history.store_coords(self.s) self.bunch.X[:, [1, 3]] = Xp def run(self): &quot;&quot;&quot;Run the simulation.&quot;&quot;&quot; self.compute_electric_field() self.store() self.kick(-0.5 * self.ds) # desync positions/slopes for i in trange(self.nsteps): self.compute_electric_field() self.kick(self.ds) self.push(self.ds) self.s += self.ds self.steps_performed += 1 self.store() self.history.package(self.mm_mrad) . Demonstration . To demonstrate our method, we need some way of checking its accuracy. Real bunch measurements will be noisy and infrequent along the accelerator, so this could be a pain. Probably the way to go is to compare with other well-tested codes. Alternatively, there is an analytic benchmark available: the Kapchinskij-Vladimirskij (KV) distribution. Without going into any detail, the beam projects to a uniform density ellipse in the $x$-$y$ plane, and the space charge forces produced within this ellipse are linear (in general space charge forces are nonlinear). If we plug the KV distribution into the Vlasov equation, it can be seen that these forces will remain linear for all time if the external focusing forces are also linear. As a consequence, a set of self-consistent differential equations describing the ellipse boundary can be written down. This is a remarkable fact. For now I won&#39;t show these equations; I&#39;ll just integrate them behind the scenes and compare with the calculations. . Now for the simulation. We&#39;ll look at two situations: 1) drift space and 2) FODO lattice. 1) is a region in which there are no external fields, so only the bunch&#39;s electric field is present. 2) is one of the basic building blocks of an accelerator, consisting of a series of alternating focusing and defocusing quadrupoles. This was covered in a previous post. Some care must be taken in the choice of simulation parameters; we need a fine enough grid to resolve the hard edge of the beam, enough macroparticles per grid cell to collect good statistics. I found the accuracy did depend on the choices here, and I sort of just played around with the numbers until things looked okay. Below are the simulation parameters we will use. . nparts = int(1e5) bunch_length = 250.0 # [m], circumference of SNS accumulator ring intensity = 40e14 # 25 times that of SNS to show change over short distance step_size = 0.025 # [m] grid_size = (128, 128) . Let&#39;s first track through a 10 meter drift. We&#39;ll compare the KV distribution to the envelope calculation and also to the Gaussian distribution. . def create_bunches(intensity, bunch_length, nparts): bunches = {} for kind in [&#39;kv&#39;, &#39;gauss&#39;]: bunches[kind] = Bunch(intensity=intensity, length=bunch_length) bunches[kind].fill(dg.generate(kind, nparts, eps=25e-6, cut=3)) return bunches bunches = create_bunches(intensity, bunch_length, nparts) tracking_distance = 10.0 # [m] emittance = 25e-6 # [m*rad] meas_every = {&#39;moments&#39;: int(0.1 * tracking_distance/step_size), &#39;coords&#39;: 2} sims = {} for kind, bunch in bunches.items(): sims[kind] = Simulation(bunch, tracking_distance, step_size, grid_size, meas_every=meas_every, samples=10000) sims[kind].run() . 100%|██████████| 400/400 [00:41&lt;00:00, 9.54it/s] 100%|██████████| 400/400 [00:40&lt;00:00, 9.90it/s] . Notice that the beam expands even without space charge; this is because there is a distribution of transverse particle velocities in the initial bunch. The accuracy seems fairly good. The runtime is also okay; the equivalent PyORBIT simulation is about 10 times faster (it directly calls C++ routines and doesn&#39;t store the bunch coordinates). Below shows the evolution of a sample of 10,000 macroparicles. . &lt;/input&gt; Once Loop Reflect Now we&#39;ll track through a FODO lattice. The external fields simply add a term proportional to $x$ or $y$ in Eq. (1), so that . $$ x&#39;&#39; + k_x(s)x = frac{q}{mv_s^2 gamma^3} E_x,$$ . and similar for $y$. These fields cause the beam to periodically breath in and out, expanding in one dimension while contracting in the other. We&#39;ll add a quadrupole every 2.5 meters in our 10 meter drift space. . The disagreement with the KV envelope model, although small, seems to grow with time, specifically in the horizontal direction. I&#39;m not totally sure why this is; it could be that we need more macroparticles, or a different grid spacing, or maybe there is a bug in my code. I don&#39;t have the the time or need to investigate further. It is at least somewhat accurate. Here again is the evolution of the bunch in the $x$-$y$ plane. . &lt;/input&gt; Once Loop Reflect Conclusion . This post implemented an electrostatic PIC simulation in Python. If I were to continue development of this code, the next step would be to consider the $ mathbf{v} times mathbf{B}$ rotation caused by magnetic fields. It would also be straightforward to extend the code to 3D. Finally, all the methods used here are applicable to gravitational simulations. Here are some helpful references: . USPAS course | Hockney &amp; Eastwood | Birdsall &amp; Langdon | .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/simulation/2021/02/15/PIC.html",
            "relUrl": "/physics/accelerators/simulation/2021/02/15/PIC.html",
            "date": " • Feb 15, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Coupled Parametric Oscillators",
            "content": "Introduction . A previous post examined the analytic solutions to the equation of motion describing a parametric oscillator, which is a harmonic oscillator whose physical properties are time-dependent (but not dependent on the state of the oscillator). This problem was motivated by describing the transverse oscillations of a particle in an accelerator. In this post, the treatment will be extended to a coupled parametric oscillator. Basically, we are trying to solve the following equation of motion: . $$x&#39;&#39; + k_{11}(s)x + k_{13}(s)y + k_{14}(s)y&#39;,$$ $$y&#39;&#39; + k_{33}(s)y + k_{31}(s)x + k_{32}(s)x&#39;,$$ . where the prime denotes differentiation with respect to $s$. We also assume that each of the $k_{ij}$ coefficients are periodic, so $k_{ij}(s + L) = k_{ij}(s)$ for some $L$. . Motivation . The previous post discussed dipole and quadrupole magnetic fields, which have the special property that their fields depend linearly on $x$ and $y$, and are also uncoupled. Of course there are many other configurations possible. First, consider a solenoid magnet: . . Credit: brilliant.org The field within the coils points in the longitudinal direction and is approximatly constant ($ mathbf{B}_{sol} = B_0 hat{s}$). Plugging this into the Lorentz force equation we find: . $$ dot{ mathbf{v}} = frac{q}{m} mathbf{v} times mathbf{B} = frac{qB_0}{m} left({v_y hat{x} - v_x hat{y}} right).$$ . This means the motion in $x$ depends on the velocity in $y$, and vice versa, so this will contribute to $k_{14}$ and $k_{32}$. Coupling can also be produced from transverse magnetic fields. We again write the multipole expansion of this field: . $$B_y + iB_x = B_{ref} sum_{n=1}^{ infty} left({B_n + iA_n} right) left( frac{x + iy}{R_{ref}} right)^{n-1}.$$ . . Credit: Jeff Holmes . There will be nonlinear coupling (terms proportional to $x^j y^k$, where $j,k &gt; 1$ when $n &gt; 2$, but we are interested in linear coupling. This occurs when the skew quadrupole term $A_2$ is nonzero, which is true anytime a quadrupole is tilted in the transverse plane. The field couples the motion in one plane to the displacement in the other, contributing to the $k_{13}$ and $k_{31}$ terms. . Approach . Let&#39;s review the approach we took in analyzing the 1D parametric oscillator. We wrote the solution in pseudo-harmonic form, with an amplitude and phase which depended on time. We then found that particles travel along the boundary of an ellipse in 2D phase space, the area of which is a constant of the motion (we will denote this area by $ epsilon_x$). To understand the motion, we just need to know the dimensions and orientation of this ellipse, for which we proposed the parameters $ alpha_x$ and $ beta_x$, as well as the location of the particle on the ellipse boundary, which is determined by the phase $ mu_x$. All the subscripts can be replaced by $y$ to handle the vertical motion. We also wrote a transfer matrix $ mathbf{M}$, which connects the initial and final phase space coordinates after tracking through one period, from the parameters in the following form: . $$ mathbf{M} = mathbf{V} mathbf{P} mathbf{V}^{-1},$$ . where $ mathbf{V}^{-1}$ is a function of $ alpha_x$ and $ beta_x$ and transforms the ellipse into a circle while preserving the area, and $ mathbf{P}$ is a rotation in phase space according to the phase advance $ mu_x$. Basically, $ mathbf{V}$ turns the parametric oscillator into a harmonic oscillator. . This is a very elegant way to describe the motion with a minimal set of parameters. The question is: can we do something similar for coupled motion, in which the phase space is 4D, not 2D? To start, let&#39;s track a particle in a lattice with a nonzero skew quadrupole coefficient, plotting its phase space coordinates at one position after every period. . &lt;/input&gt; Once Loop Reflect The particle traces interesting donut-like shapes in horizontal ($x$-$x&#39;$) and vertical ($y$-$y&#39;$) phase space instead of ellipses. Below shows the shapes after 1000 periods. . There is definitely more than one frequency present, which we see if we plot the $x$ and $y$ position vs period number and take the FFT. . This is typical of a coupled oscillator. Such systems are typically understood as the superposition of normal modes, each of which corresponds to a single frequency. For example, consider two masses connected with a spring. There are two possible ways for the masses to oscillate at the same frequency. The first is a breathing mode in which they move in opposite directions, and the second is a sloshing mode in which they move in the same direction. The motion is simply the sum of these two modes. We will try to do something similar for a coupled parameteric oscillator. . Solution . Transfer matrix eigenvectors . If the phase space coordinate vector $ mathbf{x} = (x, x&#39;, y, y&#39;)^T$ evolves according to . $$ mathbf{x} rightarrow mathbf{Mx},$$ . where $ rightarrow$ represents tracking through one period, it can be shown that $ mathbf{M}$ is symplectic due to the Hamiltonian mechanics of the system. A consequence of the symplecticity condition is that $ mathbf{M}$ is fully described by 10 numbers instead of 16. Our method examines the eigenvectors of $ mathbf{M}$: . $$ mathbf{Mv} = e^{-i mu} mathbf{v}.$$ . The symplecticity condition also causes the eigenvalues and eigenvectors come in two complex conjugate pairs; this gives $ mathbf{v}_1$, $ mathbf{v}_2$, $ mu_1$, $ mu_2$ and their complex conjugates. The seemingly complex motion seen in the last animation is greatly simplified when written in terms of the eigenvectors. We can write any cooridinate vector as a linear combination of the real and imaginary components of $ mathbf{v}_1$ and $ mathbf{v}_2$: . $$ mathbf{x} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i psi_1} + sqrt{ epsilon_2} mathbf{v}_2e^{-i psi_2} right).$$ . We&#39;ve introduced two initial amplitudes ($ epsilon_1$ and $ epsilon_2$) as well as two initial phases ($ psi_1$ and $ psi_2$). Applying the transfer matrix then simply tacks on a phase. Thus, what we are observing are the 2D projections of the real components of these eigenvectors as they rotate in the complex plane. . $$ mathbf{Mx} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i left( psi_1 + mu_1 right)} + sqrt{ epsilon_2} mathbf{v}_2e^{-i( psi_2 + mu_2)} right).$$ . Let&#39;s replay the animation, but this time draw a red arrow for $ mathbf{v}_1$ and a blue arrow for $ mathbf{v}_2$. We&#39;ve chosen $ epsilon_1 = 4 epsilon_2$ and $ psi_2 - psi_1 = pi/2$. . &lt;/input&gt; Once Loop Reflect That really simplifies things! Each eigenvector simply rotates at its frequency $ mu_l$. It also explains why the amplitude in the $x$-$x&#39;$ and $y$-$y&#39;$ planes trade back and forth: it is because the projections of the eigenvectors rotate at different frequencies, sometimes aligning and sometimes anti-aligning. Because of this, the previous invariants $ epsilon_x$ and $ epsilon_y$ are replaced by $ epsilon_1$ and $ epsilon_2$ as the invariants. It is helpful to think of a torus (shown below). The two amplitudes would determine the inner and outer radii of the torus, and the two phases determine the location of a particle on the surface. . . Credit: Wikipedia Parameterization of eigenvectors . We are now going to introduce a set of parameters for these eigenvectors, and in turn the transfer matrix. We already have two phases, so that leaves 8 parameters. Our strategy is to observe that each eigenvector traces an ellipse in both horizontal ($x$-$x&#39;$) and vertical ($y$-$y&#39;$) phase space. Then, we will simply assign an $ alpha$ function and $ beta$ function to each of these ellipses. So, for the ellipse traced by $ mathbf{v}_1$ in the $x$-$x&#39;$ plane, we have $ beta_{1x}$ and $ alpha_{1x}$, and then for the second eigenvector we have $ beta_{2x}$ and $ alpha_{2x}$. The same thing goes for the vertical dimension with $x$ replaced by $y$. . . The actual eigenvectors written in terms of the parameters are . $$ vec{v}_1 = begin{bmatrix} sqrt{ beta_{1x}} - frac{ alpha_{1x} + i(1-u)}{ sqrt{ beta_{1x}}} sqrt{ beta_{1y}}e^{i nu_1} - frac{ alpha_{1y} + iu}{ sqrt{ beta_{1y}}} e^{i nu_1} end{bmatrix}, quad vec{v}_2 = begin{bmatrix} sqrt{ beta_{2x}}e^{i nu_2} - frac{ alpha_{2x} + iu}{ sqrt{ beta_{2x}}}e^{i nu_2} sqrt{ beta_{2y}} - frac{ alpha_{2y} + i(1-u)}{ sqrt{ beta_{2y}}} end{bmatrix}$$ . So in addition to the phases $ mu_1$ and $ mu_2$ we have $ alpha_{1x}$, $ alpha_{2x}$, $ alpha_{1y}$, $ alpha_{2y}$, $ beta_{1x}$, $ beta_{2x}$, $ beta_{1y}$, and $ beta_{2y}$. That&#39;s pretty much it. There are a few other parameters we need to introduce to simplify the notation, but they are not independent. The first is $u$, which, as noted in the figure, determines the areas of the ellipses in one plane relative to the other. The second and third are $ nu_1$ and $ nu_2$, which are phase differences between the $x$ and $y$ components of the eigenvectors (in the animation they are either $0$ or $ pi$). I won&#39;t discuss these here. The last thing to note is that the parameters reduce to their 1D definitions when there is no coupling in the lattice. So we would have $ beta_{1x}, beta_{2y} rightarrow beta_{x}, beta_{y}$ and $ beta_{2x}, beta_{1y} rightarrow 0$, and similar for $ alpha$. The invariants and phase advances would also revert back to their original values: $ epsilon_{1,2} rightarrow epsilon_{x,y}$ and $ mu_{1,2} rightarrow mu_{x,y}$. . Floquet transformation . These eigenvectors can also be used to construct a transformation which removes both the variance in the focusing strength and the coupling between the planes, turning the coupled parametric oscillator into an uncoupled harmonic oscillator. In other words, we seek a matrix $ mathbf{V}$ such that . $$ mathbf{V^{-1} M V} = mathbf{P} = begin{bmatrix} cos{ mu_1} &amp; sin{ mu_1} &amp; 0 &amp; 0 - sin{ mu_1} &amp; cos{ mu_1} &amp; 0 &amp; 0 0 &amp; 0 &amp; cos{ mu_2} &amp; sin{ mu_2} 0 &amp; 0 &amp; - sin{ mu_2} &amp; cos{ mu_2} end{bmatrix} $$We can do this simply by rewriting the following equation (I haven&#39;t yet figured out how to number equations in Jupyter): . $$ mathbf{x} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i psi_1} + sqrt{ epsilon_2} mathbf{v}_2e^{-i psi_2} right)$$ . in matrix form as $ mathbf{x} = mathbf{V} mathbf{x}_n$ with . $$ mathbf{x}_n = begin{bmatrix} sqrt{ epsilon_1} cos{ psi_1} - sqrt{ epsilon_1} sin{ psi_1} sqrt{ epsilon_2} cos{ psi_2} - sqrt{ epsilon_2} sin{ psi_2} end{bmatrix} $$ $$ mathbf{V} = left[{Re( mathbf{v}_1), -Im( mathbf{v}_1), Re( mathbf{v}_2), -Im( mathbf{v}_2)} right]$$ . Let&#39;s observe the motion in these new coordinates $ mathbf{x}_n$. . &lt;/input&gt; Once Loop Reflect The motion is uncoupled after this transformation; i.e., particles move in a circle of area $ varepsilon_1$ in the $x_n$-$x_n&#39;$ plane at frequency $ mu_1$, and in a circle of area $ varepsilon_2$ in the $y_n$-$y_n&#39;$ plane at frequency $ mu_2$. . Conclusion . The method introduced here allows us to describe the evolution of a parametric oscillator using the minimum number of parameters. Our physical motivation was an accelerator lattice with linear, coupled forces, such as when skew quadrupole terms are present in the magnetic fields. There is no agreed upon method to do this among accelerator physicists, but I like (and know) this method the best, and have used it in my research. I&#39;ve left out many details which can be found in the paper by Lebedev and Bogacz. The paper by Ripken is also very helpful. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/2021/01/25/coupled_parametric_oscillators.html",
            "relUrl": "/physics/accelerators/2021/01/25/coupled_parametric_oscillators.html",
            "date": " • Jan 25, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Parametric Oscillators",
            "content": "This post presents the solution to a general problem: what is the motion of a particle in one dimension (1D) in the presence of time-dependent linear forces? This amounts to solving the following equation of motion: . $$ frac{d^2x}{dt^2} + k(t)x = 0,$$ . where $k(t + T) = k(t)$ for some $T$. This is a parametric oscillator, a harmonic oscillator whose physical properties are not static. For example, the oscillations of a pendulum (in the small angle approximation) on the surface of a planet whose gravitational pull varies periodically would be described by the above equation. The solution to this equation was derived by George William Hill in 1886 to study lunar motion, and for this reason it is known as Hill&#39;s equation. It also finds application in areas such as condensed matter physics, quantum optics, and accelerator physics. After setting up the physical problem, we will examine the solutions and discuss their relevance to the last application, accelerator physics. . Problem motivation . Accelerator physics . Particle accelerators are machines which produce groups of charged particles (known as beams), increase their kinetic energy, and guide them to a target. These machines are invaluable to modern scientific research. The most famous examples are colliders, such as the LHC, in which two beams are smashed together to generate fundamental particles. A lesser known fact is that the fields of condensed matter physics, material science, chemistry, and biology also benefit tremendously from accelerators; this is due to the effectiveness of scattering experiments in which the deflection of a beam after colliding with a target is used to learn information about the target. The scattered beam is composed of neutrons in spallation neutron sources such as SNS, electrons in electron scattering facilities such as CEBAF, or photons in synchrotron light sources such as APS. In addition to scientific research, accelerators find use in medicine, particularly for cancer treatment, and also in various industrial applications. . . A large detector at an interaction point in the LHC. There are generally a few beam properties which are very important to experimentalists; in colliders it is the energy and luminosity, in spallation sources it is the intensity, and in light sources it is the brightness. There is thus a constant need to push these parameters to new regions. For example, below is the famous Livingston plot which shows the energy achieved by various machines over the past century. . . Note: vertical axis scale is beam energy needed to produce the center of mass energy by collision with a resting proton (credit: Rasmus Ischebeck). There are many physics issues associated with the optimization of these beam parameters. Accelerator physics is a field of applied physics which studies these issues. The task of the accelerator physicist is to understand, control, and measure the journey of the beam from its creation to its final destination. The difficulty of this task has grown over time; the improvement accelerator performance has brought with it a staggering increase in size and complexity. The construction and operation of modern accelerators generally requires years of planning, thousands of scientists and engineers, and hundreds of millions or even billions of dollars. Despite this complexity, the underlying physics principles are quite simple, and the single particle motion in one of these machines can be understood analytically if a few approximations are made. In the end we will arrive at Hill&#39;s equation. . How to build an accelerator . There are three basic tasks an accelerator has to accomplish. First, it must increase the beam energy (acceleration). Second, it must guide the beam along a predetermined path (steering). Third, it must ensure the beam particles remain close together (focusing). It is helpful to use a coordinate system in which the $s$ axis points along the design trajectory, and the $x$ and $y$ axes defined in the plane transverse to $s$. In this way the motion is broken up into transverse and longitudinal dynamics. . . How are these tasks accomplished? Well, particles are charged, and the force on a point charge in an electromagnetic field is given by . $$ mathbf{F} = q left({ mathbf{E} + mathbf{v} times mathbf{B}} right),$$ . where $q$ is the particle charge, $ mathbf{v}$ is the particle velocity, $ mathbf{E}$ is the electric field, and $ mathbf{B}$ is the magnetic field. An accelerator consists of a series of elements, each with their own $ mathbf{E}$ and $ mathbf{B}$; the collection of these elements is called a lattice. We need to determine which electric and magnetic fields to use. . The first task, acceleration, is not the focus of this post; I&#39;ll just mention the basic principles that are used. Acceleration cannot be done with $ mathbf{B}$ fields, since the force they produce is always perpendicular the motion. A simple method is to produce an electric field is to create voltage difference between two conductors, but there is a limit to the field strengths that can be produced in this way. One solution is to create a series of radio-frequency (RF) cavities, each with a time-varying voltage. The positions and lengths of these cavities are chosen so that particles are only within the cavities when the electric field points along the direction of motion, as shown by this fantastic animation from Wikipedia: . . The remaining tasks, steering and focusing, concern the motion in the transverse plane. $ mathbf{B}$ fields, not $ mathbf{E}$ fields, are used since their effect grows with increased particle velocity. Any transverse magnetic field $ mathbf{B} = (B_x, B_y)^T$ can be written using a multipole expansion . $$B_y + iB_x = B_{ref} sum_{n=1}^{ infty} left({B_n + iA_n} right) left( frac{x + iy}{R_{ref}} right)^{n-1}.$$ . $B_{ref}$ and $R_{ref}$ are a reference field strength and radius, respectively; just consider them to be constants. We then have the normal multiple coefficients $B_n$, and the skew multipole coefficients $A_n$. The field lines corresponding to the first few normal multipole coefficients are shown below. . . Credit: Jeff Holmes The dipole term is perfect for steering. The field is constant in magnitude and direction: . $$ mathbf{B}_{dipole} propto hat{y},$$ . producing a force which is proportional to the $x$ position: . $$ mathbf{F}_{dipole} propto - hat{x}.$$ . The quadrupole term is used for focusing. The field takes the following form: . $$ mathbf{B}_{quad} propto y hat{x} + x hat{y},$$ . with the resulting force: . $$ mathbf{F}_{quad} propto -x hat{x} + y hat{y}.$$ . The force from the quadrupole is focusing in the horizontal direction, but defocusing in the vertical direction; however, net focusing is still achieved by alternating the direction of the quadrupoles. This is analogous to a beam of light passing through a series of converging and diverging lenses. If the spacing and curvature of the lenses is correctly chosen, a net focusing can be achieved. . . Focusing (QF) and defocusing (QD) quadrupoles modeled as magnetic lenses. The forces which result from these fields are linear, meaning they are proportional the $x$ or $y$ but not $x^2$, $y^3$, etc., and they are uncoupled, meaning the dynamics in the $x$ and $y$ dimensions are independent. Now, we may ask, can we really produce a perfect dipole or quadrupole field? The answer is no. In reality there will always be higher order multipoles present in the field, but people work very hard to ensure these are much smaller than the desired multipole. This video shows a bit of the construction process for these magnets. . Linearized equation of motion . Making the above approximation of perfect dipole and quadrupole magnets, and ignoring all other elements in the machine, we arrive at the equation of motion for a single particle in the transverse plane: . $$x&#39;&#39; + k(s)x = 0,$$ . where $x&#39; = dx/ds$ and $k(s + L) = k(s)$ for some distance $L$. We could also write a similar equation for $y$. It is conventional to use the slope $x&#39;$ instead of the velocity; this allows us to talk about the position of the particle in the lattice instead of the amount of time which has passed. The period length $L$ could be the entire circumference of a circular machine, or could be a smaller repeated subsection. . Solution . Envelope function . The general solution to Hill&#39;s equation is given by . $$x(s) = sqrt{ epsilon} ,w(s) cos left({ mu(s) + delta} right).$$ . This introduces an amplitude $w(s) = w(s + L)$ which we call the envelope function, as well as a phase $ mu$, both of which depend on $s$. The constants $ epsilon$ and $ delta$ are determined by the initial conditions. Let&#39;s plot this trajectory in a FODO (focus-off-defocus-off) lattice, which consists of evenly spaced focusing and defocusing quadrupoles. Here is the focusing strength within the lattice (QF is the focusing quadrupole and QD is the defocusing quadrupole): . . For now we can think of the lattice as repeating itself forever in the $s$ direction. Each black line below is represents the trajectory for a different initial position and slope; although the individual trajectories look rather complicated, the envelope function has a very simple form. . . Phase space . The particle motion becomes much easier to interpret if we observe it in position-momentum space, aka phase space. The following animation shows the evolution of the particle phase space coordinates at a single position in the lattice. The position shown is $s = nL/4$, where $n$ is the period number, which corresponds to the midpoint between the focusing and defocusing quadrupoles. . . &lt;/input&gt; Once Loop Reflect We see that the particle jumps along the boundary of an ellipse in phase space. The shape and orientation of the ellipse will change if we look at a different position in the lattice, but its area will be the same. So, the motion is determined by the dimensions and oriention of this ellipse throughout the lattice, as well as the location of the paricle on the ellipse boundary. This motivates the definition of the so-called Twiss parameters, which were first introduced by Courant and Snyder in 1958: . $$ beta = w^2, quad alpha = - frac{1}{2} beta&#39;, quad gamma = frac{1 + alpha^2}{ beta}.$$ . The dimensions of the phase space ellipse are nicely described by these parameters: . . The maximum extent of the ellipse is determined by $ beta$ in the $x$ direction and $ gamma$ in the $y$ direction. $ alpha$ is proportional to the slope of the $ beta$ function, and so determines the tilt angle of the ellipse. The position of a particle on the ellipse is given by the phase $ mu$. Finally, the invariant of the motion corresponding to the ellipse area is constructed from the Twiss parameters as . $$ epsilon = beta {x&#39;}^2 + 2 alpha xx&#39; + gamma x^2$$ . for any $x$ and $x&#39;$. The $ beta$ functions and phase advances in both dimensions are extremely important to measure and control in a real machine. Here is an example of the horizontal and vertical $ beta$ functions in the SNS accumulator ring. . . Transfer matrices . A helpful tool to pair with the parameterization we just introduced is the transfer matrix, a matrix which connects the phase space coordinates at two different positions: . $$ begin{bmatrix} x x&#39; end{bmatrix}_{s + L} = mathbf{M} begin{bmatrix} x x&#39; end{bmatrix}_{s}$$ . The transfer matrix can be written as $ mathbf{M} = mathbf{V} mathbf{P} mathbf{V}^{-1}$, where . $$ mathbf{V} = frac{1}{ sqrt{ beta}} begin{bmatrix} beta &amp; 0 - alpha &amp; 1 end{bmatrix}$$ and $$ mathbf{P} = begin{bmatrix} cos mu &amp; sin mu - sin mu &amp; cos mu end{bmatrix} $$ . The effect of $ mathbf{V}^{-1}$ is to deform the phase space ellipse into a circle while preserving its area. $ mathbf{P}$ is then just a rotation in phase space, and $ mathbf{V}$ then transforms back into a tilted ellipse. This is illustrated below. . . $ mathbf{V}$ can be thought of as a time-dependent transformation which removes the variance in the focusing strength, turning the parametric oscillator into a simple harmonic oscillator. Often it is called the Floquet transformation. . Conclusion . We&#39;ve presented the solution to Hill&#39;s equation, which describes a parameteric oscillator. The equation pops up in multiple areas, but we focused on its application in accelerator physics, in which Hill&#39;s equation describes the transverse motion of a single particle in an accelerator with perfectly linear magnetic fields. . The solution is best understood geometrically: particles move around the surface of an ellipse in phase space, the area of which is an invariant of the motion. The dimensions and orientation of the ellipse are determined by $ alpha$ and $ beta$, and the location of the paricle on the ellipse boundary is determined by $ mu$. These parameters can be used to construct a time-dependent transformation ($ mathbf{V}$) which turns the parametric oscillator into a simple harmonic oscillator. . The next post will examine how this treatment can be extended to include coupling between the horizontal and vertical dimensions. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/2021/01/21/parametric_oscillators.html",
            "relUrl": "/physics/accelerators/2021/01/21/parametric_oscillators.html",
            "date": " • Jan 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am completing my PhD in physics from the University of Tennessee, Knoxille. My research is in the area of accelerator physics, and I work as part of the Accelerator Physics group at the Spallation Neutron Source. Previously, I graduated from Wheaton College with a BS in physics. . Talks . Publications .",
          "url": "https://austin-hoover.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://austin-hoover.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}