{
  
    
        "post0": {
            "title": "Authorship identification",
            "content": "In this post, I&#39;ll summarize a paper by John Houvardas and Efstathios Stamatatos titled N-Gram Feature Selection for Authorship Identification [1]. The topic of the paper is authorship identification, that is, to identify the author of an unlabeled document given a list of possible authors and some sample of each author&#39;s writing. I&#39;ll first motivate the problem of authorship identification, then briefly introduce the relevant statistical methods, and finally summarize and implement the methods in the paper. My goal is to reproduce the authors&#39; results. . . Stylometry . Motivation: the Federalist Papers . The Federalist Papers are an important collection of 85 essays written by Hamilton, Madison, and Jay during 1787 and 1788. The essays were published under the alias &quot;Plubious&quot;, and although it became well-known that the three men were involved, the authorship of each individual paper was kept hidden for over a decade. This was actually in the interest of both Hamilton and Madison; both were politicians who had changed positions on a number of issues and didn&#39;t want their political opponents to use their own words against them. Days before his death, Hamilton allegedly wrote down who he believed to be the correct author of each essay, claiming over 60 for himself. Madison waited a number of years before publishing his own list, and in the end there were 12 essays claimed by both Madison and Hamilton. Many interesting details on the controversy can be found in [2]. . . Alexander Hamilton (left) and James Madison (right). Credit: Wikipedia. There are a few ways one might go about resolving this dispute. One approach is to analyze the actual content of the text. For example, perhaps an essay draws from a reference with which only Madison was intimately familiar, or maybe an essay is similar to Hamilton&#39;s previous work. This approach was used many times over the next 150 years, but perhaps the final word on the subject was by Adair, who in 1944 concluded that Madison likely wrote all 12 essays. An alternative approach is to analyze the style of the text. For example, maybe Madison used many more commas than Hamilton. The field of stylometry attempts to statistically quantify these stylistic differences. David Holmes writes the following about stylometry [3]: . At its heart lies an assumption that authors have an unconscious aspect to their style, an aspect which cannot consciously be manipulated but which possesses features which are quantifiable and which may be distinctive. . I think this a valid assumption. The question is which features best characterize the author&#39;s style and which methods are best to use in the analysis of these features. Let&#39;s go back in time a bit to see how stylometry has developed over the past 150 years. . History . The physicist Thomas Mendenhall is considered the first to statistically analyze large literary texts. He presented the following interesting idea in an 1887 paper titled The Characteristic Curves of Composition [4]: it is known that each chemical element emits light with a unique distribution of wavelengths when it is heated; perhaps each author has a unique distribution of word lengths in the texts they have written. It&#39;s a really cool idea, and I highly recommend reading his original paper. Mendenhall tallied word lengths by hand for various books, usually in batches of 1000 words or so. Here is Fig. 2 from his paper which shows the characteristic curves for a few excerpts of Oliver Twist. . . Distribution of word lengths in &quot;Oliver Twist&quot;. Each curve is for a different sample of 1000 words. From [4]. He showed that these curves are very interesting and that they do reveal similarities between different works by the same author. The use of these statistics for authorship identification was left for future work. . The next significant advance in the statistical analysis of text was made by Zipf in 1932. Zipf found an interesting relationship between an integer $k$ and the frequency $f(k)$ of the $k$th most frequent word. This is often called a rank-frequency relationship, where $k$ is the rank. The scaling law can be written as . $$ f(k) propto k^{-1}. tag{1} $$ . The idea expressed by this law is that short words are much more frequent than large words. Surprisingly, the law holds up very well, albeit not perfectly, for most texts. Why this is the case is still unknown; a comprehensive review of the current state of the law can be found in [5]. The law also shows up in other situations such as national GDP: . . National GDPs appear to be moving toward the prediction by Zipf&#39;s Law (red line). From [6]. The success of Zipf&#39;s Law was very encouraging and led to a flurry of new mathematical models. Stylometry reached a landmark case in the 1960&#39;s when researchers used the frequency distributions of short function words — words we don&#39;t think about too much like &quot;upon&quot; or &quot;therefore&quot; — to support Adair&#39;s conclusion that Madison wrote the 12 disputed Federalist Papers. At the end of the day, however, models created in the spirit of Zipf&#39;s Law are probably doomed to fail. The &quot;true&quot; underlying model must be very complex due to its dependence on human psychology. There are now many algorithms available which instead build predictive models directly from data, and these can be readily applied to the problem of authorship identification. Here we focus on the use of the Support Vector Machine (SVM). . Support Vector Machine (SVM) . I include here the basic idea behind the SVM approach. There are a huge number of resources which go into the details (such as [7]). I&#39;ll follow the Wikipedia page since it has a nice short summary. . Maximum margin hyperplane . Consider a linear binary classifier, i.e., a plane which splits the data into two classes. The equation for a plane in any number of dimensions is . $$ y( mathbf{x}) = mathbf{w}^T mathbf{x} + w_0 = 0 tag{2}. $$ . This plane is called the decision surface; points are assigned to class 1 if $y( mathbf{x}) &gt; 0$ or class 2 when $y( mathbf{x}) &lt; 0$. Suppose the data is linearly separable (able to be completely split in two) and that we&#39;ve found a plane which correctly splits the data. We could then scale the coordinates such that all points with $y( mathbf{x}) ge 1$ belong to class 1 and all points with $y( mathbf{x}) le -1$ belong to class 2. The separating plane then sits in the middle as in the following figure. . . Maximum margin separating plane. Credit: Wikipedia. Notice that the plane could be rotated while still correctly splitting the existing data; the SVM attempts to find the optimal plane by maximizing the orthogonal distance from the decision plane to the closest point. This is known as the margin, and it can be shown that it is inversely proportional to the magnitude of $ mathbf{w}$. Thus, the SVM tries to minimize $| mathbf{w}|^2$ subject to the constraint that all points are correctly categorized. New data is then assigned based on this optimal boundary. . Some datasets won&#39;t be linear separable, in which case we can add a penalty function in order to minimize the number of miscategorized points. So, for N samples we minimize . $$ frac{1}{2}| mathbf{w}|^2 + C sum_{i=1}^{N}{ max left[0, 1 - {t_i y( mathbf{x}_i)} right]} . tag{3}$$ . where $t_i$ is the true class of point $i$ ($ pm 1$) and $C$ is a positive constant. Correctly classified points don&#39;t contribute anything to the sum since $t_i y( mathbf{x}_i)$ will be greater than or equal to one. Let&#39;s try this on non-linearly separable data sampled from two Gaussian distributions in 2D space. The Python package scikit-learn has a user-friendly interface for the SVM implementation in LIBLINEAR which we use here. . import numpy as np from sklearn import svm import matplotlib.pyplot as plt import proplot as plot # Create two Gaussian distributions np.random.seed(0) n = 200 X = np.vstack([np.random.normal(size=(n, 2), loc=[0, 0], scale=2.0), np.random.normal(size=(n, 2), loc=[5, 5], scale=2.5)]) y = n * [1] + n * [-1] # Find SVM decision boundary clf = svm.LinearSVC(C=1) clf.fit(X, y) # Plot the data def despine(ax): ax.format(xticks=[], yticks=[]) for side in [&#39;left&#39;, &#39;right&#39;, &#39;top&#39;, &#39;bottom&#39;]: ax.spines[side].set_visible(False) def padded_ranges(X, pad=0.5): xmin, ymin = np.min(X, axis=0) - pad xmax, ymax = np.max(X, axis=0) + pad return (xmin, xmax), (ymin, ymax) def plot_dec_boundary(ax, clf, xlim=(-100, 100), i=0, **kws): w0 = clf.intercept_ if type(clf.intercept_) is float else clf.intercept_[i] (w1, w2) = clf.coef_[i] line_x = np.array(xlim) line_y = -(w1 / w2) * line_x - (w0 / w2) kws.setdefault(&#39;c&#39;, &#39;black&#39;) ax.plot(line_x, line_y, **kws) def plot_dec_regions(ax, clf, xlim, ylim, nsteps=500, **kws): (xmin, xmax), (ymin, ymax) = xlim, ylim xx, yy = np.meshgrid(np.linspace(xmin, xmax, nsteps), np.linspace(ymin, ymax, nsteps)) Z = np.c_[xx.ravel(), yy.ravel()] y_pred = clf.predict(Z) zz = y_pred.reshape(xx.shape) kws.setdefault(&#39;alpha&#39;, 0.05) kws.setdefault(&#39;zorder&#39;, 0) ax.contourf(xx, yy, zz, **kws) fig, ax = plot.subplots() despine(ax) ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y) xlim, ylim = padded_ranges(X, pad=0.5) plot_dec_boundary(ax, clf) plot_dec_regions(ax, clf, xlim, ylim) ax.format(xlim=xlim, ylim=ylim) ax.annotate(&#39;Decision nboundary&#39;, xy=(0.55, 0.02), xycoords=&#39;axes fraction&#39;); . . The points are colored by their true classes, and the background is shaded according to the SVM prediction at each point. It can be important to try at least a few different values of $C$, which determines the trade-off between correctly classifying all samples and maximizing the margin, and to observe the effect on the accuracy as well as the algorithm convergence. Parameters such as this one which change the algorithm behavior but aren&#39;t optimized by the algorithm itself are commonly known as hyperparameters. . Kernel trick . In some cases the linear model is going to be bad; a frequently used example is &quot;target&quot; dataset. . n = 400 r1 = np.sqrt(np.random.uniform(0.0, 0.2, size=(n,))) r2 = np.sqrt(np.random.uniform(0.5, 1.0, size=(n,))) t1 = np.random.uniform(0, 2*np.pi, size=(n,)) t2 = np.random.uniform(0, 2*np.pi, size=(n,)) X = np.vstack([np.vstack([r1*np.cos(t1), r1*np.sin(t1)]).T, np.vstack([r2*np.cos(t2), r2*np.sin(t2)]).T]) y = n * [1] + n * [-1] xlim, ylim = padded_ranges(X, pad=0.1) fig, ax = plot.subplots() despine(ax) ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y) ax.format(xlim=xlim, ylim=ylim) . . A line obviously won&#39;t work; ideally we would draw a circle around the inner cluster to split the data. The kernel trick can be used to alleviate this problem by performing a transformation to a higher dimensional space in which the data is linearly separable. For example, consider the transformation . $$ (x_1, x_2) rightarrow (x_1^2, x_2^2, sqrt{2} x_1 x_2) . tag{4}$$ . from plotly import graph_objects as go x1, x2 = X.T u = x1**2 v = np.sqrt(2) * x1 * x2 w = x2**2 fig = go.Figure(data=go.Scatter3d(x=u, y=v, z=w, mode=&#39;markers&#39;, marker=dict(color=y, size=3, opacity=0.5))) fig.update_scenes(xaxis_visible=False, yaxis_visible=False, zaxis_visible=False) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . It&#39;s clear from rotating this plot that the transformed data can be split with a 2D plane. This need not be the transformation used by the SVM — in fact, many transformations can be used — but it clearly demonstrates the idea. The linear boundary in the transformed space can then be transformed to a nonlinear boundary in the original space. One way to plot this boundary is to make a prediction on a grid of points, then make a contour plot (the boundary is shown in grey). . clf = svm.SVC(kernel=&#39;rbf&#39;) clf.fit(X, y) fig, ax = plot.subplots() despine(ax) ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y) ax.format(xlim=xlim, ylim=ylim) plot_dec_regions(ax, clf, xlim, ylim) . . There are still several advantages to the linear SVM. First, it is much faster to train, and second, the kernel trick may be unnecessary for high-dimensional data. As we&#39;ll see, text data can involve a large number of very high-dimensional samples, so we&#39;ll be sticking with linear kernels. . Multi-class . A binary classifier can also be used for multi-class problems. Here we use the one-versus-rest(OVR) approach. Suppose we had $N$ classes denoted by $c_1$, $c_2$ ... $c_N$. In the OVR approach we train $N$ different classifiers; the ith classifier $L_i$ tries to split the data into two parts: $c_i$ and not $c_i$. Then we observe a new point and ask each classifier $L_i$ how confident it is that the point belongs to $c_i$. The point is assigned to the class with the highest score. We can extend our previous example to three Gaussian distributions to get a sense of how the decision boundaries are formed. . # Create three Gaussian distributions np.random.seed(0) n = 200 X = np.vstack([np.random.normal(size=(n, 2), loc=[0, 0], scale=2.0), np.random.normal(size=(n, 2), loc=[5, 5], scale=2.5), np.random.normal(size=(n, 2), loc=[-6, 6], scale=2.5)]) y = n * [1] + n * [0] + n * [-1] # Find SVM decision boundary clf = svm.LinearSVC(C=1, multi_class=&#39;ovr&#39;, max_iter=10000) clf.fit(X, y) # Plot the data fig, ax = plot.subplots() despine(ax) ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y, cmap=plot.Colormap((&#39;pink9&#39;, &#39;grey&#39;, &#39;darkgreen&#39;))) # Plot decision boundary xlim, ylim = padded_ranges(X) for i in range(3): ls = [&#39;-&#39;, &#39;--&#39;, &#39;dotted&#39;][i] plot_dec_boundary(ax, clf, i=i, ls=ls) ax.format(xlim=xlim, ylim=ylim) ax.legend(labels=[&#39;class {} boundary&#39;.format(i) for i in range(1, 4)], ncols=1, loc=(1.1, 0.6)); plot_dec_regions(ax, clf, xlim, ylim, alpha=0.1) . . The same idea holds with more classes and dimensions. Notice that there are some regions which are claimed by multiple classifiers, so it&#39;s not a perfect method. . N-grams and feature selection methods . As I mentioned in the introduction, the paper I&#39;m following is called N-Gram Feature Selection for Authorship Identification. In short, the paper used n-gram frequencies (defined in a moment) as features in the classification task and developed a new method to select the most significant or &quot;dominant&quot; n-grams. This was tested on a collection of short news articles. Let&#39;s step through their method. . Data set description . The Reuters Corpus Volume 1 (RCV1) data set is a big collection of news articles labeled by topic. Around 100,000 of these have known authors, and there are around 2000 different authors. A specific topic was chosen, and only authors who wrote at least one article which fell under this topic were considered. From this subset of authors, the top 50 in terms of number of articles written were chosen. 100 articles from each author were selected — 5000 in total — and these were evenly split into a training and testing set. The resulting corpus is a good challenge for authorship identification because the genre is invariant across documents and because the authors write about similar topics. Hopefully this leaves the author&#39;s style as the primary distinguishing factor. The data set can be downloaded here. The files are organized like this: . . There are plenty of functions available to load the data and to extract features from it, but I&#39;ll do everything manually just for fun. To load the data, let&#39;s first create two lists of strings, texts_train and texts_test, corresponding to the 2500 training and testing documents. The class id and author name for each document are also stored. . from os import listdir from os.path import join def load_files(outer_path): texts, class_ids, class_names = [], [], [] for class_id, folder in enumerate(sorted(listdir(outer_path))): folder_path = join(outer_path, folder) for filename in listdir(folder_path): class_ids.append(class_id) class_names.append(folder) file = open(join(folder_path, filename), &#39;r&#39;) text = file.read().replace(&#39; &#39;, &#39;_&#39;) texts.append(text) file.close() return texts, class_ids, class_names texts_train, y_train, authors_train = load_files(&#39;reuters_data/train&#39;) texts_test, y_test, authors_test = load_files(&#39;reuters_data/test&#39;) . Author Name Author ID Training Text . 0 AaronPressman | 0 | A_group_of_leading_trademark_specialists_plans... | . 1 AaronPressman | 0 | Prospects_for_comprehensive_reform_of_U.S._ban... | . 2 AaronPressman | 0 | An_influential_economic_research_group_is_prep... | . 3 AaronPressman | 0 | The_Federal_Communications_Commission_proposed... | . 4 AaronPressman | 0 | An_international_task_force_charged_with_resol... | . ... ... | ... | ... | . 2495 WilliamKazer | 49 | China_could_list_more_railway_companies_and_is... | . 2496 WilliamKazer | 49 | The_choice_of_Singapore_for_the_listing_of_Chi... | . 2497 WilliamKazer | 49 | China_ushered_in_1997,_a_year_it_has_hailed_as... | . 2498 WilliamKazer | 49 | China_on_Tuesday_announced_a_ban_on_poultry_an... | . 2499 WilliamKazer | 49 | China&#39;s_leaders_have_agreed_on_a_need_to_stimu... | . 2500 rows × 3 columns . The following histogram shows the distribution of document lengths in the training set; it&#39;s expected that the short average document length will greatly increases the difficulty of the classification task relative to longer works such as books. . word_counts = [len(text) for text in texts_train] fig, ax = plot.subplots(figsize=(5, 1.5)) ax.hist(word_counts, bins=&#39;auto&#39;, color=&#39;k&#39;, density=True) ax.format(xlabel=&#39;Document length (characters)&#39;, ylabel=&#39;Num. docs&#39;, yticks=[], title=&#39;Distribution of document lengths in training set&#39;) ax.annotate(r&#39;mean = {:.0f}&#39;.format(np.mean(word_counts)), xy=(0.8, 0.5), xycoords=&#39;axes fraction&#39;) ax.annotate(&#39;std = {:.0f}&#39;.format(np.std(word_counts)), xy=(0.8, 0.3), xycoords=&#39;axes fraction&#39;); from scipy.stats import norm mean, std = norm.fit(word_counts) x = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 1000) y = norm.pdf(x, mean, std) ax.plot(x, y, c=&#39;red&#39;, alpha=0.4); ax.legend(labels=[&#39;Gaussian fit&#39;], frameon=False); . . N-grams . An obvious feature candidate is word frequency; a less obvious one is n-gram frequency. A character n-gram is a string of length n. For example, the 3-grams contained in red_bike! are red, ed_, d_b, _bi, bik, ike, ke!. These shorter strings may be useful because they capture different aspects of style such as the use of punctuation or certain prefixes/suffixes. They also remove any ambiguities in word extraction and work for all languages. In order to use these features in the SVM classifier, we need to create a feature matrix $X$ where $X_{ij}$ is the frequency of the jth n-gram in the ith document. Thus, each document is represented as a vector in $k$ dimensional space, where $k$ is the number of unique n-grams selected from the training documents. We&#39;ll also normalize each vector so that all points are mapped onto the surface of the $k$-dimensional unit sphere while preserving the angles between the vectors; this should help the SVM performance a bit. . from collections import Counter def get_ngrams(text, n): return [text[i - n : i] for i in range(n, len(text) + 1)] def get_ngrams_in_range(text, min_n, max_n): ngrams = [] for n in range(min_n, max_n + 1): ngrams.extend(get_ngrams(text, n)) return ngrams def sort_by_val(dictionary, max_items=None, reverse=True): n_items = len(dictionary) if max_items is None or max_items &gt; n_items: max_items = n_items sorted_key_val_list = sorted(dictionary.items(), key=lambda item: item[1], reverse=reverse) return {k: v for k, v in sorted_key_val_list[:max_items]} class NgramExtractor: def __init__(self, ngram_range=(3, 5)): self.vocab = {} self.set_ngram_range(ngram_range) def set_ngram_range(self, ngram_range): self.min_n, self.max_n = ngram_range def build_vocab(self, texts, max_features=None): self.vocab, index = {}, 0 for n in range(self.min_n, self.max_n + 1): ngrams = [] for text in texts: ngrams.extend(get_ngrams(text, n)) counts = sort_by_val(Counter(ngrams), max_features) for ngram, count in counts.items(): self.vocab[ngram] = (index, count) index += 1 def create_feature_matrix(self, texts, norm_rows=True): X = np.zeros((len(texts), len(self.vocab))) for text_index, text in enumerate(texts): ngrams = get_ngrams_in_range(text, self.min_n, self.max_n) for ngram, count in Counter(ngrams).items(): if ngram in self.vocab: term_index = self.vocab[ngram][0] X[text_index, term_index] = count if norm_rows: X = np.apply_along_axis(lambda row: row / np.linalg.norm(row), 1, X) return X . Now we need to decide which value(s) of n to use as features. Let&#39;s look at the distribution of n-grams in the training documents. . extractor = NgramExtractor(ngram_range=(1, 15)) extractor.build_vocab(texts_train) len_counts = Counter([len(ngram) for ngram in extractor.vocab.keys()]) fig, axes = plot.subplots(ncols=2, sharex=False) x, y = zip(*len_counts.items()) axes[0].barh(x, y, color=&#39;k&#39;) axes[1].barh(x, np.log10(y), color=&#39;k&#39;) axes.format(yticks=x, ytickminor=False, ylabel=&#39;n&#39;, suptitle=&#39;Distribution of character n-grams in training text&#39;) axes[0].format(xlabel=&#39;Counts&#39;, xformatter=&#39;sci&#39;) axes[1].format(xlabel=&#39;log$_{10}$(Counts)&#39;, xformatter=&#39;sci&#39;) for ax in axes: ax.grid(axis=&#39;x&#39;) . . The total number of n-grams with 1 $ le$n $ le$ 15 is about 31 million; training a classifier on data with this number of dimensions is probably infeasible, and even more so on a larger data set. Previous studies have apparently had success with fixing the value of n to be either 3, 4, or 5, so the authors chose to restrict their attention to these values. Their new idea was to use all n-grams in the range 3 $ le$n $ le$ 5. This leaves a few hundred thousand features. . The next section will discuss statistical methods to prune the features; for now, though, we&#39;ll implement the simple method of keeping the $k$ most frequent across all the training documents. As long as this doesn&#39;t affect the accuracy too much, we reap the benefits of a reduction in computational time and the ability to fix the feature space dimensionality for comparison of different feature types. To see why many low frequency terms may be unimportant, suppose one of the authors wrote a single article about sharks in the training set. The the term &quot;shark&quot; would have a small global frequency and be very useful in the training set since no other writers write about sharks, but it&#39;s probabably a good idea to discard it since its unlikely to appear in the testing set. We must be careful, however, because some low-frequency terms could be important. These are probably terms which an author uses rarely but consistently over time. Maybe they like to use &quot;incredible&quot; as an adjective; the global frequency of &quot;incred&quot; would be much less than, say, &quot;that_&quot;, but it&#39;s valuable because its frequency distribution will likely be the same in future writing. A quick test on our data set shows that $k$ = 15,000 is a good number. Let&#39;s try this out on the 15,000 most frequent 3-grams. . ngram_range = (3, 3) max_features = 15000 norm_rows = True extractor = NgramExtractor(ngram_range) extractor.build_vocab(texts_train, max_features) X_train = extractor.create_feature_matrix(texts_train, norm_rows) X_test = extractor.create_feature_matrix(texts_test, norm_rows) . Here are some of the values in X_train. The columns have been sorted by descending frequency from left to right. . _th he_ the _in ed_ _to ng_ ing to_ _of ... Agi gip L_I +12 n t t _VW VW_ ig. kw_ d-n . 0 0.129339 | 0.129339 | 0.086226 | 0.129339 | 0.086226 | 0.043113 | 0.129339 | 0.129339 | 0.043113 | 0.086226 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 0.278956 | 0.266276 | 0.272616 | 0.088759 | 0.088759 | 0.044379 | 0.145818 | 0.133138 | 0.044379 | 0.069739 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 0.308339 | 0.247880 | 0.266018 | 0.108825 | 0.102780 | 0.102780 | 0.120917 | 0.114871 | 0.102780 | 0.096734 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 0.244807 | 0.244807 | 0.231207 | 0.102003 | 0.108803 | 0.102003 | 0.061202 | 0.068002 | 0.108803 | 0.108803 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 0.231920 | 0.248486 | 0.207072 | 0.066263 | 0.149092 | 0.115960 | 0.066263 | 0.066263 | 0.074546 | 0.082829 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2495 0.148444 | 0.197926 | 0.141375 | 0.091894 | 0.077756 | 0.127238 | 0.247407 | 0.162582 | 0.106032 | 0.162582 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2496 0.264067 | 0.193649 | 0.211254 | 0.146704 | 0.158440 | 0.082154 | 0.146704 | 0.228858 | 0.082154 | 0.193649 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2497 0.346437 | 0.276067 | 0.276067 | 0.092022 | 0.151566 | 0.113675 | 0.276067 | 0.151566 | 0.102849 | 0.151566 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2498 0.286280 | 0.293262 | 0.251367 | 0.125684 | 0.181543 | 0.111719 | 0.104736 | 0.076807 | 0.104736 | 0.160596 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2499 0.334275 | 0.227915 | 0.205123 | 0.159540 | 0.151943 | 0.129152 | 0.091166 | 0.113957 | 0.121554 | 0.189929 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2500 rows × 15000 columns . We can now feed this array to the SVM and make predictions on the testing data. I&#39;ll keep the $C$ parameter fixed at $C = 1$ in all cases since this is what is done in the paper (I tried a few different values of $C$ and there wasn&#39;t a large effect on the accuracy). Here is the confusion matrix obtained after training and testing: . from sklearn.metrics import accuracy_score, confusion_matrix import seaborn as sns clf = svm.LinearSVC(C=1) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) acc = accuracy_score(y_test, y_pred) cmat = confusion_matrix(y_test, y_pred) fig, ax = plt.subplots(figsize=(4.5, 3.5)) sns.heatmap(cmat, ax=ax, cmap=&#39;binary&#39;, cbar_kws=dict(label=&#39;Number of documents&#39;)) ax.set_title(&#39;Confusion Matrix (accuracy = {:.3f})&#39;.format(acc)) ax.set_ylabel(&#39;True class&#39;); ax.set_xlabel(&#39;Predicted class&#39;); . . Feature selection . In the rest of this post, we&#39;ll study how to use statistical methods to further eliminate features from this initial set of 15,000. This process of selecting features which are &quot;best&quot; in a statistical sense is known as feature selection. . Information gain . A classical statistical measure of feature &quot;goodness&quot; is called information gain (IG). The idea is that knowing whether or not a term t is found in a document of a known class $c$ gives information about $c$, and that some terms will contribute more information than others. The information gain can be written as [8] . $$ IG(t) = p(t) sum_{i=1}^{m}p(c_i | t) log p(c_i | t) + p( bar{t}) sum_{i=1}^{m}p(c_i | bar{t}) log p(c_i | bar{t}) - sum_{i=1}^{m}p(c_i) log p(c_i). tag{5}$$ . The probability of choosing term $t$ out of all terms in the corpus is given by $p(t)$, and $p(t) + p( bar{t}) = 1$. Similarly, $p(c_i)$ is the probability that a randomly chosen document belongs to class $c_i$, and $p(c_i) + p( bar{c_i}) = 1$. The probability that a document belongs to $c_i$ given that it contains $t$ is $p(c_i | t)$, or $p(c_i | bar{t})$ if it doesn&#39;t contain $t$. The strategy is then to keep the terms with the highest information gain scores. . class InfoGainSelector: def __init__(self): self.idx = None def fit(self, X, y): # Compute probability distributions n_docs, n_terms = X.shape n_classes = len(np.unique(y)) P_c_and_t = np.zeros((n_classes, n_terms)) for doc_index, class_index in enumerate(y): P_c_and_t[class_index, :] += (X[doc_index, :] &gt; 0).astype(int) P_c_and_t /= np.sum(P_c_and_t) P_t = np.sum(P_c_and_t, axis=0) P_c = np.sum(P_c_and_t, axis=1) P_c_given_t = P_c_and_t / P_t P_c_given_tbar = 1 - (1 - P_c_and_t) / (1 - P_t) # Compute information gain for each feature def XlogX(X): return X * np.log2(X, out=np.zeros_like(X), where=(X &gt; 0)) scores = np.zeros(n_terms) scores += np.sum(P_t * XlogX(P_c_given_t), axis=0) scores += np.sum((1 - P_t) * XlogX(P_c_given_tbar), axis=0) scores -= np.sum(XlogX(P_c)) self.idx = np.argsort(scores) def select(self, X, k=-1): return X[:, self.idx[:k]] . We&#39;ll now compare 4 sets of 15,000 features: 3-grams, 4-grams, 5-grams, and equal parts 3/4/5-grams, each time using IG to select the best $k$ features and plotting the accuracy vs. $k$. I&#39;ll start from $k$ = 1 to 200. . extractor = NgramExtractor() selector = InfoGainSelector() clf = svm.LinearSVC(C=1) def compare_acc_ig(ngram_ranges, kmin, kmax, kstep): n_keep = np.arange(kmin, kmax + kstep, kstep).astype(int) accuracies = np.zeros((len(ngram_ranges), len(n_keep))) for i, ngram_range in enumerate(ngram_ranges): extractor.set_ngram_range(ngram_range) max_features = 5000 if ngram_range == (3, 5) else 15000 extractor.build_vocab(texts_train, max_features) X_train = extractor.create_feature_matrix(texts_train) X_test = extractor.create_feature_matrix(texts_test) selector.fit(X_train, y_train) for j, k in enumerate(n_keep): X_train_red = selector.select(X_train, k) X_test_red = selector.select(X_test, k) clf.fit(X_train_red, y_train) y_pred = clf.predict(X_test_red) accuracies[i, j] = accuracy_score(y_test, y_pred) return accuracies def plot_accs(accuracies, kmin, kmax, kstep): fig, ax = plot.subplots(figsize=(4, 3)) for i in range(4): m = [&#39;D&#39;, &#39;s&#39;, &#39;^&#39;, &#39;s&#39;][i] mfc = [None, None, &#39;w&#39;, &#39;w&#39;][i] ax.plot(np.arange(kmin, kmax + kstep, kstep), accuracies[i, :], marker=m, mew=1, mfc=mfc) ax.format(title=&#39;Information Gain feature selection&#39;) ax.format(xlabel=&#39;Number of features selected (k)&#39;) ax.format(ylabel=&#39;Accuracy&#39;) ax.legend(labels=[&#39;n = 3&#39;, &#39;n = 4&#39;, &#39;n = 5&#39;, &#39;n = (3, 4, 5)&#39;], ncols=1); return ax . . The accuracy at $k$ = 1 is 0.04, so using the feature with the highest IG score is actually twice as effective as random guessing! By the end of the plot 3-grams and variable length n-grams have taken a clear leaad, with 5-grams in last place. The performance gap between the different n-grams also appears to be growing with $k$. The next region we&#39;ll look at is $200 le k le 2000$. . Now the gap is decreasing as we approach an upper performance limit at higher $k$, especially for 3-grams. We&#39;ll now look at the region which is plotted in the paper: $2,000 le k le 10,000$. . One interesting thing is that 5-grams make a big jump from last place to first place. I&#39;m not sure if I have any deep insights into this behavior, but it&#39;s interesting that the best n-gram to choose depends on the number of features selected. Now, I should compare with Fig. 1 from the paper: . . The first difference is the maximum achieved accuracy which is a few percentage points higher. The second difference is that the authors found 3-grams to be worst at low $k$ and best at high $k$.and the opposite for 5-grams. I&#39;ll leave this as an open problem for now. . LocalMaxs algorithm . Let&#39;s look at the top IG scoring n-grams from from the variable-length feature set. . extractor.set_ngram_range((3, 5)) extractor.build_vocab(texts_train, max_features=5000) X_train = extractor.create_feature_matrix(texts_train) X_test = extractor.create_feature_matrix(texts_test) selector.fit(X_train, y_train) def get_term(i): for key, (idx, count) in extractor.vocab.items(): if idx == i: return key for rank, i in enumerate(selector.idx[:10], start=1): print(&#39;{:02}. {}&#39;.format(rank, get_term(i))) . . 01. _th 02. _the_ 03. the_ 04. _to 05. _the 06. _in 07. he_ 08. ed_ 09. the 10. on_ . Notice all the variants of the which were included. IG has no way of knowing that these are basically the same. This motivates the definition of something called &quot;glue&quot;. Consider the word bigram Amelia Earhart. These two words are very likely to be found next to each other and could probably be treated as a single multi-word unit; it is as if there is a glue holding the two words together. The amount of glue is probably higher than that between, say, window and Earhart. A technique has been developed to quantify this glue and extend its calculation to word n-grams instead of just word bi-grams [9]. The same idea can then be applied to character n-grams. . Let $g(C)$ be the glue of character n-gram $C = c_1 dots c_n$. Assuming we had a way to calculate the glue, how could this concept be used for feature selection? One solution is called the LocalMaxs algorithm. First define an antecedent $ant(C)$ as an (n-1)-gram which is contained in $C$, e.g., &quot;string&quot; $ rightarrow$ &quot;strin&quot; or &quot;tring&quot;. Then define a successor $succ(C)$ as an (n+1)-gram which contains $C$, e.g., &quot;string&quot; $ rightarrow$ &quot;strings&quot; or &quot;astring&quot;. C is selected as a feature if . $$ g(C) ge g(ant(C)) , , and , , g(C) &gt; g(succ(C)) tag{6}$$ . for all ant(C) and succ(C). Since we&#39;re dealing with 3 $ le$ n $ le$ 5, only the latter condition is checked if n = 3, and only the former condition is checked for n = 5. Eq. (6) says that the glue of a selected feature shouldn&#39;t increase by adding a character to or removing a character from the start or end of the n-gram, i.e., the glue is at a local maximum with respect to similar n-grams. Now that the selection criteria are established, we can move on to calculating the glue. Here there are several options, but the one used in the paper is called symmetrical conditional probability (SCP). If we have a bigram $C = c_1c_2$, then . $$ SCP(c_1c_2) = p(c_1|c_2) cdot p(c_2|c_1) = frac{p(c_1,c_2)^2}{p(c_1)p(c_2)}, tag{7}$$ . so SCP is a measure of how likely one character is given the other and vice versa. This formula can be applied to an n-gram $C = c_1 dots c_n$ by performing a pseudo bigram transformation, which means splitting the n-gram into two parts at a chosen dispersion point; for example, &quot;help&quot; could be split as &quot;h*elp&quot;, &quot;he*lp&quot;, or &quot;hel*p&quot;, where is the dispersion point. Splitting $C$ as $c_1 dots c_{n-1}$$c_n$ would give . $$ SCP((c_1 dots c_{n-1})c_n) = frac{p(c_1 dots c_n)^2}{p(c_1 dots c_{n-1})p(c_n)}. tag{8}$$ . Of course, the answer will depend on the dispersion point. We therefore introduce the FairSCP which averages over the possible dispersion points: . $$ FairSCP(c_1 dots c_n) = frac{p(c_1 dots c_n)^2}{ frac{1}{n-1} sum_{i=1}^{n-1} p(c_1 dots c_i)p(c_{i+1} dots c_n)}. tag{9}$$ . In summary, LocalMaxs loops through every n-gram in the vocabulary, computes the glue as $g(C) = FairSCP(C)$, and keeps the n-gram if Eq. (6) is satisfied. It differs from IG selection in that the features are not ranked, so the number of selected features is completely determined by the text. The method is implemented below. . import string def antecedents(ngram): return [ngram[:-1], ngram[1:]] def successors(ngram, characters=None): if characters is None: characters = string.printable successors = [] for character in characters: successors.append(character + ngram) successors.append(ngram + character) return successors . class LocalMaxsExtractor(NgramExtractor): def __init__(self, ngram_range=(3, 5)): super().__init__(ngram_range) self.counts_list = [] # ith element is dictionary of unique (i+1)-gram counts self.sum_counts_list = [] # ith element is the sum of `counts_list[i].values()` def build_vocab(self, texts, max_features=None): # Count all n-grams with n &lt;= self.max_n self.counts_list, self.sum_counts_list = [], [] candidate_ngrams = {} for n in range(1, self.max_n + 1): ngrams = [] for text in texts: ngrams.extend(get_ngrams(text, n)) counts = Counter(ngrams) self.counts_list.append(counts) self.sum_counts_list.append(sum(counts.values())) if self.min_n &lt;= n &lt;= self.max_n: candidate_ngrams.update(sort_by_val(counts, max_features)) self.available_characters = self.counts_list[0].keys() # Select candidate n-grams whose glue is at local maximum self.vocab, index = {}, 0 for ngram, count in candidate_ngrams.items(): if self.is_local_max(ngram): self.vocab[ngram] = (index, count) index += 1 def is_local_max(self, ngram): glue, n = self.glue(ngram), len(ngram) if n &lt; self.max_n: for succ in successors(ngram, self.available_characters): if self.glue(succ) &gt;= glue: return False if n &gt; self.min_n: for ant in antecedents(ngram): if self.glue(ant) &gt; glue: return False return True def glue(self, ngram): n = len(ngram) P = self.counts_list[n-1].get(ngram, 0) / self.sum_counts_list[n-1] if P == 0: return 0.0 Avp = 0.0 for disp_point in range(1, n): ngram_l, ngram_r = ngram[:disp_point], ngram[disp_point:] n_l, n_r = disp_point, n - disp_point P_l = self.counts_list[n_l-1].get(ngram_l, 0) / self.sum_counts_list[n_l-1] P_r = self.counts_list[n_r-1].get(ngram_r, 0) / self.sum_counts_list[n_r-1] Avp += P_l * P_r Avp /= (n - 1) return P**2 / Avp . The first thing we should do is check the the glue of the derivative n-grams the, _the, etc. . extractor = LocalMaxsExtractor(ngram_range=(3, 5)) extractor.build_vocab(texts_train) for ngram in [&#39;the&#39;, &#39;_the&#39;, &#39;the_&#39;, &#39;_the_&#39;]: glue = extractor.glue(ngram) selected = extractor.is_local_max(ngram) freq = extractor.counts_list[len(ngram) - 1][ngram] print(&#39;{:&lt;5}: glue = {:.4f}, selected = {}&#39;.format(ngram, glue, selected)) . the : glue = 0.0856, selected = True _the : glue = 0.0846, selected = False the_ : glue = 0.0748, selected = False _the_: glue = 0.0808, selected = False . It seems to be working correctly. Now we&#39;d like to compare the performance to IG. There&#39;s no way to directly compare since LocalMaxs doesn&#39;t rank features; however, it&#39;s possible to vary the size of the initial set of features from which LocalMaxs makes its selections. Below, this initial size is varied from 3,000 to 24,000 using equal parts 3/4/5 grams as features. . lm_extractor = LocalMaxsExtractor(ngram_range=(3, 5)) clf = svm.LinearSVC(C=1) max_features_list = np.arange(2000, 8000, 1000).astype(int) lm_accuracies, lm_vocabs = [], [] for max_features in max_features_list: lm_extractor.build_vocab(texts_train, max_features) X_train_red = lm_extractor.create_feature_matrix(texts_train) X_test_red = lm_extractor.create_feature_matrix(texts_test) clf.fit(X_train_red, y_train) y_pred = clf.predict(X_test_red) lm_accuracies.append(accuracy_score(y_test, y_pred)) lm_vocabs.append(lm_extractor.vocab) n_keep = [len(vocab) for vocab in lm_vocabs] ig_extractor = NgramExtractor(ngram_range=(3, 5)) ig_extractor.build_vocab(texts_train, max_features=5000) X_train = ig_extractor.create_feature_matrix(texts_train) X_test = ig_extractor.create_feature_matrix(texts_test) ig_selector = InfoGainSelector() ig_selector.fit(X_train, y_train) ig_accuracies, ig_vocabs = [], [] for lm_vocab in lm_vocabs: k = len(lm_vocab) X_train_red = ig_selector.select(X_train, k) X_test_red = ig_selector.select(X_test, k) clf.fit(X_train_red, y_train) y_pred = clf.predict(X_test_red) ig_accuracies.append(accuracy_score(y_test, y_pred)) ig_vocabs.append(extractor.vocab) fig, ax = plot.subplots(figsize=(4, 3)) ax.plot(n_keep, lm_accuracies, c=&#39;k&#39;, marker=&#39;D&#39;) ax.plot(n_keep, ig_accuracies, marker=&#39;D&#39;, c=&#39;red8&#39;) ax.format(xlabel=&#39;Number of features selected&#39;, ylabel=&#39;Accuracy&#39;, title=&#39;IG vs. LocalMaxs feature selection&#39;) ax.legend(labels=[&#39;n = (3, 4, 5) — LocalMaxs&#39;, &#39;n = (3, 4, 5) — IG&#39;], ncols=1); . . As you can see, LocalMaxs achieves a higher accuracy with the same number of features. The neat thing is that the vocabularies are totally different; for example, at the last data point, only about 15% of the n-grams are found in both sets! Let&#39;s count the number of related n-grams in the two sets, where x is related to y if x is an antecedent or successor of y. . def count_related(ngrams): count = 0 for n1 in ngrams: for n2 in ngrams: if n1 != n2 and n1 in n2: count += 1 break return count lm_ngrams = list(lm_vocabs[-1]) vocab_size = len(lm_ngrams) ig_vocab = list(ig_extractor.vocab) ig_ngrams = [ig_vocab[i] for i in ig_selector.idx[:vocab_size]] shared = len([ig_ngram for ig_ngram in ig_ngrams if ig_ngram in lm_ngrams]) print(&#39;Vocab size: {}&#39;.format(vocab_size)) print(&#39;n-grams selected by both IG and LM: {}&#39;.format(shared)) print(&#39;IG related n-grams: {}&#39;.format(count_related(ig_ngrams))) print(&#39;LM related n-grams: {}&#39;.format(count_related(lm_ngrams))) . . Vocab size: 2706 n-grams selected by both IG and LM: 426 IG related n-grams: 1413 LM related n-grams: 178 . As mentioned earlier, IG selects many related terms such as the and the_. The LocalMaxs vocabulary is much &quot;richer&quot;, as the authors put it. Here is the corresponding figure from the paper (ignore the white squares): . . For some reason, their implementation extracted way more features than mine did. I don&#39;t have access to the author&#39;s code, and I couldn&#39;t find any implementation of LocalMaxs online, so it&#39;s hard for me to say what&#39;s happening. I&#39;m happy with my implementation since it exhibits the expected behavior (less related terms, better performance at lower feature numbers). . Conclusion . This post summarized a research paper in the field of Natural Language Processing (NLP) which focused on feature selection techniques. I didn&#39;t exactly reproduce the authors&#39; results, so if anyone reads this (unlikely) and finds a mistake, I would love to know about it. . In a future post I may apply these methods to my own data set; I&#39;m particularly interested in what would happen with Chinese characters. There are, of course, a ton of different techniques and experiments to explore which involve NLP. A different problem I&#39;d like to examine is that of artist identification; the problem would be to match a collection of paintings with their painters. The Web Gallery of Art is a potential database that I found after a quick search, and I&#39;m sure there are others. This would give me the chance to learn about image classification techniques. . References . [1] J. Houvardas &amp; E. Stamatatos, &quot;N-Gram Feature Selection for Authorship Identification,&quot; In J. Euzenat, &amp; J. Domingue,eds., Artificial Intelligence: Methodology, Systems, and Applications (Berlin, Heidelberg: Springer Berlin Heidelberg, 2006), pp. 77–86. . [2] A. Douglass, &quot;The Authorship of the Disputed Federalist Papers,&quot; The William and Mary Quarterly 1:97–122 (1944). . [3] D. Holmes, &quot;The Evolution of Stylometry in Humanities Scholarship,&quot; Literary and Linguistic Computing 13:111–117 (1998). . [4] T. C. Mendenhall, &quot;THE CHARACTERISTIC CURVES OF COMPOSITION,&quot; Science ns-9:237–246 (1887). https://doi.org/10.1126/science.ns-9.214S.237. . [5] S. T. Piantadosi, &quot;Zipf’s word frequency law in natural language: A critical review and future directions,&quot; Psychon Bull Rev 21:1112–1130 (2014). https://doi.org/https://doi.org/10.3758/s13423-014-0585-6. . [6] M. Cristelli, M. Batty, &amp; L. Pietronero, &quot;There is More than a Power Law in Zipf,&quot; Sci Rep 2 (2012). https://doi.org/https://doi.org/10.1038/srep00812. . [7] C. M. Bishop, Pattern Recognition and Machine Learning (Springer, 2006). . [8] Y. Yang &amp; J. O. Pedersen, &quot;A Comparative Study on Feature Selection in Text Categorization,&quot; ICML (1997). . [9] J. Silva, &quot;A Local Maxima method and a Fair Dispersion Normalization for extracting multi-word units from corpora,&quot; (2009). .",
            "url": "https://austin-hoover.github.io/blog/stylometry/machine%20learning/natural%20language%20processing/2021/04/29/authorship_identification.html",
            "relUrl": "/stylometry/machine%20learning/natural%20language%20processing/2021/04/29/authorship_identification.html",
            "date": " • Apr 29, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Nonlinear resonances",
            "content": "Most of us are familiar with the experience of pushing someone else on a playground swing. We intuitively know that we should sync our pushes with the swing oscillation frequency, which appears to be independent of the swing amplitude. This strategy employs the idea of a resonance, which is an increase of the oscillation amplitude of a system for certain driving frequencies. In this post we first review the mathematics of this simple example, then extend the machinery to the nonlinear dynamics in a particle accelerator. My goal here is to write down the main results which are relevant to accelerators in order to improve my own understanding of the topic. . . Linear resonances . Consider a mass on a spring which, if left alone, oscillates at freqency $ omega_0^2$. . . The equation of motion for $x$ is . $$ frac{d^2}{dt^2}{x} + omega_0^2x = 0. tag{1}$$ . Now consider a sinusoidal driving force $f(t) = f_0 cos( omega t)$ as well as a damping term: . $$ frac{d^2}{dt^2}{x} + b dot{x} + omega_0^2x = f_0 cos( omega t).$$ . After doing some work it can be shown that the gravitational, damping, and driving forces initially fight against each other, but in the end the driving force dominates and the position oscillates as . $$ x(t) = A cos( omega t - delta) tag{2}$$ . where . $$A^2 = frac{f_0^2}{( omega - omega_0)^2 + b omega^2}. tag{3}$$ . The figure below shows the squared amplitude as the driving frequency is varied. The maximum amplitude approaches infinity as the damping term goes to zero. . The next step is to consider what happens when the driving force is not a pure sine wave. We&#39;ll only consider periodic driving forces, and any periodic function can be written as a sum of sines and cosines of different frequencies. Assuming $f(t)$ is an even function so that we can drop the sine terms in the Fourier expansion, the equation of motion becomes . $$ ddot{x} + b dot{x} + omega_0^2 x = sum_{n=0}^{ infty} {f_n cos(n omega t)}. tag{4}$$ . The long-term solution is found by just adding up the solutions to each term in the sum: . $$x(t) = sum_{n = 0}^{ infty}{A_n cos{(n omega t - delta_n)}}, tag{5}$$ . where $A_n$ is given by Eq. (3) for the frequency $n omega$. The resonance condition will apply to each of these amplitudes individually, which means that a resonance could be excited if any component of the driving force is near the natural frequency. . Sources of nonlinearity . We&#39;re now going to apply these ideas to a particle accelerator. We&#39;ll assume small transverse oscillations, no acceleration, no deviation from the design momentum, and no particle-particle interactions. Under these assumptions, the transverse equation of motion of a particle with charge $q$ and momentum $p$ in a magnetic field $ mathbf{B} = (B_x, B_y)^T$ is . $$ x&#39;&#39; = - frac{q}{p} B_y(s), tag{6}$$ $$ y&#39;&#39; = + frac{q}{p} B_x(s). $$ . Remember that $x&#39; = dx/ds$, and $s$ is the position in accelerator (from now on we&#39;ll assume a circular accelerator or &quot;ring&quot; of circumference $L$). Any 2D magnetic field can be expanded as the following infinite sum: . $$B_y - iB_x = sum_{n=1}^{ infty} left({b_n - ia_n} right) left( frac{x + iy}{r_0} right)^{n-1}, tag{7}$$ . where $r_0$ is a constant. The $b_n$ and $a_n$ terms are called the multipole coefficients and skew multipole coefficients, respectively. The $n^{th}$ term in the expansion is the field produced by $2n$ symmetrically arranged magnetic poles. . . We can see that terms with $n &gt; 2$ introduce nonlinear powers of $x$ and $y$ on the right side of Eq. (6), while terms with $n le 2$ introduce linear or constant terms. One may ask why we are considering a general magnetic field when in reality we use only dipoles and quadrupoles. The answer is two-fold. First, the best we can do in a real magnet is to make the $n &gt; 2$ terms as small as possible; they aren&#39;t zero and we need to know how they affect the motion. Second, sextupoles (and sometimes even octopoles) can be introduced intentionally. Their primary use is to correct for the fact that not all beam particles have the same momentum. . . An example of a sextupole electromagnet. Credit: CERN. Perturbation analysis . The nonlinear terms in Eq. (6) eliminate any hope of an analytic solution. There are two options in situations such as these: 1) use a computer, or 2) use perturbation theory. The strategy of option 2 is to make approximations until an exact solution can be found, then to add in small nonlinear terms and see how the solution changes. The process can be repeated to solve the problem up to a certain order of accuracy. Usually this is infeasible beyond a few iterations, but it is a helpful tool for gaining intuition and interpreting numerical results. In particular, we&#39;ll be looking for regions where the particle may encounter a resonance. Without many details, let&#39;s try out the perturbation approach. Later on we&#39;ll use a computer and see if our analysis was accurate. . Floquet coordinates . The first step is to find an exact solution under some approximation. We&#39;ll neglect coupling by setting $y = 0$ and focus on one dimension to make things easier. Let&#39;s denote the linear focusing from the lattice by $k$, with all other terms in the field expansion folded into $ Delta B$ (there are still $n = 1$ and $n = 2$ terms in $ Delta B$, but they represent deviations from the design values). We&#39;re also assuming that these variables are normalized by the ratio $q / p$. This results in the equation of motion . $$ x&#39;&#39; + k(s)x = Delta B. tag{8}$$ . This is Hill&#39;s equation with a nonlinear driving term. The stable solution when $ Delta B = 0$ is . $$x(s) = sqrt{ epsilon beta(s)} cos left({ mu(s) + delta} right), tag{9}$$ . with the phase advance is given by . $$ mu(s) = int_{0}^{s}{ frac{ds}{ beta(s)}}. tag{10}$$ . These pseudo-harmonic oscillations are still a bit difficult to visualize, so it&#39;s helpful to perform the Floquet transformation which scales the $x$ coordinate as . $$x(s) rightarrow u(s) = frac{x(s)}{ sqrt{ beta_x(s)}}. tag{11}$$ . Furthermore, it is convenient to replace the $s$ coordinate with . $$ phi(s) = frac{1}{ nu_0} int_{0}^{C}{ frac{ds}{ beta_x(s)}}. tag{12}$$ . Here $ nu_0$ is the number of phase space oscillations per trip around the ring. As a result, the unperturbed equation of motion becomes (with $ dot{x} = dx / d phi$) . $$ ddot{u} + nu_0^2 u = 0. tag{13}$$ . But this is just a harmonic oscillator! The trajectory in phase space is a circle, and the particle revolves once around this circle for every turn around the ring. Finally, we can write $ Delta B$ as a power series in $u$ and derive the equation of motion in Floquet coordinates: . $$ ddot{u} + nu_0^2 u = - nu_0^2 beta^{3/2} Delta B = - nu_0^2 sum_{n=0}^{ infty} left({ beta^{ frac{n + 3}{2}} b_{n+1}} right) u^n. tag{14}$$ . Fourier expansion . The tools to analyze driven harmonic oscillators are now available to us. Similar to Eq. (4), each term on the right hand side can be Fourier expanded, the reason being that $ beta$ (the oscillation amplitude of the unperturbed motion) and $b_n$ (a multipole coefficient) depend only on the position in the ring, so of course they are periodic in $ phi$. Grouping these terms together and performing the expansion gives . $$ ddot{u} + nu_0^2 u = - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} C_{n,k} , u^n , e^{ik phi}. tag{16}$$ . We&#39;re now going to linearize this equation. This means plugging in $u = u_0 + delta u$, where $u_0$ is the unperturbed solution and $ delta_u$ is small, and discarding all higher powers of $ delta_u$. This gives . $$ ddot{ delta u} + nu_0^2 delta u approx - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} C_{n,k} , u_0^n , e^{ik phi} . tag{17}$$ . This equation tells us how the perturbation evolves with time — ideally it remains finite, but at a resonant condition it will grow without bound. The final step is to write $u_0^n$ in a managable form. There is this trick involving the binomial expansion: . $$ u_0^n propto cos^n( nu phi) = frac{1}{2^n} sum_{m=0}^{n} binom{n}{m} e^{i(n-2m) nu_0 phi}. tag{17}$$ . So, we finally arrive at . $$ ddot{ delta u} + nu_0^2 delta u approx - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} sum_{m=0}^{n} {n choose m} frac{C_{n,k}}{2^n} e^{i left[(n - 2m) nu_0 + k right] phi}. tag{18} $$ . There are a lot of indices floating around; $n$ is one less than the multipole coefficient of the magnetic field, $k$ is for the Fourier expansion, and $m$ is just a dummy index we used to binomially expand $u_0^2$. . Resonance diagram . Eq. (18) describes a driven harmonic oscillator like Eq. (5), so we can expect a resonance condition to occur when any of the frequency components of the driving force are close to the natural frequency $ nu_0$. In other words, a resonance could occur when . $$ (n - 2m) nu_0 + k = pm nu_0. tag{19}$$ . If you write out the different cases ($n$ = 0, 1, 2, ...), you&#39;ll find that dipole terms ($n = 0$) forbid integer tunes, quadrupole terms forbid 1/2 integer tunes, sextupole terms forbid 1/3 integer tunes, and so on. The same thing can be done for the vertical dimension. Once coupling is included between $x$ and $y$, we&#39;re lead to the definition of resonance lines: . $$ M_x nu_x + M_y nu_y = N, tag{20}$$ . where $M_x$, $M_y$, and $N$ are integers and $|M_x| + |M_y|$ is the order of the resonance. The reason for calling these resonance lines is because they define lines in $ nu_x$-$ nu_y$ space (tune space). You can click through the following animation to see how the lines fill up the space as higher order resonances are included. . &lt;/input&gt; Once Loop Reflect Resonance strengths tend to decrease with order number, so people generally don&#39;t consider anything beyond order 3 or 4. That being said, the machine tunes $ nu_x$ and $ nu_y$ need to be carefully chosen to avoid all low order resonance lines. Ideally all beam particles occupy this single point in tune space, but space charge complicates things by decreasing the tune by different amounts for each particle, possible placing them on one of the above resonance lines. This effect, called tune spread, places a fundamental limit on the number of particles in the beam. . Numerical exploration of the sextupole . Let&#39;s explore the behavior of a beam under the influence of a sextupole magnet. This section recreates some figures from the book Accelerator Physics by S. Y. Lee. The easiest way to do this is to approximate the multipole as an instantaneous change to the slope of the particle&#39;s trajectory. This is valid if the magnet isn&#39;t too long. . import numpy as np import numpy.linalg as la import scipy class Multipole: &quot;&quot;&quot;Class to apply multipole kick to particle. Adapted from PyORBIT tracking routine in `py-orbit/src/teapotbase.cc`. Attributes - order : int The order of the multipole term (dipole: 1, quadrupole: 2, ...). strength : float Integrated multipole strength [m^-(order - 1)]. skew : bool If True, rotate the magnet 45 degrees. &quot;&quot;&quot; def __init__(self, order, strength, skew=False): self.order, self.strength, self.skew = order, strength, skew def track_part(self, vec): &quot;&quot;&quot;Apply transverse kick to particle slopes. vec : ndarray, shape (4,) Transverse phase space coordinate vector [x, x&#39;, y, y&#39;]. &quot;&quot;&quot; x, xp, y, yp = vec k = self.strength / np.math.factorial(self.order - 1) zn = (x + 1j*y)**(self.order- 1) if self.skew: vec[1] += k * zn.imag vec[3] += k * zn.real else: vec[1] -= k * zn.real vec[3] += k * zn.imag return vec . order = 3 strength = 0.5 multipole = Multipole(order, strength, skew=False) . The situation we&#39;ll consider is a circular lattice which is made of linear uncoupled elements + one thin sextupole. We&#39;ll observe the beam at the location of the sextupole after each turn. A key result of the linear theory is that the details of the rest of the lattice are unimportant for this task. All we need to do is choose the Twiss parameters and tune in each dimension to form the transfer matrix, then we can just track using matrix multiplication. Recall that the transfer matrix is written as $ mathbf{M} = mathbf{V P V^{-1}}$, where $ mathbf{V} = mathbf{V}( alpha_x, alpha_y, beta_x, beta_y)$ performs the Floquet normalization and $ mathbf{P} = mathbf{P}( nu_x, nu_y)$ is a rotation in the $x$-$x&#39;$ and $y$-$y&#39;$ phase spaces by the angle $2 pi nu_x$ and $2 pi nu_y$, respectively. The following class implements this representation of the lattice. . def V_2D(alpha, beta): &quot;&quot;&quot;Floquet normalization matrix in 2D phase space.&quot;&quot;&quot; return np.array([[beta, 0.0], [-alpha, 1.0]]) / np.sqrt(beta) def P_2D(tune): &quot;&quot;&quot;Phase advance matrixmin 2D phase space.&quot;&quot;&quot; phase_advance = 2 * np.pi * tune cos, sin = np.cos(phase_advance), np.sin(phase_advance) return np.array([[cos, sin], [-sin, cos]]) class Lattice: &quot;&quot;&quot;Represents lattice as linear one-turn transfer matrix + multipole kick. Attributes - M : ndarray, shape (4, 4) Linear one-turn transfer matrix. aperture : float Radius of cylindical boundary containing the particles [m]. multipole : Multipole object Must implement `track_part(vec)`, where vec = [x, xp, y, yp]. &quot;&quot;&quot; def __init__(self, alpha_x, alpha_y, beta_x, beta_y, tune_x, tune_y, aperture=0.2): &quot;&quot;&quot;Constructor. Parameters - alpha_x, alpha_y, beta_x, beta_y : float Twiss parameters at the lattice entrance. tune_x, tune_y : float Number of phase space oscillations per turn. &quot;&quot;&quot; self.P = np.zeros((4, 4)) self.V = np.zeros((4, 4)) self.M = np.zeros((4, 4)) self.P[:2, :2] = P_2D(tune_x) self.P[2:, 2:] = P_2D(tune_y) self.V[:2, :2] = V_2D(alpha_x, beta_x) self.V[2:, 2:] = V_2D(alpha_y, beta_y) self.M = la.multi_dot([self.V, self.P, la.inv(self.V)]) self.aperture = aperture self.multipole = None def add_multipole(self, multipole): self.multipole = multipole def track_part(self, vec): &quot;&quot;&quot;Track a single particle through the lattice. vec : ndarray, shape (4,) Transverse phase space coordinate vector [x, x&#39;, y, y&#39;]. &quot;&quot;&quot; vec = np.matmul(self.M, vec) if self.multipole is not None: vec = self.multipole.track_part(vec) return vec def track_bunch(self, X): &quot;&quot;&quot;Track a particle bunch through the lattice. X : ndarray, shape (nparts, 4) Transverse phase space coordinate array. &quot;&quot;&quot; X = np.apply_along_axis(self.track_part, 1, X) return self.collimate(X) def collimate(self, X): &quot;&quot;&quot;Delete particles outside aperture.&quot;&quot;&quot; radii = np.sqrt(X[:, 0]**2 + X[:, 2]**2) return np.delete(X, np.where(radii &gt; self.aperture), axis=0) def get_matched_bunch(self, nparts=2000, emittance=10e-6, cut=3.0): &quot;&quot;&quot;Generate truncated Gaussian distribution matched to the lattice.&quot;&quot;&quot; X = scipy.stats.truncnorm.rvs(a=4*[-cut], b=4*[cut], size=(nparts, 4)) A = np.sqrt(emittance) * np.identity(4) V = self.V X = np.apply_along_axis(lambda vec: np.matmul(A, vec), 1, X) X = np.apply_along_axis(lambda vec: np.matmul(V, vec), 1, X) return X . 1/3 integer resonance . We focus first on the 1/3 integer resonance. Below, a particle is tracked over 100 turns starting from few different initial amplitudes. We set $y = y&#39; = 0$ in all cases. The $x$-$x&#39;$ trajectories should be upright ellipses in the absence of nonlinear elements. Some helper functions are defined in the collapsed cell. . # Define Twiss parameters at the observation point alpha_x = alpha_y = 0.0 beta_x = beta_y = 20.0 def create_lattice(tune_x, tune_y, multipole=None): lattice = Lattice(alpha_x, alpha_y, beta_x, beta_y, tune_x, tune_y) lattice.add_multipole(multipole) return lattice def get_traj(lattice, emittance, nturns=1): &quot;&quot;&quot;Return array of shape (nturns, 4) of tracked single particle coordinates. The vertical coordinate and slope are set to zero. &quot;&quot;&quot; X = np.array([[np.sqrt(emittance * beta_x), 0, 0, 0]]) tracked_vec = [X[0]] for _ in range(nturns): X = lattice.track_bunch(X) if X.shape[0] == 0: # particle was deleted break tracked_vec.append(X[0]) return 1000 * np.array(tracked_vec) # convert from m to mm def compare_traj(tunes_x, tune_y, emittances, nturns=1, multipole=None, limits=(45, 2.5), **kws): &quot;&quot;&quot;Compare trajectories w/ different emittances as horizontal tune is scaled.&quot;&quot;&quot; kws.setdefault(&#39;s&#39;, 1) kws.setdefault(&#39;c&#39;, &#39;pink8&#39;) fig, axes = plot.subplots(nrows=2, ncols=3, figsize=(7.25, 4)) xlim, ylim = (-limits[0], limits[0]), (-limits[1], limits[1]) axes.format(xlabel=&quot;x [mm]&quot;, ylabel=&quot;x&#39; [mrad]&quot;, xlim=xlim, ylim=ylim) for ax, tune_x in zip(axes, tunes_x): lattice = create_lattice(tune_x, tune_y, multipole) for emittance in emittances: tracked_vec = get_traj(lattice, emittance, nturns) ax.scatter(tracked_vec[:, 0], tracked_vec[:, 1], **kws) ax.annotate(r&#39;$ nu_x = {:.3f}$&#39;.format(tune_x), xy=(0.7, 0.9), xycoords=&#39;axes fraction&#39;, bbox=dict(fc=&#39;w&#39;, ec=&#39;k&#39;)) return axes def track_bunch(X, lattice, nturns=1): &quot;&quot;&quot;Track and return list of coordinate array after each turn. Also return the fraction of particles which were lost (exceeded aperture) at each frame.&quot;&quot;&quot; coords, nparts, frac_lost = [X], X.shape[0], [0.0] for _ in range(nturns): X = lattice.track_bunch(X) coords.append(X) frac_lost.append(1 - X.shape[0] / nparts) return [1000*X for X in coords], frac_lost def animate_phase_space(coords, frac_lost=None, limits=(55, 5)): &quot;&quot;&quot;Create animation of turn-by-turn x-x&#39; and y-y&#39; distributions.&quot;&quot;&quot; fig, axes = plot.subplots(ncols=2, figsize=(5, 2.5), wspace = 0.75, sharey=False, sharex=False) xlim, ylim = (-limits[0], limits[0]), (-limits[1], limits[1]) axes.format(xlim=xlim, ylim=ylim) axes[0].format(xlabel=&quot;x [mm]&quot;, ylabel=&quot;x&#39; [mrad]&quot;) axes[1].format(xlabel=&quot;y [mm]&quot;, ylabel=&quot;y&#39; [mrad]&quot;) myplt.despine(axes) plt.close() kws = dict(marker=&#39;.&#39;, c=&#39;steelblue&#39;, ms=3, lw=0, markeredgewidth=0, fillstyle=&#39;full&#39;) line0, = axes[0].plot([], [], **kws) line1, = axes[1].plot([], [], **kws) def update(t): x, xp, y, yp = coords[t].T line0.set_data(x, xp) line1.set_data(y, yp) axes[0].set_title(&#39;Turn {}&#39;.format(t)) if frac_lost: axes[1].set_title(&#39;Frac. lost = {:.3f}&#39;.format(frac_lost[t])) return animation.FuncAnimation(fig, update, frames=len(coords)) . . tunes_x = np.linspace(0.61, 0.66, 6) tune_y = 0.518 emittances = 1e-6 * np.array([10, 30, 60, 90]) nturns = 100 axes = compare_traj(tunes_x, tune_y, emittances, nturns) axes.format(suptitle=&#39;Linear lattice&#39;) . Now turn on the sextupole magnet. . axes = compare_traj(tunes_x, tune_y, emittances, nturns, multipole) axes.format(suptitle=&#39;Linear lattice + sextupole&#39;) . The initially elliptical orbits are morphed into a triangular shape as the tune approaches the resonance condition, and some of the larger orbits become unstable. It turns out that by looking at the Hamiltonian you can find a triangular region defining a separatrix between stable and unstable motion. Particles inside the triangle will oscillate forever, particles at the corner of the triangle are at unstable equilibrium points, and particles outside the triangle will eventually stream outward from the corners. This is easier to see by tracking a bunch of particles. The interesting stuff will be in the horizontal plane, but I&#39;ll plot the vertical plane as well for comparison. . lattice = create_lattice(0.66, tune_y, multipole) X = lattice.get_matched_bunch() coords, frac_lost = track_bunch(X, lattice, nturns=50) animate_phase_space(coords, frac_lost) . &lt;/input&gt; Once Loop Reflect The triangular region of stability is clearly visible at the end of 50 turns. Interestingly, the third order resonance can be used to extract a beam from an accelerator at a much slower rate than normal. To do this, the strength and spacing of sextupole magnets must be carefully chosen to control the shape and orientation of the stability triangle, then tune is slowly moved closer to the 1/3 integer resonance value. The result is that the triangle shrinks as the stable phase space area decreases, and that more and more particles will find themselves in the unstable area and eventually stream out along the vertices. . Integer resonance . The sextupole should also excite the integer resonance. . compare_traj(np.linspace(0.96, 0.976, 6), tune_y, emittances, nturns, multipole, limits=(60, 2.5)); . lattice = create_lattice(0.99, 0.18, multipole) animate_phase_space(*track_bunch(X, lattice, nturns=75)) . &lt;/input&gt; Once Loop Reflect Cool pattern! The separatrix is now shaped like a tear drop. It looks like it&#39;s evolving more slowly because the tune is close to an integer, so the particles almost return to the same location in phase space after a turn. . Higher order resonances . There are also higher order resonances which a sextupole can drive. You can actually find fourth and fifth order resonances if you perform perturbation theory up to second order (at least that&#39;s what I&#39;m told in a textbook... I&#39;d like to avoid carrying out such a procedure). Do these show up using our mapping equations? They are expected to be weaker, so we&#39;ll double the sextupole strength. . emittances = 1e-6 * np.array([1, 2, 7, 10, 25, 50, 100, 150, 200, 250, 350]) compare_traj(np.linspace(0.7496, 0.798, 6), 0.23, emittances, 1000, Multipole(3, 1.0), limits=(150, 6), s=0.1); . These are really interesting plots. The tune near 0.75 (it&#39;s actually 0.7496) is exciting a fourth order resonance, while the tune near 0.8 is exciting a fifth order resonance. In all the plots, the low amplitude orbits are stable ellipses. We then see the behavior change as the amplitude is increased, with the particle jumping between distinct &quot;islands&quot;. Eventually the trajectories once again form closed loops, but in deformed shapes. The motion is unstable at even larger amplitudes. Understanding exactly why the the plots look like they do would take more work. . Conclusion . This post outlined the theory of nonlinear resonances driven by magnetic multipoles. The effect of a sextupole-driven resonance on the phase space trajectory was then examined using mapping equations. Taking the time to write down the steps which lead to Eq. (20), an equation which is often referenced in accelerator physics, was a rewarding experience and helped make the topic less mysterious to me (although I&#39;m no expert). Here are a number of helpful references: . Lectures S. Lund, Transverse Particle Resonances with Application to Circular Accelerators | E. Prebys, Resonances and Coupling | . | Textbooks D. Edwards and M. Syphers, An introduction to the Physics of High Energy Accelerators | H. Wiedemann, Particle Accelerator Physics | S. Y. Lee, Accelerator Physics | L. Reichl, The Transition to Chaos — Conservative Classical Systems and Quantum Manifestations | J. Taylor, Classical Mechanics | H. Goldstein, Classical Mechanics | . | .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/nonlinear%20systems/resonances/2021/03/28/nonlinear_resonances.html",
            "relUrl": "/physics/accelerators/nonlinear%20systems/resonances/2021/03/28/nonlinear_resonances.html",
            "date": " • Mar 28, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Particle-in-cell simulation",
            "content": "Many simulation codes exist for beam physics (one example is PyORBIT). A key component of the these simulations is the inclusion of the electromagnetic interactions between particles in the beam, also known as space charge forces. One way to compute space charge forces is the particle-in-cell (PIC) method. This post implements the PIC method in Python. . Theoretical model . We&#39;ll use bunch to refer to a group of particles in three-dimensional (3D) space, and we&#39;ll use a local cartesian coordinate system whose origin moves with the center of the bunch as shown below: . . The $s$ coordinate specifies the position of the bunch in the accelerator, and the path can be curved. Now for a few assumptions and approximations. First, assume all particles in the bunch move at a constant velocity $ beta c$, where $c$ is the speed of light. We then make the paraxial approximation. It&#39;s conventional to use the slope $x&#39; = dx/ds$ instead of the velocity, and the paraxial approximation assumes this slope is very small. Usually we report this slope in milliradians since $tan theta approx theta$ for small angles. Next we assume that the transverse ($x$-$y$) size of the bunch varies slowly along the $s$ axis. If this is true and we look at the electric field in a transverse slice of the bunch, there won&#39;t be much difference between the true field and the field of an infinitely long, uniform density cylinder. Our focus will be on the transverse dynamics of such a slice, so we&#39;ll treat each &quot;particle&quot; as an infinite line of charge. The figure below illustrates this approximation. . . Credit: G. Franchetti Another approximation is to neglect any magnetic fields generated by the beam, which is again valid if the transverse velocities are very small relative to $ beta c$. All this being said, the equations of motion without any external forces, i.e., in free space, can be written as . $$ mathbf{x}&#39;&#39; = frac{q}{mc^2 beta^2 gamma^3} mathbf{E}, tag{1}$$ . where $ mathbf{x} = [x, y]^T$ is the coordinate vector, $ mathbf{E} = [E_x, E_y]^T$ is the self-generated electric field, $m$ is the particle mass, and $ gamma = left({1 - beta^2} right)^{-1/2}$. Let&#39;s first address the factor $ gamma^{-3}$ in the equation of motion, which means that the space charge force goes to zero as the velocity approaches the speed of light. This is because parallel moving charges generate an attractive magnetic force which grows with velocity, completely cancelling the electric force in the limit $v rightarrow c$. . . Credit: OpenStax University PhysicsOne may ask: what about the rest frame in which there is no magnetic field? But special relativity says that electrogmagnetic fields change with reference frame. Using the transformations defined here, you can quickly prove that . $$ mathbf{E}_{lab} = frac{ mathbf{E}_{rest}}{ gamma}. tag{2}$$ . This inverse relationship between velocity and the space charge force has real-life consequences. It tells us that space charge is important if 1) the beam is very intense, meaning there are many particles in a small area, or 2) the beam is very energetic, meaning it is moving extremely fast. For example, space charge can usually be ignored in electron beams, which move near the speed of light for very modest energies due to their tiny mass, but is significant in high-intensity, low-energy hadron accelerators such as FRIB, SNS, and ESS. . We should now address the difficulty in determining the evolution of this system: the force on a particle in an $n$-particle bunch depends on the positions of the other $n - 1$ particles. The approach of statistical mechanics to this problem is to introduce a distribution function $f( mathbf{x}, mathbf{x}&#39;, s)$ which gives the particle density at axial position $s$ and phase space coordinates $ mathbf{x}$, $ mathbf{x}&#39;$. The Vlasov-Poisson system of equations determines the evolution of $f$ as long as we ignore collisions between particles: . $$ frac{ partial{f}}{ partial{s}} + mathbf{x}&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}}} + mathbf{x}&#39;&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}&#39;}} = 0. tag{3}$$ . We know $ mathbf{x&#39;&#39;}$ from Eq. (1). The electric field is obtained from Poisson&#39;s equation: . $$ nabla cdot mathbf{E} = - nabla^2 phi = frac{ rho}{ varepsilon_0}. tag{4}$$ . Finally, the transverse charge density $ rho$ is determined by . $$ rho = q int{f dx&#39;dy&#39;}. tag{5}$$ . Although these equations are easy to write down, they are generally impossible to solve analytically. We need to turn to a computer for help. . Computational method . The Vlasov equation could be solved directly, but this is difficult, especially in 2D or 3D. On the other end of the spectrum, the notion of a fluid in phase space could be abandoned and each particle could be tracked individually, computing the forces using direct sums. But this is infeasible with current hardware; the time complexity would by $O(n^2)$, where $n$ is the number of particles, and $n$ may be on the order of $10^{14}$. The particle-in-cell (PIC) method is a sort of combination of these two approaches. The idea is to track a group of macroparticles according to Eq. (1), each of which represents a large number of real particles. The fields, however, are solved from Eq. (4). The key step is transforming back and forth between a discrete and continuous representation of the bunch. The simulation loop for the PIC method is shown below. . . In the next sections I will discuss each of these steps and implement them in Python. Here are all the imports needed to run the code. . import numpy as np from scipy.interpolate import RegularGridInterpolator from scipy.fft import fft2, ifft2 from scipy.integrate import odeint from scipy.stats import truncnorm import Cython %load_ext cython . Let&#39;s first create a Bunch class, which is a simple container for the bunch coordinates. . class Bunch: &quot;&quot;&quot;Container for 2D distribution of positive elementary charges. Attributes - intensity : float Number of physical particles in the bunch. length : float Length of the bunch [m]. mass, kin_energy : float Mass [GeV/c^2], charge [C], and kinetic energy [GeV] per particle. nparts : float Number of macroparticles in the bunch. X : ndarray, shape (nparts, 4) Array of particle coordinates. Columns are [x, x&#39;, y, y&#39;]. Units are meters and radians. positions : ndarray, shape (nparts, 2): Just the x and y positions (for convenience). &quot;&quot;&quot; def __init__(self, intensity=1e14, length=250., mass=0.938, kin_energy=1.0): self.intensity, self.length = intensity, length self.mass, self.kin_energy = mass, kin_energy self.gamma = 1 + (kin_energy / mass) # Lorentz factor self.beta = np.sqrt(1 - (1 / self.gamma)**2) # v/c r0 = 1.53469e-18 # classical proton radius [m] self.perveance = 2 * r0 * intensity / (length * self.beta**2 * self.gamma**3) self.nparts = 0 self.compute_macrosize() self.X, self.positions = None, None def compute_macrosize(self): &quot;&quot;&quot;Update the macrosize and macrocharge.&quot;&quot;&quot; self.macrosize = self.intensity // self.nparts if self.nparts &gt; 0 else 0 def fill(self, X): &quot;&quot;&quot;Fill with particles.&quot;&quot;&quot; self.X = X if self.X is None else np.vstack([self.X, X]) self.positions = self.X[:, [0, 2]] self.nparts = self.X.shape[0] self.compute_macrosize() def compute_extremum(self): &quot;&quot;&quot;Get extreme x and y coorinates.&quot;&quot;&quot; self.xmin, self.ymin = np.min(self.positions, axis=0) self.xmax, self.ymax = np.max(self.positions, axis=0) self.xlim, self.ylim = (self.xmin, self.xmax), (self.ymin, self.ymax) . Weighting . Starting from a group of macroparticles, we need to produce a charge density $ rho_{i,j}$ on a grid. The most simple approach is the nearest grid point (NGP) method, which, as the name suggests, assigns the full particle charge to the closest grid point. This is commonly called zero-order weighting; although it is very fast and easy to implement, it is not commonly used because it can lead to significant noise. A better method called cloud-in-cell (CIC) treats each particle as a rectangular, uniform density cloud of charge with dimensions equal to the grid spacing. A fractional part of the charge is assigned based on the fraction of the cloud overlapping with a given cell. This can be thought of as first-order weighting. To get a sense of what these methods are doing (in 1D), we can slide a particle across a cell and plot the resulting density of the cell at each position, thus giving an effective particle shape. . The NGP method leads to a discontinuous boundary while the CIC method leads to a continous boundary (but discontinous derivative). There are also higher order methods which lead to a smooth boundary, but I don&#39;t cover those here. . We also need to perform the inverse operation: given the electric field at each grid point, interpolate the value at each particle position. The same method applies here. NGP just uses the electric field at the nearest grid point, while CIC weights the four nearest grid points. The following Grid class implements the CIC method. Notice that Cython is used in the for-loop in the distribute method. I couldn&#39;t figure out a way to perform this operation with the loop, and in pure Python it took about 90% of the runtime for a single simulation step. Using Cython gave a significant performance boost. . %%cython import numpy as np from scipy.interpolate import RegularGridInterpolator class Grid: &quot;&quot;&quot;Class for 2D grid. Attributes - xmin, ymin, xmax, ymax : float Minimum and maximum coordinates. Nx, Ny : int Number of grid points. dx, dy : int Spacing between grid points. x, y : ndarray, shape (Nx,) or (Ny,) Positions of each grid point. cell_area : float Area of each cell. &quot;&quot;&quot; def __init__(self, xlim=(-1, 1), ylim=(-1, 1), size=(64, 64)): self.xlim, self.ylim = xlim, ylim (self.xmin, self.xmax), (self.ymin, self.ymax) = xlim, ylim self.size = size self.Nx, self.Ny = size self.dx = (self.xmax - self.xmin) / (self.Nx - 1) self.dy = (self.ymax - self.ymin) / (self.Ny - 1) self.cell_area = self.dx * self.dy self.x = np.linspace(self.xmin, self.xmax, self.Nx) self.y = np.linspace(self.ymin, self.ymax, self.Ny) def set_lims(self, xlim, ylim): &quot;&quot;&quot;Set the min and max grid coordinates.&quot;&quot;&quot; self.__init__(xlim, ylim, self.size) def zeros(self): &quot;&quot;&quot;Create array of zeros with same size as the grid.&quot;&quot;&quot; return np.zeros((self.size)) def distribute(self, positions): &quot;&quot;&quot;Distribute points on the grid using the cloud-in-cell (CIC) method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. Returns - rho : ndarray, shape (Nx, Ny) The value rho[i, j] gives the number of macroparticles in the i,j cell. &quot;&quot;&quot; # Compute area overlapping with 4 nearest neighbors (A1, A2, A3, A4) ivals = np.floor((positions[:, 0] - self.xmin) / self.dx).astype(int) jvals = np.floor((positions[:, 1] - self.ymin) / self.dy).astype(int) ivals[ivals &gt; self.Nx - 2] = self.Nx - 2 jvals[jvals &gt; self.Ny - 2] = self.Ny - 2 x_i, x_ip1 = self.x[ivals], self.x[ivals + 1] y_j, y_jp1 = self.y[jvals], self.y[jvals + 1] _A1 = (positions[:, 0] - x_i) * (positions[:, 1] - y_j) _A2 = (x_ip1 - positions[:, 0]) * (positions[:, 1] - y_j) _A3 = (positions[:, 0] - x_i) * (y_jp1 - positions[:, 1]) _A4 = (x_ip1 - positions[:, 0]) * (y_jp1 - positions[:, 1]) # Distribute fractional areas rho = self.zeros() cdef double[:, :] rho_view = rho cdef int i, j for i, j, A1, A2, A3, A4 in zip(ivals, jvals, _A1, _A2, _A3, _A4): rho_view[i, j] += A4 rho_view[i + 1, j] += A3 rho_view[i, j + 1] += A2 rho_view[i + 1, j + 1] += A1 return rho / self.cell_area def interpolate(self, grid_vals, positions): &quot;&quot;&quot;Interpolate values from the grid using the CIC method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. grid_vals : ndarray, shape (n, 2) Scalar value at each coordinate point. Returns - int_vals : ndarray, shape (nparts,) Interpolated value at each position. &quot;&quot;&quot; int_func = RegularGridInterpolator((self.x, self.y), grid_vals) return int_func(positions) def gradient(self, grid_vals): &quot;&quot;&quot;Compute gradient using 2nd order centered differencing. Parameters - grid_vals : ndarray, shape (Nx, Ny) Scalar values at each grid point. Returns - gradx, grady : ndarray, shape (Nx, Ny) The x and y gradient at each grid point. &quot;&quot;&quot; return np.gradient(grid_vals, self.dx, self.dy) . It should also be mentioned that the field interpolation method should be the same as the charge deposition method; if this is not true, it is possible for a particle to exert a force on itself! Let&#39;s test the method on a Gaussian distribution of 100,000 macroparticles in the $x$-$y$ plane, truncated at three standard devations. We&#39;ll choose the number of grid points to be $N_x = N_y = 64$. . # Create coordinate array nparts = 100000 cut = 3.0 X = truncnorm.rvs(a=4*[-cut], b=4*[cut], size=(nparts, 4)) X *= 10e-6 # Create bunch bunch = Bunch() bunch.fill(X) bunch.compute_extremum() # Distribute bunch particles on grid grid = Grid(bunch.xlim, bunch.ylim, size=(64, 64)) rho = grid.distribute(bunch.positions) . Field solver . The workhorse in the simulation loop is the field solver. We need to solve Poisson&#39;s equation: . $$ left({ frac{ partial^2}{ partial x^2} + frac{ partial^2}{ partial y^2}} right) = - frac{ rho left(x, y right)}{ varepsilon_0}. tag{6}$$ . The discretized version of the equation reads . $$ frac{ phi_{i+1,j} -2 phi_{i,j} + phi_{i-1,j}}{{ Delta_x}^2} + frac{ phi_{i,j+1} -2 phi_{i,j} + phi_{i,j-1}}{{ Delta_y}^2} = - frac{ rho_{i,j}}{ varepsilon_0} tag{7}$$ . for a grid with spacing $ Delta_x$ and $ Delta_y$. There are multiple paths to a solution; we will focus on the method implemented in PyORBIT which utilizes the Fourier convolution theorem. Let&#39;s briefly go over this method. The potential from an infinite line of elementary charges at the origin with number density $ lambda$ is . $$ phi( mathbf{x}) = - frac{ lambda e}{2 pi varepsilon_0} ln{| mathbf{x}|} = - frac{ lambda e}{2 pi varepsilon_0} int{ ln{| mathbf{x} - mathbf{y}|} delta( mathbf{y})d mathbf{y}}. tag{8}$$ . Note that $ mathbf{y}$ is just a dummy variable. By letting $G( mathbf{x} - mathbf{y}) = - ln{| mathbf{x} - mathbf{y}|}$ and $ rho( mathbf{x}) = delta( mathbf{x})$, then up to a scaling factor we have . $$ phi( mathbf{x}) = int{G( mathbf{x} - mathbf{y}) rho( mathbf{y})d mathbf{y}} = G( mathbf{x}) * rho( mathbf{x}). tag{9}$$ . In this form the potential is a convolution (represented by $*$) of the charge density $ rho$ with $G$, which is called the Green&#39;s function. On the grid this will look like . $$ phi_{i, j} = sum_{k,l ne i,j}{G_{i-k, j-l} rho_{k, l}}. tag{11}$$ . This solves the problem in $O(N^2)$ time complexity for $N$ grid points. This is already much faster than a direct force calculation but could still get expensive for fine grids. We can speed things up by exploiting the convolution theorem, which says that the Fourier transform of a convolution of two functions is equal to the product of their Fourier transforms. The Fourier transform is defined by . $$ hat{ phi}( mathbf{k})= mathcal{F} left[ phi( mathbf{x}) right] = int_{- infty}^{ infty}{e^{-i mathbf{k} cdot mathbf{x}} phi( mathbf{x}) d mathbf{x}}. tag{12}$$ . The convolution theorem then says $$ mathcal{F} left[ rho * G right] = mathcal{F} left[ rho right] cdot mathcal{F} left[G right]. tag{13}$$ . For the discrete equation this gives . $$ hat{ phi}_{n, m} = hat{ rho}_{n, m} hat{G}_{n, m}, tag{14}$$ . where the hat represents the discrete Fourier transform. The time complexity can be reduced to $O left(N log N right)$ with the FFT algorithm at our disposal. . There is a caveat to this method: Eq. (11) must be a circular convolution in order to use the FFT algorithm, which means $G$ must be periodic. But the beam is in free space (we&#39;ve neglected any conducting boundary), so this is not true. We can make it true by doubling the grid size in each dimension. We then make $G$ a mirror reflection in the new quadrants so that it is periodic, and also set the charge density equal to zero in these regions. After running the method on this larger grid, the potential in the new quadrants will be unphysical; however, the potential in the original quadrant will be correct. There are also some tricks we can play to reduce the space complexity, and in the end doubling the grid size is not much of a price to pay for the gain in speed. The method is implemented in the PoissonSolver class. . class PoissonSolver: &quot;&quot;&quot;Class to solve Poisson&#39;s equation on a 2D grid. Attributes - rho, phi, G : ndarray, shape (2*Nx, 2*Ny) The density (rho), potential (phi), and Green&#39;s function (G) at each grid point on a doubled grid. Only one quadrant (i &lt; Nx, j &lt; Ny) corresponds to to the real potential. &quot;&quot;&quot; def __init__(self, grid, sign=-1.): self.grid = grid new_shape = (2 * self.grid.Nx, 2 * self.grid.Ny) self.rho, self.G = np.zeros(new_shape), np.zeros(new_shape) self.phi = np.zeros(new_shape) def set_grid(self, grid): self.__init__(grid) def compute_greens_function(self): &quot;&quot;&quot;Compute Green&#39;s function on doubled grid.&quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny Y, X = np.meshgrid(self.grid.x - self.grid.xmin, self.grid.y - self.grid.ymin) self.G[:Nx, :Ny] = -0.5 * np.log(X**2 + Y**2, out=np.zeros_like(X), where=(X + Y &gt; 0)) self.G[Nx:, :] = np.flip(self.G[:Nx, :], axis=0) self.G[:, Ny:] = np.flip(self.G[:, :Ny], axis=1) def get_potential(self, rho): &quot;&quot;&quot;Compute the scaled electric potential on the grid. Parameters - rho : ndarray, shape (Nx, Ny) Number of macroparticles at each grid point. Returns - phi : ndarray, shape (Nx, Ny) Scaled electric potential at each grid point. &quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny self.rho[:Nx, :Ny] = rho self.compute_greens_function() self.phi = ifft2(fft2(self.G) * fft2(self.rho)).real return self.phi[:Nx, :Ny] . Running the algorithm gives the following potential on the doubled grid: . solver = PoissonSolver(grid) phi = solver.get_potential(rho) . We can then approximate the gradient of the potential using second-order centered differencing. This gives . $$( nabla phi)_{i,j} = frac{ phi_{i+1,j} - phi_{i-1,j}}{2 Delta_x} hat{x} + frac{ phi_{i,j+1} - phi_{i,j-1}}{2 Delta_y} hat{y}. tag{15}$$ . Ex, Ey = grid.gradient(-phi) . Finally, the value of the electric field at each particle position can be interpolated from the grid. . Ex_int = grid.interpolate(Ex, bunch.positions) Ey_int = grid.interpolate(Ey, bunch.positions) . Particle mover . All we need to do in this step is integrate the equations of motion. A common method is leapfrog integration in which the position and velocity are integrated out of phase as follows: . $$ m left( frac{ mathbf{v}_{i+1/2} - mathbf{v}_{i-1/2}}{ Delta_t} right) = mathbf{F}( mathbf{x}_i), tag{16}$$ . $$ frac{ mathbf{x}_{i+1} - mathbf{x}_i}{ Delta_t} = mathbf{v}_{i+1/2} tag{17}$$ . . Credit: S. LundA different scheme must be used when velocity-dependent forces are present. This is a symplectic integrator, which means it conserves energy. It is also second-order accurate, meaning that its error is proportional to the square of the $ Delta_t$. Finally, it is time-reversible. The only complication is that, because the velocity and position are out of phase, we need to push the velocity back one half-step before starting the simulation, and push it one half-step forward when taking a measurement. . Putting it all together . Simulation loop . We have all the tools to implement the simulation loop. While $s &lt; s_{max}$ we: . Compute the charge density on the grid. | Find the electric potential on the grid. | Interpolate the electric field at the particle positions. | Update the particle positions. | We&#39;ll first create a History class which stores the beam moments or phase space coordinates. . class History: &quot;&quot;&quot;Class to store bunch data over time. Atributes moments : list Second-order bunch moments. Each element is ndarray of shape (10,). coords : list Bunch coordinate arrays. Each element is ndarray of shape (nparts, 4) moment_positions, coord_positions : list Positions corresponding to each element of `moments` or `coords`. &quot;&quot;&quot; def __init__(self, bunch, samples=&#39;all&#39;): self.X = bunch.X self.moments, self.coords = [], [] self.moment_positions, self.coord_positions = [], [] if samples == &#39;all&#39; or samples &gt;= bunch.nparts: self.idx = np.arange(bunch.nparts) else: self.idx = np.random.choice(bunch.nparts, samples, replace=False) def store_moments(self, s): Sigma = np.cov(self.X.T) self.moments.append(Sigma[np.triu_indices(4)]) self.moment_positions.append(s) def store_coords(self, s): self.coords.append(np.copy(self.X[self.idx, :])) self.coord_positions.append(s) def package(self): self.moments = np.array(self.moments) self.coords = np.array(self.coords) . Now we&#39;ll create a Simulation class. . class Simulation: &quot;&quot;&quot;Class to simulate the evolution of a charged particle bunch in free space. Attributes - bunch : Bunch: The bunch to track. distance : float Total tracking distance [m]. step_size : float Distance between force calculations [m]. nsteps : float Total number of steps = int(length / ds). steps_performed : int Number of steps performed so far. s : float Current bunch position. history : History object Object storing historic bunch data. meas_every : dict Dictionary with keys: &#39;moments&#39; and &#39;coords&#39;. Values correspond to the number of simulations steps between storing these quantities. For example, `meas_every = {&#39;coords&#39;:4, &#39;moments&#39;:2}` will store the moments every 4 steps and the moments every other step. Defaults to storing only the initial and final positions. samples : int Number of bunch particles to store when measuring phase space coordinates. Defaults to the entire coordinate array. &quot;&quot;&quot; def __init__(self, bunch, distance, step_size, grid_size, meas_every={}, samples=&#39;all&#39;): self.bunch = bunch self.distance, self.step_size = distance, step_size self.nsteps = int(distance / step_size) self.grid = Grid(size=grid_size) self.solver = PoissonSolver(self.grid) self.fields = np.zeros((bunch.nparts, 2)) self.history = History(bunch, samples) self.s, self.steps_performed = 0.0, 0 self.meas_every = meas_every self.meas_every.setdefault(&#39;moments&#39;, self.nsteps) self.meas_every.setdefault(&#39;coords&#39;, self.nsteps) self.sc_factor = bunch.perveance / bunch.nparts def set_grid(self): &quot;&quot;&quot;Set grid limits from bunch size.&quot;&quot;&quot; self.bunch.compute_extremum() self.grid.set_lims(self.bunch.xlim, self.bunch.ylim) self.solver.set_grid(self.grid) def compute_electric_field(self): &quot;&quot;&quot;Compute self-generated electric field.&quot;&quot;&quot; self.set_grid() rho = self.grid.distribute(self.bunch.positions) phi = self.solver.get_potential(rho) Ex, Ey = self.grid.gradient(-phi) self.fields[:, 0] = self.grid.interpolate(Ex, self.bunch.positions) self.fields[:, 1] = self.grid.interpolate(Ey, self.bunch.positions) def kick(self, step_size): &quot;&quot;&quot;Update particle slopes.&quot;&quot;&quot; self.bunch.X[:, 1] += self.sc_factor * self.fields[:, 0] * step_size self.bunch.X[:, 3] += self.sc_factor * self.fields[:, 1] * step_size def push(self, step_size): &quot;&quot;&quot;Update particle positions.&quot;&quot;&quot; self.bunch.X[:, 0] += self.bunch.X[:, 1] * step_size self.bunch.X[:, 2] += self.bunch.X[:, 3] * step_size def store(self): &quot;&quot;&quot;Store bunch data.&quot;&quot;&quot; store_moments = self.steps_performed % self.meas_every[&#39;moments&#39;] == 0 store_coords = self.steps_performed % self.meas_every[&#39;coords&#39;] == 0 if not (store_moments or store_coords): return Xp = np.copy(self.bunch.X[:, [1, 3]]) self.kick(+0.5 * self.step_size) # sync positions/slopes if store_moments: self.history.store_moments(self.s) if store_coords: self.history.store_coords(self.s) self.bunch.X[:, [1, 3]] = Xp def run(self, meas_every={}): &quot;&quot;&quot;Run the simulation.&quot;&quot;&quot; self.store() self.compute_electric_field() self.kick(-0.5 * self.step_size) # desync positions/slopes for i in trange(self.nsteps): self.compute_electric_field() self.kick(self.step_size) self.push(self.step_size) self.s += self.step_size self.steps_performed += 1 self.store() self.history.package() . Demonstration . We need some way of checking our method&#39;s accuracy. Luckily there is an analytic benchmark available: the Kapchinskij-Vladimirskij (KV) distribution. Without going into any detail, the beam projects to a uniform density ellipse in the $x$-$y$ plane, and the space charge forces produced within this ellipse are linear (in general space charge forces are nonlinear). If we plug the KV distribution into the Vlasov equation, it can be seen that these forces will remain linear for all time if the external focusing forces are also linear. As a consequence, a set of self-consistent differential equations describing the evolution of the ellipse boundary can be written down. If we consider the beam to be an upright ellipse with semi-axis $a$ along the $x$ axis and $b$ along the $y$ axis, then without external fields the equations read: . $$ a&#39;&#39; = frac{2Q}{a + b} + frac{ varepsilon_x}{a^3}, $$ $$ b&#39;&#39; = frac{2Q}{a + b} + frac{ varepsilon_y}{b^3}. tag{18}$$ . These are known as the KV envelope equations or simply envelope equations. $Q$, called the perveance, is a dimensionless number which is proportional to the beam intensity but reduced by the beam energy. We can think of this constant as a measure of the space charge strength. The $ varepsilon_x$ and $ varepsilon_y$ terms are called the emittances and determine the area occupied by the beam in $x$-$x&#39;$ and $y$-$y&#39;$ phase space. For example, a beam with all particles sitting perfectly still in the $x$-$y$ plane has no emittance, but a beam which is instead spreading out has a nonzero emittance. These emittances will also be conserved for the KV distribution. The following function integrates the envelope equations. . def track_env(X, positions, perveance=0.0): &quot;&quot;&quot;Track beam moments (assuming KV distribution) through free space. Parameters - X : ndarray, shape (nparts, 4) Transverse bunch coordinate array. positions : list List of positions at which to evaluate the equations. perveance : float The dimensionless space charge perveance. Returns - ndarray, shape (len(positions), 4) Each row gives [a, a&#39;, b, b&#39;], where a and b are the beam radii in the x and y dimension, respectively. &quot;&quot;&quot; Sigma = np.cov(X.T) a, b = np.sqrt(Sigma[0, 0]), np.sqrt(Sigma[2, 2]) ap, bp = Sigma[0, 1] / a, Sigma[2, 3] / b epsx = np.sqrt(np.linalg.det(Sigma[:2, :2])) epsy = np.sqrt(np.linalg.det(Sigma[2:, 2:])) def derivs(env, s): a, ap, b, bp = env envp = np.zeros(4) envp[0], envp[2] = ap, bp envp[1] = 0.5 * perveance/(a + b) + epsx**2 / a**3 envp[3] = 0.5 * perveance/(a + b) + epsy**2 / b**3 return envp return odeint(derivs, [a, ap, b, bp], positions, atol=1e-14) . Some care must be taken in the choice of simulation parameters; we need a fine enough grid to resolve the hard edge of the beam and enough macroparticles per grid cell to collect good statistics. I chose what I thought was reasonable: 128,000 macroparticles, a step size of 2.5 cm, and a $128 times 128$ grid. . nparts = 128000 bunch_length = 250.0 # [m] intensities = [0.0, 10e14, 20e14, 40e14] # Simulation parameters distance = 10.0 # [m] step_size = 0.025 # [m] grid_size = (128, 128) samples = 10000 meas_every = {&#39;moments&#39;: int(0.1 * distance/step_size), &#39;coords&#39;: 4} . Below we create and track four identical KV distributions, each with a different intensity. . # Create KV bunch in normalized coordinates (surface of 4D unit sphere) X = np.random.normal(size=(nparts, 4)) # 4D Gaussian X = np.apply_along_axis(lambda row: row/np.linalg.norm(row), 1, X) # normalize rows # Scale by emittance eps_x, eps_y = 10e-6, 10e-6 A = 2 * np.sqrt(np.diag([eps_x, eps_x, eps_y, eps_y])) X = np.apply_along_axis(lambda row: np.matmul(A, row), 1, X) # Scale beam size and divergence relative to emittance alpha_x, alpha_y = 0.0, 0.0 beta_x, beta_y = 20.0, 20.0 V = np.zeros((4, 4)) V[:2, :2] = np.sqrt(1/beta_x)* np.array([[beta_x, 0], [alpha_x, 1]]) V[2:, 2:] = np.sqrt(1/beta_y)* np.array([[beta_y, 0], [alpha_y, 1]]) X = np.apply_along_axis(lambda row: np.matmul(V, row), 1, X) # Create and track bunches sims = [] for intensity in intensities: bunch = Bunch(intensity, bunch_length) bunch.fill(np.copy(X)) sim = Simulation(bunch, distance, step_size, grid_size, meas_every=meas_every, samples=samples) sim.run() sims.append(sim) . 100%|██████████| 400/400 [00:56&lt;00:00, 7.02it/s] 100%|██████████| 400/400 [00:55&lt;00:00, 7.26it/s] 100%|██████████| 400/400 [00:56&lt;00:00, 7.02it/s] 100%|██████████| 400/400 [00:53&lt;00:00, 7.45it/s] . This plot shows the horizontal and vertical beam size over time for each of the four chosen beam intensities. The solid lines are the result of integrating the envelope equations, while the black dots are the result of the PIC calculation. Notice that the beam expands on its own due to the nonzero emittance and that the effect of space charge is to increase the expansion rate. It seems to be quite accurate over this distance, and the runtime is acceptable for my purposes. Here is the evolution of a sample of 10,000 of the macroparicles as well as an ellipse showing the KV envelope. . &lt;/input&gt; Once Loop Reflect Conclusion . This post implemented an electrostatic PIC solver in Python. I learned quite a bit from doing this and was happy to see my calculations agree with the theoretical benchmark. One extension of this code would be to consider the velocity-dependent force from magnetic fields. It would also be straightforward to extend the code to 3D. Finally, all the methods used here are applicable to gravitational simulations. Here are some helpful references: . USPAS course | Hockney &amp; Eastwood | Birdsall &amp; Langdon | .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/simulation/space%20charge/2021/02/22/PIC.html",
            "relUrl": "/physics/accelerators/simulation/space%20charge/2021/02/22/PIC.html",
            "date": " • Feb 22, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Coupled parametric oscillators",
            "content": "Introduction . A previous post examined the analytic solutions to the equation of motion describing a parametric oscillator, which is a harmonic oscillator whose physical properties are time-dependent (but not dependent on the state of the oscillator). This problem was motivated by describing the transverse oscillations of a particle in an accelerator. In this post, the treatment will be extended to a coupled parametric oscillator. Basically, we are trying to solve the following equation of motion: . $$x&#39;&#39; + k_{11}(s)x + k_{13}(s)y + k_{14}(s)y&#39;,$$ $$y&#39;&#39; + k_{33}(s)y + k_{31}(s)x + k_{32}(s)x&#39;,$$ . where the prime denotes differentiation with respect to $s$. We also assume that each of the $k_{ij}$ coefficients are periodic, so $k_{ij}(s + L) = k_{ij}(s)$ for some $L$. . Motivation . The previous post discussed dipole and quadrupole magnetic fields, which have the special property that their fields depend linearly on $x$ and $y$, and are also uncoupled. Of course there are many other configurations possible. First, consider a solenoid magnet: . . Credit: brilliant.org The field within the coils points in the longitudinal direction and is approximatly constant ($ mathbf{B}_{sol} = B_0 hat{s}$). Plugging this into the Lorentz force equation we find: . $$ dot{ mathbf{v}} = frac{q}{m} mathbf{v} times mathbf{B} = frac{qB_0}{m} left({v_y hat{x} - v_x hat{y}} right).$$ . This means the motion in $x$ depends on the velocity in $y$, and vice versa, so this will contribute to $k_{14}$ and $k_{32}$. Coupling can also be produced from transverse magnetic fields. We again write the multipole expansion of this field: . $$B_y + iB_x = B_{ref} sum_{n=1}^{ infty} left({B_n + iA_n} right) left( frac{x + iy}{R_{ref}} right)^{n-1}.$$ . . Credit: Jeff Holmes . There will be nonlinear coupling (terms proportional to $x^j y^k$, where $j,k &gt; 1$ when $n &gt; 2$, but we are interested in linear coupling. This occurs when the skew quadrupole term $A_2$ is nonzero, which is true anytime a quadrupole is tilted in the transverse plane. The field couples the motion in one plane to the displacement in the other, contributing to the $k_{13}$ and $k_{31}$ terms. . Approach . Let&#39;s review the approach we took in analyzing the 1D parametric oscillator. We wrote the solution in pseudo-harmonic form, with an amplitude and phase which depended on time. We then found that particles travel along the boundary of an ellipse in 2D phase space, the area of which is a constant of the motion (we will denote this area by $ epsilon_x$). To understand the motion, we just need to know the dimensions and orientation of this ellipse, for which we proposed the parameters $ alpha_x$ and $ beta_x$, as well as the location of the particle on the ellipse boundary, which is determined by the phase $ mu_x$. All the subscripts can be replaced by $y$ to handle the vertical motion. We also wrote a transfer matrix $ mathbf{M}$, which connects the initial and final phase space coordinates after tracking through one period, from the parameters in the following form: . $$ mathbf{M} = mathbf{V} mathbf{P} mathbf{V}^{-1},$$ . where $ mathbf{V}^{-1}$ is a function of $ alpha_x$ and $ beta_x$ and transforms the ellipse into a circle while preserving the area, and $ mathbf{P}$ is a rotation in phase space according to the phase advance $ mu_x$. Basically, $ mathbf{V}$ turns the parametric oscillator into a harmonic oscillator. . This is a very elegant way to describe the motion with a minimal set of parameters. The question is: can we do something similar for coupled motion, in which the phase space is 4D, not 2D? To start, let&#39;s track a particle in a lattice with a nonzero skew quadrupole coefficient, plotting its phase space coordinates at one position after every period. . &lt;/input&gt; Once Loop Reflect The particle traces interesting donut-like shapes in horizontal ($x$-$x&#39;$) and vertical ($y$-$y&#39;$) phase space instead of ellipses. Below shows the shapes after 1000 periods. . There is definitely more than one frequency present, which we see if we plot the $x$ and $y$ position vs period number and take the FFT. . This is typical of a coupled oscillator. Such systems are typically understood as the superposition of normal modes, each of which corresponds to a single frequency. For example, consider two masses connected with a spring. There are two possible ways for the masses to oscillate at the same frequency. The first is a breathing mode in which they move in opposite directions, and the second is a sloshing mode in which they move in the same direction. The motion is simply the sum of these two modes. We will try to do something similar for a coupled parameteric oscillator. . Solution . Transfer matrix eigenvectors . If the phase space coordinate vector $ mathbf{x} = (x, x&#39;, y, y&#39;)^T$ evolves according to . $$ mathbf{x} rightarrow mathbf{Mx},$$ . where $ rightarrow$ represents tracking through one period, it can be shown that $ mathbf{M}$ is symplectic due to the Hamiltonian mechanics of the system. A consequence of the symplecticity condition is that $ mathbf{M}$ is fully described by 10 numbers instead of 16. Our method examines the eigenvectors of $ mathbf{M}$: . $$ mathbf{Mv} = e^{-i mu} mathbf{v}.$$ . The symplecticity condition also causes the eigenvalues and eigenvectors come in two complex conjugate pairs; this gives $ mathbf{v}_1$, $ mathbf{v}_2$, $ mu_1$, $ mu_2$ and their complex conjugates. The seemingly complex motion seen in the last animation is greatly simplified when written in terms of the eigenvectors. We can write any cooridinate vector as a linear combination of the real and imaginary components of $ mathbf{v}_1$ and $ mathbf{v}_2$: . $$ mathbf{x} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i psi_1} + sqrt{ epsilon_2} mathbf{v}_2e^{-i psi_2} right).$$ . We&#39;ve introduced two initial amplitudes ($ epsilon_1$ and $ epsilon_2$) as well as two initial phases ($ psi_1$ and $ psi_2$). Applying the transfer matrix then simply tacks on a phase. Thus, what we are observing are the 2D projections of the real components of these eigenvectors as they rotate in the complex plane. . $$ mathbf{Mx} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i left( psi_1 + mu_1 right)} + sqrt{ epsilon_2} mathbf{v}_2e^{-i( psi_2 + mu_2)} right).$$ . Let&#39;s replay the animation, but this time draw a red arrow for $ mathbf{v}_1$ and a blue arrow for $ mathbf{v}_2$. We&#39;ve chosen $ epsilon_1 = 4 epsilon_2$ and $ psi_2 - psi_1 = pi/2$. . &lt;/input&gt; Once Loop Reflect That really simplifies things! Each eigenvector simply rotates at its frequency $ mu_l$. It also explains why the amplitude in the $x$-$x&#39;$ and $y$-$y&#39;$ planes trade back and forth: it is because the projections of the eigenvectors rotate at different frequencies, sometimes aligning and sometimes anti-aligning. Because of this, the previous invariants $ epsilon_x$ and $ epsilon_y$ are replaced by $ epsilon_1$ and $ epsilon_2$ as the invariants. It is helpful to think of a torus (shown below). The two amplitudes would determine the inner and outer radii of the torus, and the two phases determine the location of a particle on the surface. . . Credit: Wikipedia Parameterization of eigenvectors . We are now going to introduce a set of parameters for these eigenvectors, and in turn the transfer matrix. We already have two phases, so that leaves 8 parameters. Our strategy is to observe that each eigenvector traces an ellipse in both horizontal ($x$-$x&#39;$) and vertical ($y$-$y&#39;$) phase space. Then, we will simply assign an $ alpha$ function and $ beta$ function to each of these ellipses. So, for the ellipse traced by $ mathbf{v}_1$ in the $x$-$x&#39;$ plane, we have $ beta_{1x}$ and $ alpha_{1x}$, and then for the second eigenvector we have $ beta_{2x}$ and $ alpha_{2x}$. The same thing goes for the vertical dimension with $x$ replaced by $y$. . . The actual eigenvectors written in terms of the parameters are . $$ vec{v}_1 = begin{bmatrix} sqrt{ beta_{1x}} - frac{ alpha_{1x} + i(1-u)}{ sqrt{ beta_{1x}}} sqrt{ beta_{1y}}e^{i nu_1} - frac{ alpha_{1y} + iu}{ sqrt{ beta_{1y}}} e^{i nu_1} end{bmatrix}, quad vec{v}_2 = begin{bmatrix} sqrt{ beta_{2x}}e^{i nu_2} - frac{ alpha_{2x} + iu}{ sqrt{ beta_{2x}}}e^{i nu_2} sqrt{ beta_{2y}} - frac{ alpha_{2y} + i(1-u)}{ sqrt{ beta_{2y}}} end{bmatrix}$$ . So in addition to the phases $ mu_1$ and $ mu_2$ we have $ alpha_{1x}$, $ alpha_{2x}$, $ alpha_{1y}$, $ alpha_{2y}$, $ beta_{1x}$, $ beta_{2x}$, $ beta_{1y}$, and $ beta_{2y}$. That&#39;s pretty much it. There are a few other parameters we need to introduce to simplify the notation, but they are not independent. The first is $u$, which, as noted in the figure, determines the areas of the ellipses in one plane relative to the other. The second and third are $ nu_1$ and $ nu_2$, which are phase differences between the $x$ and $y$ components of the eigenvectors (in the animation they are either $0$ or $ pi$). I won&#39;t discuss these here. The last thing to note is that the parameters reduce to their 1D definitions when there is no coupling in the lattice. So we would have $ beta_{1x}, beta_{2y} rightarrow beta_{x}, beta_{y}$ and $ beta_{2x}, beta_{1y} rightarrow 0$, and similar for $ alpha$. The invariants and phase advances would also revert back to their original values: $ epsilon_{1,2} rightarrow epsilon_{x,y}$ and $ mu_{1,2} rightarrow mu_{x,y}$. . Floquet transformation . These eigenvectors can also be used to construct a transformation which removes both the variance in the focusing strength and the coupling between the planes, turning the coupled parametric oscillator into an uncoupled harmonic oscillator. In other words, we seek a matrix $ mathbf{V}$ such that . $$ mathbf{V^{-1} M V} = mathbf{P} = begin{bmatrix} cos{ mu_1} &amp; sin{ mu_1} &amp; 0 &amp; 0 - sin{ mu_1} &amp; cos{ mu_1} &amp; 0 &amp; 0 0 &amp; 0 &amp; cos{ mu_2} &amp; sin{ mu_2} 0 &amp; 0 &amp; - sin{ mu_2} &amp; cos{ mu_2} end{bmatrix} $$We can do this simply by rewriting the following equation (I haven&#39;t yet figured out how to number equations in Jupyter): . $$ mathbf{x} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i psi_1} + sqrt{ epsilon_2} mathbf{v}_2e^{-i psi_2} right)$$ . in matrix form as $ mathbf{x} = mathbf{V} mathbf{x}_n$ with . $$ mathbf{x}_n = begin{bmatrix} sqrt{ epsilon_1} cos{ psi_1} - sqrt{ epsilon_1} sin{ psi_1} sqrt{ epsilon_2} cos{ psi_2} - sqrt{ epsilon_2} sin{ psi_2} end{bmatrix} $$ $$ mathbf{V} = left[{Re( mathbf{v}_1), -Im( mathbf{v}_1), Re( mathbf{v}_2), -Im( mathbf{v}_2)} right]$$ . Let&#39;s observe the motion in these new coordinates $ mathbf{x}_n$. . &lt;/input&gt; Once Loop Reflect The motion is uncoupled after this transformation; i.e., particles move in a circle of area $ varepsilon_1$ in the $x_n$-$x_n&#39;$ plane at frequency $ mu_1$, and in a circle of area $ varepsilon_2$ in the $y_n$-$y_n&#39;$ plane at frequency $ mu_2$. . Conclusion . The method introduced here allows us to describe the evolution of a parametric oscillator using the minimum number of parameters. Our physical motivation was an accelerator lattice with linear, coupled forces, such as when skew quadrupole terms are present in the magnetic fields. There is no agreed upon method to do this among accelerator physicists, but I like (and know) this method the best, and have used it in my research. I&#39;ve left out many details which can be found in the paper by Lebedev and Bogacz. The paper by Ripken is also very helpful. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/coupling/2021/01/25/coupled_parametric_oscillators.html",
            "relUrl": "/physics/accelerators/coupling/2021/01/25/coupled_parametric_oscillators.html",
            "date": " • Jan 25, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Parametric oscillators",
            "content": "This post presents the solution to a general problem: what is the motion of a particle in one dimension (1D) in the presence of time-dependent, linear, periodic forces? This amounts to solving the following equation of motion: . $$ frac{d^2x}{dt^2} + k(t)x = 0,$$ . where $k(t + T) = k(t)$ for some $T$. This is a parametric oscillator, a harmonic oscillator whose physical properties are not static. For example, the oscillations of a pendulum (in the small angle approximation) on the surface of a planet whose gravitational pull varies periodically would be described by the above equation. The solution to this equation was derived by George William Hill in 1886 to study lunar motion, and for this reason it is known as Hill&#39;s equation. It also finds application in areas such as condensed matter physics, quantum optics, and accelerator physics. After setting up the physical problem, we will examine the solutions and discuss their relevance to the last application, accelerator physics. . Problem motivation . Accelerator physics . Particle accelerators are machines which produce groups of charged particles (known as beams), increase their kinetic energy, and guide them to a target. These machines are invaluable to modern scientific research. The most famous examples are colliders, such as the LHC, in which two beams are smashed together to generate fundamental particles. A lesser known fact is that the fields of condensed matter physics, material science, chemistry, and biology also benefit tremendously from accelerators; this is due to the effectiveness of scattering experiments in which the deflection of a beam after colliding with a target is used to learn information about the target. The scattered beam is composed of neutrons in spallation neutron sources such as SNS, electrons in electron scattering facilities such as CEBAF, or photons in synchrotron light sources such as APS. In addition to scientific research, accelerators find use in medicine, particularly for cancer treatment, and also in various industrial applications. . . A large detector at an interaction point in the LHC. There are generally a few beam properties which are very important to experimentalists; in colliders it is the energy and luminosity, in spallation sources it is the intensity, and in light sources it is the brightness. There is thus a constant need to push these parameters to new regions. For example, below is the famous Livingston plot which shows the energy achieved by various machines over the past century. . . Note: vertical axis scale is beam energy needed to produce the center of mass energy by collision with a resting proton (credit: Rasmus Ischebeck). There are many physics issues associated with the optimization of these beam parameters. Accelerator physics is a field of applied physics which studies these issues. The task of the accelerator physicist is to understand, control, and measure the journey of the beam from its creation to its final destination. The difficulty of this task has grown over time; the improvement accelerator performance has brought with it a staggering increase in size and complexity. The construction and operation of modern accelerators generally requires years of planning, thousands of scientists and engineers, and hundreds of millions or even billions of dollars. Despite this complexity, the underlying physics principles are quite simple, and the single particle motion in one of these machines can be understood analytically if a few approximations are made. In the end we will arrive at Hill&#39;s equation. . How to build an accelerator . There are three basic tasks an accelerator has to accomplish. First, it must increase the beam energy (acceleration). Second, it must guide the beam along a predetermined path (steering). Third, it must ensure the beam particles remain close together (focusing). It is helpful to use a coordinate system in which the $s$ axis points along the design trajectory, and the $x$ and $y$ axes defined in the plane transverse to $s$. In this way the motion is broken up into transverse and longitudinal dynamics. . . How are these tasks accomplished? Well, particles are charged, and the force on a point charge in an electromagnetic field is given by . $$ mathbf{F} = q left({ mathbf{E} + mathbf{v} times mathbf{B}} right),$$ . where $q$ is the particle charge, $ mathbf{v}$ is the particle velocity, $ mathbf{E}$ is the electric field, and $ mathbf{B}$ is the magnetic field. An accelerator consists of a series of elements, each with their own $ mathbf{E}$ and $ mathbf{B}$; the collection of these elements is called a lattice. We need to determine which electric and magnetic fields to use. . The first task, acceleration, is not the focus of this post. The remaining tasks, steering and focusing, concern the motion in the transverse plane. $ mathbf{B}$ fields, not $ mathbf{E}$ fields, are used since their effect grows with increased particle velocity. Any transverse magnetic field $ mathbf{B} = (B_x, B_y)^T$ can be written using a multipole expansion . $$B_y + iB_x = B_{ref} sum_{n=1}^{ infty} left({B_n + iA_n} right) left( frac{x + iy}{R_{ref}} right)^{n-1}.$$ . $B_{ref}$ and $R_{ref}$ are a reference field strength and radius, respectively; just consider them to be constants. We then have the normal multiple coefficients $B_n$, and the skew multipole coefficients $A_n$. The field lines corresponding to the first few normal multipole coefficients are shown below. . . Credit: Jeff Holmes The dipole term is perfect for steering. The field is constant in magnitude and direction: . $$ mathbf{B}_{dipole} propto hat{y},$$ . producing a force which is proportional to the $x$ position: . $$ mathbf{F}_{dipole} propto - hat{x}.$$ . The quadrupole term is used for focusing. The field takes the following form: . $$ mathbf{B}_{quad} propto y hat{x} + x hat{y},$$ . with the resulting force: . $$ mathbf{F}_{quad} propto -x hat{x} + y hat{y}.$$ . The force from the quadrupole is focusing in the horizontal direction, but defocusing in the vertical direction; however, net focusing is still achieved by alternating the direction of the quadrupoles. This is analogous to a beam of light passing through a series of converging and diverging lenses. If the spacing and curvature of the lenses is correctly chosen, a net focusing can be achieved. . . Focusing (QF) and defocusing (QD) quadrupoles modeled as magnetic lenses. The forces which result from these fields are linear, meaning they are proportional the $x$ or $y$ but not $x^2$, $y^3$, etc., and they are uncoupled, meaning the dynamics in the $x$ and $y$ dimensions are independent. Now, we may ask, can we really produce a perfect dipole or quadrupole field? The answer is no. In reality there will always be higher order multipoles present in the field, but people work very hard to ensure these are much smaller than the desired multipole. This video shows a bit of the construction process for these magnets. . Linearized equation of motion . Making the above approximation of perfect dipole and quadrupole magnets, and ignoring all other elements in the machine, we arrive at the equation of motion for a single particle in the transverse plane: . $$x&#39;&#39; + k(s)x = 0,$$ . where $x&#39; = dx/ds$ and $k(s + L) = k(s)$ for some distance $L$. We could also write a similar equation for $y$. It is conventional to use the slope $x&#39;$ instead of the velocity; this allows us to talk about the position of the particle in the lattice instead of the amount of time which has passed. The period length $L$ could be the entire circumference of a circular machine, or could be a smaller repeated subsection. . Solution . Envelope function . The general solution to Hill&#39;s equation is given by . $$x(s) = sqrt{ epsilon} ,w(s) cos left({ mu(s) + delta} right).$$ . This introduces an amplitude $w(s) = w(s + L)$ which we call the envelope function, as well as a phase $ mu$, both of which depend on $s$. The constants $ epsilon$ and $ delta$ are determined by the initial conditions. Let&#39;s plot this trajectory in a FODO (focus-off-defocus-off) lattice, which consists of evenly spaced focusing and defocusing quadrupoles. Here is the focusing strength within the lattice (QF is the focusing quadrupole and QD is the defocusing quadrupole): . . For now we can think of the lattice as repeating itself forever in the $s$ direction. Each black line below is represents the trajectory for a different initial position and slope; although the individual trajectories look rather complicated, the envelope function has a very simple form. . . Phase space . The particle motion becomes much easier to interpret if we observe it in position-momentum space, aka phase space. The following animation shows the evolution of the particle phase space coordinates at a single position in the lattice. The position shown is $s = nL/4$, where $n$ is the period number, which corresponds to the midpoint between the focusing and defocusing quadrupoles. . . &lt;/input&gt; Once Loop Reflect We see that the particle jumps along the boundary of an ellipse in phase space. The shape and orientation of the ellipse will change if we look at a different position in the lattice, but its area will be the same. So, the motion is determined by the dimensions and oriention of this ellipse throughout the lattice, as well as the location of the paricle on the ellipse boundary. This motivates the definition of the so-called Twiss parameters, which were first introduced by Courant and Snyder in 1958: . $$ beta = w^2, quad alpha = - frac{1}{2} beta&#39;, quad gamma = frac{1 + alpha^2}{ beta}.$$ . The dimensions of the phase space ellipse are nicely described by these parameters: . . The maximum extent of the ellipse is determined by $ beta$ in the $x$ direction and $ gamma$ in the $y$ direction. $ alpha$ is proportional to the slope of the $ beta$ function, and so determines the tilt angle of the ellipse. The position of a particle on the ellipse is given by the phase $ mu$. Finally, the invariant of the motion corresponding to the ellipse area is constructed from the Twiss parameters as . $$ epsilon = beta {x&#39;}^2 + 2 alpha xx&#39; + gamma x^2$$ . for any $x$ and $x&#39;$. The $ beta$ functions and phase advances in both dimensions are extremely important to measure and control in a real machine. Here is an example of the horizontal and vertical $ beta$ functions in the SNS accumulator ring. . . Transfer matrices . A helpful tool to pair with the parameterization we just introduced is the transfer matrix, a matrix which connects the phase space coordinates at two different positions: . $$ begin{bmatrix} x x&#39; end{bmatrix}_{s + L} = mathbf{M} begin{bmatrix} x x&#39; end{bmatrix}_{s}$$ . The transfer matrix can be written as $ mathbf{M} = mathbf{V} mathbf{P} mathbf{V}^{-1}$, where . $$ mathbf{V} = frac{1}{ sqrt{ beta}} begin{bmatrix} beta &amp; 0 - alpha &amp; 1 end{bmatrix}$$ and $$ mathbf{P} = begin{bmatrix} cos mu &amp; sin mu - sin mu &amp; cos mu end{bmatrix} $$ . The effect of $ mathbf{V}^{-1}$ is to deform the phase space ellipse into a circle while preserving its area. $ mathbf{P}$ is then just a rotation in phase space, and $ mathbf{V}$ then transforms back into a tilted ellipse. This is illustrated below. . . $ mathbf{V}$ can be thought of as a time-dependent transformation which removes the variance in the focusing strength, turning the parametric oscillator into a simple harmonic oscillator. Often it is called the Floquet transformation. . Conclusion . We&#39;ve presented the solution to Hill&#39;s equation, which describes a parameteric oscillator. The equation pops up in multiple areas, but we focused on its application in accelerator physics, in which Hill&#39;s equation describes the transverse motion of a single particle in an accelerator with perfectly linear magnetic fields. . The solution is best understood geometrically: particles move around the surface of an ellipse in phase space, the area of which is an invariant of the motion. The dimensions and orientation of the ellipse are determined by $ alpha$ and $ beta$, and the location of the paricle on the ellipse boundary is determined by $ mu$. These parameters can be used to construct a time-dependent transformation ($ mathbf{V}$) which turns the parametric oscillator into a simple harmonic oscillator. . The next post will examine how this treatment can be extended to include coupling between the horizontal and vertical dimensions. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/oscillators/2021/01/21/parametric_oscillators.html",
            "relUrl": "/physics/accelerators/oscillators/2021/01/21/parametric_oscillators.html",
            "date": " • Jan 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m currently completing a PhD in physics from the University of Tennessee, Knoxille, studying high-intensity proton beam dynamics as part of the Accelerator Physics group at the Spallation Neutron Source (SNS). I previously obtained a BS in physics from Wheaton College. . Outside of physics, I’ve recently enjoyed learning about machine learning through courses in the UTK computer science department, and now through self-study. My current main non-STEM interest is philosophy, particularly philosophy of religion. In my spare time, I sometimes exercise, play chess (badly), or play piano (badly). . This blog is a space for me to organize my thoughts in writing; I’ve found this to be a useful exercise when exploring new topics or improving my understanding of familiar topics. All my posts are categorized here. . Publications . Computation of the matched envelope of the Danilov distribution, PRAB, 04.29.2021 | . Presentations . Computation of the matched envelope of the Danilov distribution, AP Group Meeting, SNS, 01.29.2021 | Parameterization of coupled motion, AP Group Meeting, SNS, 01.15.2021 | Thesis proposal, University of Tennessee, 09.30.2020 | . Recommendations . Here are some things I recommend checking out. I’ll try to update this list as I remember(discover) old(new) things. . Articles The Unreasonable Effectiveness of Mathematics in the Natural Sciences — Eugene Wigner | Why are (some) physicists so bad at philosophy? — Edward Feser | . | Books (mostly textbooks at the moment) Accelerator physics Accelerator Physics — Lee | Introduction to the Physics of High Energy Accelerators — Edwards/Syphers | Measurement and Control of Charged Particle Beams — Minty/Zimmerman | Particle Accelerator Physics — Wiedemann | Space Charge Physics for Particle Accelerators — Hofmann | . | Computer science Algorithms — Sedgewick/Wayne | Elements of Programming Interviews in Python — Aziz/Lee/Prakash | Neural Networks and Deep Learning — Nielsen | Pattern Recognition and Machine Learning — Bishop | . | Math Mathematics of Classical and Quantum Physics — Byron/Fuller | Mathematical Methods for Physicists — Arfken/Weber | . | Philosophy/religion Infinity, Causation, and Paradox — Alexander Pruss | Knowledge and Christian Belief - Alvin Plantinga | . | Physics core Classical Mechanics — Taylor | Classical Mechanics — Goldstein | Introduction to Electrodynamics — Griffiths | Introduction to Thermal Physics — Schroeder | Quantum Mechanics: Concepts and Applications — Zettili | Statistical Physics of Particles — Kardar | . | Other Crime and Punishment — Fyodor Dostoevsky | Who We Are and How We Got Here — David Reich | . | . | Places Tiger Leaping Gorge | Black Canyon of the Gunnison National Park | . | Videos Dirac Lecture 1 (of 4) - Quantum Mechanics | Freeman Dyson: A ‘Rebel’ Without a Ph.D. | History, Development and Application of Neutron Sources | Open science: Michael Nielsen at TEDxWaterloo | Toph is blind | . | .",
          "url": "https://austin-hoover.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://austin-hoover.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}