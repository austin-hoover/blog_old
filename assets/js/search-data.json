{
  
    
        "post0": {
            "title": "Nonlinear resonances",
            "content": "Most of us are familiar with the experience of pushing someone else on a playground swing. We intuitively know that we should sync our pushes with the swing oscillation frequency, which appears to be independent of the swing amplitude. This strategy employs the idea of a resonance, which is an increase of the oscillation amplitude of a system for certain driving frequencies. In this post we first review the mathematics of this simple example, then extend the machinery to the nonlinear dynamics in a particle accelerator. My goal here is to write down the main results which are relevant to accelerators in order to improve my own understanding of the topic. . . Linear resonances . Consider a simple pendulum in the small angle approximation which, if left alone, oscillates at freqency $ omega_0^2$. . . The equation of motion for $ phi$ is . $$ frac{d^2}{dt^2}{ phi} + omega_0^2 phi = 0. tag{1}$$ . Now consider a sinusoidal driving force $f(t) = f_0 cos( omega t)$ as well as a damping term: . $$ frac{d^2}{dt^2}{ phi} + b dot{ phi} + omega_0^2 phi = f_0 cos( omega t).$$ . After doing some work it can be shown that the gravitational, damping, and driving forces initially fight against each other, but in the end the driving force dominates and the angle oscillates as . $$ phi(t) = A cos( omega t - delta) tag{2}$$ . where . $$A^2 = frac{f_0^2}{( omega - omega_0)^2 + b omega^2}. tag{3}$$ . The figure below shows the squared amplitude as the driving frequency is varied. The maximum amplitude approaches infinity as the damping term goes to zero. (Keep in mind that this is valid for a harmonic oscillator, but a pendulum is only approximately a harmonic oscillator for small angles.) . The next step is to consider what happens when the driving force is not a pure sine wave. We&#39;ll only consider periodic driving forces, and any periodic function can be written as a sum of sines and cosines of different frequencies. From now on we&#39;ll forget about the pendulum and just think of the position $x$ of a general linear oscillator. Assuming $f(t)$ is an even function so that we can drop the sine terms in the Fourier expansion, the equation of motion becomes . $$ ddot{x} + b dot{x} + omega_0^2 = sum_{n=0}^{ infty} {f_n cos(n omega t)}. tag{4}$$ . The solution is found by just adding up the solutions to each term in the sum: . $$x(t) = sum_{n = 0}^{ infty}{A_n cos{(n omega t - delta_n)}}. tag{5}$$ . The resonance condition will apply to each of these amplitudes individually, which means that a resonance could be excited if any component of the driving force is near the natural frequency. . Sources of nonlinearity . We&#39;re now going to apply these ideas to a particle accelerator. We&#39;ll assume small transverse oscillations, no acceleration, no deviation from the design momentum, and no particle-particle interactions. Under these assumptions, the transverse equation of motion of a particle with charge $q$ and momentum $p$ in a magnetic field $ mathbf{B} = (B_x, B_y)^T$ is . $$ x&#39;&#39; = - frac{q}{p} B_y(s), tag{6}$$ $$ y&#39;&#39; = + frac{q}{p} B_x(s). $$ . Remember that $x&#39; = dx/ds$, and $s$ is the position in accelerator (from now on we&#39;ll assume a circular accelerator or &quot;ring&quot; of circumference $L$). Any 2D magnetic field can be expanded as the following infinite sum: . $$B_y - iB_x = sum_{n=1}^{ infty} left({b_n - ia_n} right) left( frac{x + iy}{r_0} right)^{n-1}, tag{7}$$ . where $r_0$ is a constant. The $b_n$ and $a_n$ terms are called the multipole coefficients and skew multipole coefficients, respectively. The $n^{th}$ term in the expansion is the field produced by $2n$ symmetrically arranged magnetic poles. . . We can see that terms with $n &gt; 2$ introduce nonlinear powers of $x$ and $y$ on the right side of Eq. (6), while terms with $n le 2$ introduce linear or constant terms. One may ask why we are considering a general magnetic field when in reality we use only dipoles and quadrupoles. The answer is two-fold. First, the best we can do in a real magnet is to make the $n &gt; 2$ terms as small as possible; they aren&#39;t zero and we need to know how they affect the motion. Second, sextupoles (and sometimes even octopoles) can be introduced intentionally. Their primary use is to correct for the fact that not all beam particles have the same momentum. . . An example of a sextupole electromagnet. Credit: CERN. Perturbation analysis . The nonlinear terms in Eq. (6) eliminate any hope of an analytic solution. There are two options in situations such as these: 1) use a computer, or 2) use perturbation theory. The strategy of option 2 is to make approximations until an exact solution can be found, then to add in small nonlinear terms and see how the solution changes. The process can be repeated to solve the problem up to a certain order of accuracy. Usually this is infeasible beyond a few iterations, but it is a helpful tool for gaining intuition and interpreting numerical results. In particular, we&#39;ll be looking for regions where the particle may encounter a resonance. Without many details, let&#39;s try out the perturbation approach. Later on we&#39;ll use a computer and see if our analysis was accurate. . Floquet coordinates . The first step is to find an exact solution under some approximation. We&#39;ll neglect coupling by setting $y = 0$ and focus on one dimension to make things easier. Let&#39;s denote the linear focusing from the lattice by $k$, with all other terms in the field expansion folded into $ Delta B$ (there are still $n = 1$ and $n = 2$ terms in $ Delta B$, but they represent deviations from the design values). We&#39;re also assuming that these variables are normalized by the ratio $q / p$. This results in the equation of motion . $$ x&#39;&#39; + k(s)x = Delta B. tag{8}$$ . This is Hill&#39;s equation with a nonlinear driving term. The stable solution when $ Delta B = 0$ is . $$x(s) = sqrt{ epsilon beta(s)} cos left({ mu(s) + delta} right), tag{9}$$ . with the phase advance is given by . $$ mu(s) = int_{0}^{s}{ frac{ds}{ beta(s)}}. tag{10}$$ . These pseudo-harmonic oscillations are still a bit difficult to visualize, so it&#39;s helpful to perform the Floquet transfromation which scales the $x$ coordinate as . $$x(s) rightarrow u(s) = frac{x(s)}{ sqrt{ beta_x(s)}}. tag{11}$$ . Furthermore, it is convenient to replace the $s$ coordinate with . $$ phi(s) = frac{1}{ nu_0} int_{0}^{C}{ frac{ds}{ beta_x(s)}}. tag{12}$$ . Here $ nu_0$ is the number of phase space oscillations per trip around the ring. As a result, the unperturbed equation of motion becomes (with $ dot{x} = dx / d phi$) . $$ ddot{u} + nu_0^2 u = 0. tag{13}$$ . But this is just a harmonic oscillator! The trajectory in phase space is a circle, and the particle revolves once around this circle for every turn around the ring. Finally, we can write $ Delta B$ as a power series in $u$ and derive the equation of motion in Floquet coordinates: . $$ ddot{u} + nu_0^2 u = - nu_0^2 beta^{3/2} Delta B = - nu_0^2 sum_{n=0}^{ infty} left({ beta^{ frac{n + 3}{2}} b_{n+1}} right) u^n. tag{14}$$ . Fourier expansion . The tools to analyze driven harmonic oscillators are now available to us. Similar to Eq. (4), each term on the right hand side can be Fourier expanded, the reason being that $ beta$ (the oscillation amplitude of the unperturbed motion) and $b_n$ (a multipole coefficient) depend only on the position in the ring, so of course they are periodic in $ phi$. Grouping these terms together and performing the expansion gives . $$ ddot{u} + nu_0^2 u = - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} C_{n,k} , u^n , e^{ik phi}. tag{16}$$ . We&#39;re now going to linearize this equation. This means plugging in $u = u_0 + delta u$, where $u_0$ is the unperturbed solution and $ delta_u$ is small, and discarding all higher powers of $ delta_u$. This gives . $$ ddot{ delta u} + nu_0^2 delta u approx - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} C_{n,k} , u_0^n , e^{ik phi} . tag{17}$$ . This equation tells us how the perturbation evolves with time — ideally it remains finite, but at a resonant condition it will grow without bound. The final step is to write $u_0^n$ in a managable form. There is this trick involving the binomial expansion: . $$ u_0^n propto cos^n( nu phi) = frac{1}{2^n} sum_{m=0}^{n} binom{n}{m} e^{i(n-2m) nu_0 phi}. tag{17}$$ . So, we finally arrive at . $$ ddot{ delta u} + nu_0^2 delta u approx - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} sum_{m=0}^{n} {n choose m} frac{C_{n,k}}{2^n} e^{i left[(n - 2m) nu_0 + k right] phi}. tag{18} $$ . There are a lot of indices floating around; $n$ is one less than the multipole coefficient of the magnetic field, $k$ is for the Fourier expansion, and $m$ is just a dummy index we used to binomially expand $u_0^2$. . Resonance diagram . Eq. (18) describes a driven harmonic oscillator like Eq. (5), so we can expect a resonance condition to occur when any of the frequency components of the driving force aare close to the natural frequency $ nu_0$. In other words, a resonance could occur when . $$ (n - 2m) nu_0 + k = pm nu_0. tag{19}$$ . If you write out the different cases ($n$ = 0, 1, 2, ...), you&#39;ll find that dipole terms ($n = 0$) forbid integer tunes, quadrupole terms forbid 1/2 integer tunes, sextupole terms forbid 1/3 integer tunes, and so on. The same thing can be done for the vertical dimension. Once coupling is included $x$ and $y$, we&#39;re lead to the definition of resonance lines: . $$ M_x nu_x + M_y nu_y = N, tag{20}$$ . where $M_x$, $M_y$, and $N$ are integers and $|M_x| + |M_y|$ is the order of the resonance. The reason for calling these resonance lines is because they define lines in $ nu_x$-$ nu_y$ space (tune space). You can click through the following animation to see how the lines fill up the space as higher order resonances are included. . &lt;/input&gt; Once Loop Reflect Resonance strengths tend to decrease with order number, so people generally don&#39;t consider anything beyond order 3 or 4. That being said, the machine tunes $ nu_x$ and $ nu_y$ need to be carefully chosen to avoid the low order resonances. Ideally all beam particles occupy this single point in tune space, but space charge complicates things by decreasing the tune by different amounts for each particle, possible placing them on one of the above resonance lines. This effect, called tune spread, places a fundamental limit on the number of particles in the beam. . Numerical exploration of the sextupole . Let&#39;s explore the behavior of a beam under the influence of a sextupole magnet. This section recreates some figures from the book Accelerator Physics by S. Y. Lee. The easiest way to do this is to approximate the multipole as an instantaneous change to the slope of the particle&#39;s trajectory. This is valid if the magnet isn&#39;t too long. . import numpy as np import numpy.linalg as la import scipy class Multipole: &quot;&quot;&quot;Class to apply multipole kick to particle. Adapted from PyORBIT tracking routine in `py-orbit/src/teapotbase.cc`. Attributes - order : int The order of the multipole term (dipole: 1, quadrupole: 2, ...). strength : float Integrated multipole strength [m^-(order - 1)]. skew : bool If True, rotate the magnet 45 degrees. &quot;&quot;&quot; def __init__(self, order, strength, skew=False): self.order, self.strength, self.skew = order, strength, skew def track_part(self, vec): &quot;&quot;&quot;Apply transverse kick to particle slopes. vec : ndarray, shape (4,) Transverse phase space coordinate vector [x, x&#39;, y, y&#39;]. &quot;&quot;&quot; x, xp, y, yp = vec k = self.strength / np.math.factorial(self.order - 1) zn = (x + 1j*y)**(self.order- 1) if self.skew: vec[1] += k * zn.imag vec[3] += k * zn.real else: vec[1] -= k * zn.real vec[3] += k * zn.imag return vec . order = 3 strength = 0.5 multipole = Multipole(order, strength, skew=False) . The situation we&#39;ll consider is a circular lattice which is made of linear uncoupled elements + one thin sextupole. We&#39;ll observe the beam at the location of the sextupole after each turn. A key result of the linear theory is that the details of the rest of the lattice are unimportant for this task. All we need to do is choose the Twiss parameters and tune in each dimension to form the transfer matrix, then we can just track using matrix multiplication. Recall that the transfer matrix is written as $ mathbf{M} = mathbf{V P V^{-1}}$, where $ mathbf{V} = mathbf{V}( alpha_x, alpha_y, beta_x, beta_y)$ performs the Floquet normalization and $ mathbf{P} = mathbf{P}( nu_x, nu_y)$ is a rotation in the $x$-$x&#39;$ and $y$-$y&#39;$ phase spaces by the angle $2 pi nu_x$ and $2 pi nu_y$, respectively. The following class implements this representation of the lattice. . def V_2D(alpha, beta): &quot;&quot;&quot;Floquet normalization matrix in 2D phase space.&quot;&quot;&quot; return np.array([[beta, 0.0], [-alpha, 1.0]]) / np.sqrt(beta) def P_2D(tune): &quot;&quot;&quot;Phase advance matrixmin 2D phase space.&quot;&quot;&quot; phase_advance = 2 * np.pi * tune cos, sin = np.cos(phase_advance), np.sin(phase_advance) return np.array([[cos, sin], [-sin, cos]]) class Lattice: &quot;&quot;&quot;Represents lattice as linear one-turn transfer matrix + multipole kick. Attributes - M : ndarray, shape (4, 4) Linear one-turn transfer matrix. aperture : float Radius of cylindical boundary containing the particles [m]. multipole : Multipole object Must implement `track_part(vec)`, where vec = [x, xp, y, yp]. &quot;&quot;&quot; def __init__(self, alpha_x, alpha_y, beta_x, beta_y, tune_x, tune_y, aperture=0.2): &quot;&quot;&quot;Constructor. Parameters - alpha_x, alpha_y, beta_x, beta_y : float Twiss parameters at the lattice entrance. tune_x, tune_y : float Number of phase space oscillations per turn. &quot;&quot;&quot; self.P = np.zeros((4, 4)) self.V = np.zeros((4, 4)) self.M = np.zeros((4, 4)) self.P[:2, :2] = P_2D(tune_x) self.P[2:, 2:] = P_2D(tune_y) self.V[:2, :2] = V_2D(alpha_x, beta_x) self.V[2:, 2:] = V_2D(alpha_y, beta_y) self.M = la.multi_dot([self.V, self.P, la.inv(self.V)]) self.aperture = aperture self.multipole = None def add_multipole(self, multipole): self.multipole = multipole def track_part(self, vec): &quot;&quot;&quot;Track a single particle through the lattice. vec : ndarray, shape (4,) Transverse phase space coordinate vector [x, x&#39;, y, y&#39;]. &quot;&quot;&quot; vec = np.matmul(self.M, vec) if self.multipole is not None: vec = self.multipole.track_part(vec) return vec def track_bunch(self, X): &quot;&quot;&quot;Track a particle bunch through the lattice. X : ndarray, shape (nparts, 4) Transverse phase space coordinate array. &quot;&quot;&quot; X = np.apply_along_axis(self.track_part, 1, X) return self.collimate(X) def collimate(self, X): &quot;&quot;&quot;Delete particles outside aperture.&quot;&quot;&quot; radii = np.sqrt(X[:, 0]**2 + X[:, 2]**2) return np.delete(X, np.where(radii &gt; self.aperture), axis=0) def get_matched_bunch(self, nparts=2000, emittance=10e-6, cut=3.0): &quot;&quot;&quot;Generate truncated Gaussian distribution matched to the lattice.&quot;&quot;&quot; X = scipy.stats.truncnorm.rvs(a=4*[-cut], b=4*[cut], size=(nparts, 4)) A = np.sqrt(emittance) * np.identity(4) V = self.V X = np.apply_along_axis(lambda vec: np.matmul(A, vec), 1, X) X = np.apply_along_axis(lambda vec: np.matmul(V, vec), 1, X) return X . The next few sections produce plots showing the beam behavior near a few different resonance conditions. First some helper functions. . # Define Twiss parameters at the observation point alpha_x = alpha_y = 0.0 beta_x = beta_y = 20.0 def create_lattice(tune_x, tune_y, multipole=None): lattice = Lattice(alpha_x, alpha_y, beta_x, beta_y, tune_x, tune_y) lattice.add_multipole(multipole) return lattice def get_traj(lattice, emittance, nturns=1): &quot;&quot;&quot;Return array of shape (nturns, 4) of tracked single particle coordinates. The vertical coordinate and slope are set to zero. &quot;&quot;&quot; X = np.array([[np.sqrt(emittance * beta_x), 0, 0, 0]]) tracked_vec = [X[0]] for _ in range(nturns): X = lattice.track_bunch(X) if X.shape[0] == 0: # particle was deleted break tracked_vec.append(X[0]) return 1000 * np.array(tracked_vec) # convert from m to mm def compare_traj(tunes_x, tune_y, emittances, nturns=1, multipole=None, limits=(45, 2.5), **kws): &quot;&quot;&quot;Compare trajectories w/ different emittances as horizontal tune is scaled.&quot;&quot;&quot; kws.setdefault(&#39;s&#39;, 1) kws.setdefault(&#39;c&#39;, &#39;pink8&#39;) fig, axes = plot.subplots(nrows=2, ncols=3, figsize=(7.25, 4)) xlim, ylim = (-limits[0], limits[0]), (-limits[1], limits[1]) axes.format(xlabel=&quot;x [mm]&quot;, ylabel=&quot;x&#39; [mrad]&quot;, xlim=xlim, ylim=ylim) for ax, tune_x in zip(axes, tunes_x): lattice = create_lattice(tune_x, tune_y, multipole) for emittance in emittances: tracked_vec = get_traj(lattice, emittance, nturns) ax.scatter(tracked_vec[:, 0], tracked_vec[:, 1], **kws) ax.annotate(r&#39;$ nu_x = {:.3f}$&#39;.format(tune_x), xy=(0.7, 0.9), xycoords=&#39;axes fraction&#39;, bbox=dict(fc=&#39;w&#39;, ec=&#39;k&#39;)) return axes def track_bunch(X, lattice, nturns=1): &quot;&quot;&quot;Track and return list of coordinate array after each turn. Also return the fraction of particles which were lost (exceeded aperture) at each frame.&quot;&quot;&quot; coords, nparts, frac_lost = [X], X.shape[0], [0.0] for _ in range(nturns): X = lattice.track_bunch(X) coords.append(X) frac_lost.append(1 - X.shape[0] / nparts) return [1000*X for X in coords], frac_lost def animate_phase_space(coords, frac_lost=None, limits=(55, 5)): &quot;&quot;&quot;Create animation of turn-by-turn x-x&#39; and y-y&#39; distributions.&quot;&quot;&quot; fig, axes = plot.subplots(ncols=2, figsize=(5, 2.5), wspace = 0.75, sharey=False, sharex=False) xlim, ylim = (-limits[0], limits[0]), (-limits[1], limits[1]) axes.format(xlim=xlim, ylim=ylim) axes[0].format(xlabel=&quot;x [mm]&quot;, ylabel=&quot;x&#39; [mrad]&quot;) axes[1].format(xlabel=&quot;y [mm]&quot;, ylabel=&quot;y&#39; [mrad]&quot;) myplt.despine(axes) plt.close() kws = dict(marker=&#39;.&#39;, c=&#39;steelblue&#39;, ms=3, lw=0, markeredgewidth=0, fillstyle=&#39;full&#39;) line0, = axes[0].plot([], [], **kws) line1, = axes[1].plot([], [], **kws) def update(t): x, xp, y, yp = coords[t].T line0.set_data(x, xp) line1.set_data(y, yp) axes[0].set_title(&#39;Turn {}&#39;.format(t)) if frac_lost: axes[1].set_title(&#39;Frac. lost = {:.3f}&#39;.format(frac_lost[t])) return animation.FuncAnimation(fig, update, frames=len(coords)) . 1/3 integer resonance . We focus first on the 1/3 integer resonance. Below, a particle is tracked over 100 turns at a for a few different initial amplitudes, setting $y = y&#39; = 0$ in all cases. The $x$-$x&#39;$ trajectory should be an upright ellipse in the absence of nonlinear elements. . tunes_x = np.linspace(0.61, 0.66, 6) tune_y = 0.518 emittances = 1e-6 * np.array([10, 30, 60, 90]) nturns = 100 axes = compare_traj(tunes_x, tune_y, emittances, nturns) axes.format(suptitle=&#39;Linear lattice&#39;) . Now turn on the sextupole magnet. . axes = compare_traj(tunes_x, tune_y, emittances, nturns, multipole) axes.format(suptitle=&#39;Linear lattice + sextupole&#39;) . The initially elliptical orbits are morphed into a triangular shape as the tune approaches the resonance condition, and some of the larger orbits become unstable. It turns out that by looking at the Hamiltonian you can find a triangular region defining a separatrix between stable and unstable motion. Particles inside the triangle will oscillate forever, particles at the corner of the triangle are at unstable equilibrium points, and particles outside the triangle will eventually stream outward from the corners. This is easier to see by tracking a bunch of particles. The interesting stuff will be in the horizontal plane, but I&#39;ll plot the vertical plane as well for comparison. . lattice = create_lattice(0.66, tune_y, multipole) X = lattice.get_matched_bunch() coords, frac_lost = track_bunch(X, lattice, nturns=50) animate_phase_space(coords, frac_lost) . &lt;/input&gt; Once Loop Reflect The triangular region of stability is clearly visible at the end of 50 turns. Interestingly, the third order resonance can actually be exploited to extract a beam from an accelerator at a much slower rate than normal. To do this, the strength and spacing of sextupole magnets must be carefully chosen to control the shape and orientation of the stability triangle, then tune is slowly moved closer to the 1/3 integer resonance value. The result is that the triangle shrinks as the stable phase space area decreases, and that more and more particles will find themselves in the unstable area and eventually stream out along the vertices. . Integer resonance . The sextupole should also excite the integer resonance. . compare_traj(np.linspace(0.96, 0.976, 6), tune_y, emittances, nturns, multipole, limits=(60, 2.5)); . lattice = create_lattice(0.99, 0.18, multipole) animate_phase_space(*track_bunch(X, lattice, nturns=75)) . &lt;/input&gt; Once Loop Reflect Cool pattern! The separatrix is now shaped like a tear drop. It looks like it&#39;s evolving more slowly because the tune is close to an integer, so the particles almost return to the same location in phase space after a turn. . Higher order resonances . There are also higher order resonances which a sextupole can drive. You can actually find fourth and fifth order resonances if you perform perturbation theory up to second order (at least that&#39;s what I&#39;m told in a textbook... I&#39;d like to avoid carrying out such a procedure). Do these show up using our mapping equations? They are expected to be weaker, so we&#39;ll double the sextupole strength. . emittances = 1e-6 * np.array([1, 2, 7, 10, 25, 50, 100, 150, 200, 250, 350]) compare_traj(np.linspace(0.7496, 0.798, 6), 0.23, emittances, 1000, Multipole(3, 1.0), limits=(150, 6), s=0.1); . These are really interesting plots. The tune near 0.75 (it&#39;s actually 0.7496) is exciting a fourth order resonance, while the tune near 0.8 is exciting a fifth order resonance. In all the plots, the low amplitude orbits are stable ellipses. We then see the behavior change as the amplitude is increase, with the particle jumping between distinct &quot;islands&quot;. Eventually the trajectories once again form closed loops, but in deformed shapes. The motion is unstable at even larger amplitudes. Understanding exactly why the the plots look like they do would probably require carrying out the second-order perturbation in the Hamiltonian formalism. . Conclusion . This post outlined the theory of nonlinear resonances driven by magnetic multipoles. The effect of a sextupole-driven resonance on the phase space trajectory was then examined using mapping equations. I can&#39;t say that I have a great grasp of this huge topic, but taking the time to just write down the equations really helped to make it less mysterious to me. Eq. (20) is referenced all the time in accelerator physics; for example, I talked about it in my thesis proposal. During that proposal, one of my commitee members asked what was actually going on at the resonance condition. I didn&#39;t really know, so I said something like &quot;well... that&#39;s just how it is&quot;. Luckily, one of the other committee members gave a succinct answer about how this is the result of classical perturbation analysis in the presence of nonlinear driving terms. Since then I have been wanting to be able to at least sketch how one would arrive at this equation, so this post wasn&#39;t a total waste of time. Here are a number of helpful references: . Lectures S. Lund, Transverse Particle Resonances with Application to Circular Accelerators | E. Prebys, Resonances and Coupling | . | Textbooks D. Edwards and M. Syphers, An introduction to the Physics of High Energy Accelerators | H. Wiedemann, Particle Accelerator Physics | S. Y. Lee, Accelerator Physics | L. Reichl, The Transition to Chaos — Conservative Classical Systems and Quantum Manifestations | J. Taylor, Classical Mechanics | H. Goldstein, Classical Mechanics | . | .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/nonlinear%20systems/resonances/2021/03/28/nonlinear_resonances.html",
            "relUrl": "/physics/accelerators/nonlinear%20systems/resonances/2021/03/28/nonlinear_resonances.html",
            "date": " • Mar 28, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Particle-in-cell simulation",
            "content": "In my research I utilize PyORBIT, one of many simulation codes for beam physics. One of the key components of the these simulations is the inclusion of the electromagnetic interactions between particles in the beam; these are known as space charge forces, which refers to the charge density of the beam in free space. This post examines the particle-in-cell (PIC) algorithm used to compute space charge forces by building a simple simulation engine in Python. . Theoretical model . We&#39;ll use bunch to refer to a group of particles in three-dimensional (3D) space, and we&#39;ll use a local cartesian coordinate system whose origin moves with the center of the bunch as shown below: . . The $s$ coordinate specifies the position of the bunch in the accelerator, and the path can be curved. Now for a few assumptions and approximations. First, the center of the bunch is moving at a constant velocity $ beta c$, where $c$ is the speed of light, so the time variable can be replaced by the position $s = beta c t$. The slope $x&#39; = dx/ds$ can be used instead of the velocity, and is measured in milliradians based on the small angle approximation. Second, we&#39;ll assume the transverse ($x$-$y$) size of the bunch is much smaller than its length, so that for a given transverse slice it is as if the bunch had a uniform density and infinite length in the longitudinal direction. Our focus will be on the transverse dynamics of this slice, as shown below, so each tracked &quot;particle&quot; will really be an infinite line of charge; any dynamics in the longitudinal direction will be ignored. . . Credit: G. Franchetti Another approximation is to neglect any magnetic fields generated by the beam, again valid if the transverse momenta are very small compared to the bunch kinetic energy. All this being said, the equations of motion without any external foces, i.e., in free space, can be written as . $$ mathbf{x}&#39;&#39; = frac{q}{mc^2 beta^2 gamma^3} mathbf{E}, tag{1}$$ . where $ mathbf{x} = [x, y]^T$ is the coordinate vector, $ mathbf{E} = [E_x, E_y]^T$ is the self-generated electric field, $m$ is the particle mass, and $ gamma = left({1 - beta^2} right)^{-1/2}$. Let&#39;s first address the factor $ gamma^{-3}$ in the equation of motion, which means that the space charge force goes to zero as the velocity approaches the speed of light. This is because parallel moving charges generate an attractive magnetic force which grows with velocity, completely cancelling the electric force in the limit $v rightarrow c$. . . Credit: OpenStax University PhysicsOne may ask: what about the rest frame, in which there is no magnetic field? But special relativity says that electrogmagnetic fields change with reference frame. Using the transformations defined here, you can quickly prove that . $$ mathbf{E}_{lab} = frac{ mathbf{E}_{rest}}{ gamma}. tag{2}$$ . This inverse relationship between velocity and the space charge force has real-life consequences. It tells us that space charge is important if 1) the beam is very intense, meaning there are many particles in a small area, or 2) the beam is very energetic, meaning it is moving extremely fast. For example, space charge can usually be ignored in electron beams, which move near the speed of light for very modest energies due to their tiny mass, but is significant in high-intensity, low-energy hadron accelerators such as FRIB, SNS, and ESS. . We should now address the difficulty in determining the evolution of this system: the force on a particle in an $n$-particle bunch depends on the positions of the other $n - 1$ particles. The approach of statistical mechanics to this problem is to introduce a distribution function $f( mathbf{x}, mathbf{x}&#39;, t)$ which gives the number of particles in an infinitesimal volume of phase space. The Vlasov-Poisson system of equations determines the evolution of $f$ (as long as we ignore collisions between particles and any magnetic fields): . $$ frac{ partial{f}}{ partial{s}} + mathbf{x}&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}}} + mathbf{x}&#39;&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}&#39;}}. tag{3}$$ . We know $ mathbf{x&#39;&#39;}$ from Eq. (1). The electric field is obtained from Poisson&#39;s equation: . $$ nabla cdot mathbf{E} = - nabla^2 phi = frac{ rho}{ varepsilon_0}. tag{4}$$ . Finally, the transverse charge density $ rho$ is determined by . $$ rho = q int{f dx&#39;dy&#39;}. tag{5}$$ . Although these equations are easy to write down, they are generally impossible to solve analytically. We need to turn to a computer for help. . Computational method . The Vlasov equation could be solved directly, but this is difficult, especially in 2D or 3D. On the other end of the spectrum, the notion of a fluid in phase space could be abandonded and each particle could be tracked individually, computing the forces using direct sums. But this is infeasible with current hardware; the time complexity would by $O(n^2)$, where $n$ is the number of particles, and $n$ may be on the order of $10^{14}$. The particle-in-cell (PIC) method is a sort of combination of these two approaches. The idea is to track a group of macroparticles according to Eq. (1), each of which represents a large number of real particles. The fields, however, are solved from Eq. (4). The key step is transforming back and forth between a discrete and continuous representation of the bunch. The simulation loop for the PIC method is shown below. . . In the next sections I will discuss each of these steps and implement them in Python code. Let&#39;s first create a Bunch class, which is a simple container for the bunch coordinates. (Note: there are some constants and helper functions I&#39;ve imported that I don&#39;t define. The whole package is posted on GitHub.) . import numpy as np class Bunch: &quot;&quot;&quot;Container for 2D distribution of particles. Attributes - intensity : float Number of physical particles in the bunch. Default 1.5e14. length : float Length of the bunch [m]. Default: 250. mass, charge, kin_energy : float Mass [GeV/c^2], charge [C], and kinetic energy [GeV] per particle. line_density : float Longitudinal particle density [1 / m]. Default 1.5e14 / 250. line_charge_density : float Longitudinal charge density [C / m]. nparts : float Number of macroparticles in the bunch. macrosize : float Number of physical particles represented by each macroparticle. macrocharge : float Charge represented by each macroparticle [C]. perveance : float Dimensionless space charge perveance. sc_factor : float Factor for space charge kicks such that that x&#39;&#39; = factor * Ex. X : ndarray, shape (nparts, 4) Array of particle coordinates. Columns are [x, x&#39;, y, y&#39;]. Units are meters and radians. positions : ndarray, shape (nparts, 2): Just the x and y positions (for convenience). &quot;&quot;&quot; def __init__(self, intensity=1e14, length=250., mass=0.938, kin_energy=1., charge=elementary_charge): self.intensity, self.length = intensity, length self.mass, self.kin_energy, self.charge = mass, kin_energy, charge self.gamma = 1 + (self.kin_energy / self.mass) self.beta = np.sqrt(1 - (1 / self.gamma)**2) self.nparts = 0 self.line_density = intensity / length self.line_charge_density = charge * self.line_density self.perveance = get_perveance(self.line_density, self.beta, self.gamma) self.sc_factor = get_sc_factor(charge, mass, self.beta, self.gamma) self.compute_macrosize() self.X, self.positions = None, None def compute_macrosize(self): &quot;&quot;&quot;Update the macrosize and macrocharge.&quot;&quot;&quot; if self.nparts &gt; 0: self.macrosize = self.intensity // self.nparts else: self.macrosize = 0 self.macrocharge = self.charge * self.macrosize def fill(self, X): &quot;&quot;&quot;Fill with particles.&quot;&quot;&quot; self.X = X if self.X is None else np.vstack([self.X, X]) self.positions = self.X[:, [0, 2]] self.nparts = self.X.shape[0] self.compute_macrosize() def compute_extremum(self): &quot;&quot;&quot;Get extreme x and y coorinates.&quot;&quot;&quot; self.xmin, self.ymin = np.min(self.positions, axis=0) self.xmax, self.ymax = np.max(self.positions, axis=0) self.xlim, self.ylim = (self.xmin, self.xmax), (self.ymin, self.ymax) . Weighting . Starting from a group of macroparticles, we need to produce a charge density $ rho_{i,j}$ on a grid. The most simple approach is the nearest grid point (NGP) method, which, as the name suggests, assigns the full particle charge to the closest grid point. This is commonly called zero-order weighting; although it is very fast and easy to implement, it is not commonly used because it can lead to significant noise. A better method called cloud-in-cell (CIC) treats each particle as a rectangular, uniform density cloud of charge with dimensions equal to the grid spacing. A fractional part of the charge is assigned based on the fraction of the cloud overlapping with a given cell. This can be thought of as first-order weighting. To get a sense of what these methods are doing (in 1D), we can slide a particle across a cell and plot the resulting density of the cell at each position, thus giving an effective particle shape. . The NGP method leads to a discontinuous boundary, while the CIC method leads to a continous boundary (but discontinous derivative). There are also higher order methods which lead to a smooth boundary, but I don&#39;t cover those here. . We also need to perform the inverse operation: given the electric field at each grid point, interpolate the value at each particle position. The same method applies here. NGP just uses the electric field at the nearest grid point, while CIC weights the four nearest grid points. The following Grid class implements the CIC method. Notice that I utilized Cython in the for-loop in the distribute method. I couldn&#39;t figure out a way to perform the operation with this loop, and in pure Python it took about 90% of the runtime for a single simulation step. Using Cython gave a significant performance boost. . %%cython import numpy as np from scipy.interpolate import RegularGridInterpolator class Grid: &quot;&quot;&quot;Class for 2D grid. Attributes - xmin, ymin, xmax, ymax : float Minimum and maximum coordinates. Nx, Ny : int Number of grid points. dx, dy : int Spacing between grid points. x, y : ndarray, shape (Nx,) or (Ny,) Positions of each grid point. cell_area : float Area of each cell. &quot;&quot;&quot; def __init__(self, xlim=(-1, 1), ylim=(-1, 1), size=(64, 64)): self.xlim, self.ylim = xlim, ylim (self.xmin, self.xmax), (self.ymin, self.ymax) = xlim, ylim self.size = size self.Nx, self.Ny = size self.dx = (self.xmax - self.xmin) / (self.Nx - 1) self.dy = (self.ymax - self.ymin) / (self.Ny - 1) self.cell_area = self.dx * self.dy self.x = np.linspace(self.xmin, self.xmax, self.Nx) self.y = np.linspace(self.ymin, self.ymax, self.Ny) def set_lims(self, xlim, ylim): &quot;&quot;&quot;Set the min and max grid coordinates.&quot;&quot;&quot; self.__init__(xlim, ylim, self.size) def zeros(self): &quot;&quot;&quot;Create array of zeros with same size as the grid.&quot;&quot;&quot; return np.zeros((self.size)) def distribute(self, positions): &quot;&quot;&quot;Distribute points on the grid using the cloud-in-cell (CIC) method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. Returns - rho : ndarray, shape (Nx, Ny) Density at each grid point. &quot;&quot;&quot; # Compute area overlapping with 4 nearest neighbors ivals = np.floor((positions[:, 0] - self.xmin) / self.dx).astype(int) jvals = np.floor((positions[:, 1] - self.ymin) / self.dy).astype(int) ivals[ivals &gt; self.Nx - 2] = self.Nx - 2 jvals[jvals &gt; self.Ny - 2] = self.Ny - 2 x_i, x_ip1 = self.x[ivals], self.x[ivals + 1] y_j, y_jp1 = self.y[jvals], self.y[jvals + 1] _A1 = (positions[:, 0] - x_i) * (positions[:, 1] - y_j) _A2 = (x_ip1 - positions[:, 0]) * (positions[:, 1] - y_j) _A3 = (positions[:, 0] - x_i) * (y_jp1 - positions[:, 1]) _A4 = (x_ip1 - positions[:, 0]) * (y_jp1 - positions[:, 1]) # Distribute areas for each point rho = self.zeros() cdef double[:, :] rho_view = rho cdef int i, j for i, j, A1, A2, A3, A4 in zip(ivals, jvals, _A1, _A2, _A3, _A4): rho_view[i, j] += A4 rho_view[i + 1, j] += A3 rho_view[i, j + 1] += A2 rho_view[i + 1, j + 1] += A1 return rho / self.cell_area def interpolate(self, grid_vals, positions): &quot;&quot;&quot;Interpolate values from the grid using the CIC method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. grid_vals : ndarray, shape (n, 2) Scalar value at each coordinate point. Returns - int_vals : ndarray, shape (nparts,) Interpolated value at each position. &quot;&quot;&quot; int_func = RegularGridInterpolator((self.x, self.y), grid_vals) return int_func(positions) def gradient(self, grid_vals): &quot;&quot;&quot;Compute gradient using 2nd order centered differencing. Parameters - grid_vals : ndarray, shape (Nx, Ny) Scalar values at each grid point. neg : Bool If True, return the negative of the gradient. Returns - gradx, grady : ndarray, shape (Nx, Ny) The x and y gradient at each grid point. &quot;&quot;&quot; return np.gradient(grid_vals, self.dx, self.dy) . It should be mentioned that the field interpolation method should be the same as the charge deposition method; if this is not true, it is possible for a particle to exert a force on itself! Let&#39;s test the method on a gaussian distribution of 100,000 macroparticles in the $x$-$y$ plane, truncated at three standard devations. We&#39;ll choose the number of grid points to be $N_x = N_y = 64$. I call a DistGenerator class that I don&#39;t show here. . dg = DistGenerator() bunch = Bunch() bunch.fill(dg.generate(kind=&#39;gauss&#39;, nparts=int(1e5), cut=3.)) bunch.compute_extremum() grid = Grid(bunch.xlim, bunch.ylim, size=(64, 64)) rho = bunch.line_density * grid.distribute(bunch.positions) . Field solver . The workhorse in the simulation loop is the field solver. We need to solve Poisson&#39;s equation: . $$ left({ frac{ partial^2}{ partial x^2} + frac{ partial^2}{ partial y^2}} right) = - frac{ rho left(x, y right)}{ varepsilon_0}. tag{6}$$ . The discretized version of the equation reads . $$ frac{ phi_{i+1,j} + -2 phi_{i,j} + phi_{i-1,j}}{{ Delta_x}^2} + frac{ phi_{i,j+1} + -2 phi_{i,j} + phi_{i,j-1}}{{ Delta_y}^2} = - frac{ rho_{i,j}}{ varepsilon_0} tag{7}$$ . for a grid with spacing $ Delta_x$ and $ Delta_y$. There are multiple paths to a solution; we will focus on the method implemented in PyORBIT which utilizes the Fourier convolution theorem. Let&#39;s briefly go over this method. The potential from an infinite line charge at the origin with charge density $ lambda$ is . $$ phi( mathbf{x}) = - frac{ lambda}{2 pi varepsilon_0} ln{| mathbf{x}|} = - frac{ lambda}{2 pi varepsilon_0} int{ ln{| mathbf{x} - mathbf{q}|} delta( mathbf{q})d mathbf{q}}. tag{8}$$ . Note that $ mathbf{q}$ is just a dummy variable; usually a prime is used, but we already assigned physical meaning to $ mathbf{x}&#39;$. By letting $G( mathbf{x} - mathbf{q}) = - ln{| mathbf{x} - mathbf{q}|}$ and $ rho( mathbf{x}) = delta( mathbf{x})$, then up to a scaling factor we have . $$ phi( mathbf{x}) = int{G( mathbf{x} - mathbf{q}) rho( mathbf{q})d mathbf{q}} = G( mathbf{x}) * rho( mathbf{x}). tag{9}$$ . In this form the potential is a convolution (represented by $*$) of the charge density $ rho$ with $G$, which is called the Green&#39;s function. On the grid this will look like . $$ phi_{i, j} = sum_{k,l ne i,j}{G_{i-k, j-l} rho_{k, l}}. tag{11}$$ . This solves the problem in $O(N^2)$ time complexity for $N$ grid points. This is already much faster than a direct force calculation but could still get expensive for fine grids. We can speed things up by exploiting the convolution theorem, which says that the Fourier transform of a convolution of two functions is equal to the product of their Fourier transforms. The Fourier transform is defined by . $$ hat{ phi}( mathbf{k})= mathcal{F} left[ phi( mathbf{x}) right] = int_{- infty}^{ infty}{e^{- mathbf{k} cdot mathbf{x}} phi( mathbf{x}) d mathbf{x}}. tag{12}$$ . The convolution theorem then says $$ mathcal{F} left[ rho * G right] = mathcal{F} left[ rho right] cdot mathcal{F} left[G right]. tag{13}$$ . For the discrete equation this gives . $$ hat{ phi}_{n, m} = hat{ rho}_{n, m} hat{G}_{n, m}, tag{14}$$ . where the hat represents the discrete Fourier transform. With the FFT algorithm at our disposal, the time complexity can be reduced to $O left(N log N right)$. . There is a caveat to this method: to use the FFT algorithm, Eq. (11) must be a circular convolution, which means $G$ must be periodic. But the beam is in free space (we&#39;ve neglected any conducting boundary), so this is not true. We can make it true by doubling the grid size in each dimension. We then make $G$ a mirror reflection in the new quadrants so that it is periodic, and also set the charge density equal to zero in these regions. After running the method on this larger grid, the potential in the new quadrants will be unphysical; however, the potential in the original quadrant will be correct. There are also some tricks we can play to reduce the space complexity, and in the end doubling the grid size is not much of a price to pay for the gain in speed. The method is implemented in the PoissonSolver class. . from scipy.fft import fft2, ifft2 class PoissonSolver: &quot;&quot;&quot;Class to solve Poisson&#39;s equation on a 2D grid. Attributes - rho, phi, G : ndarray, shape (2*Nx, 2*Ny) Charge density (rho), potential (phi), and Green&#39;s function (G) at each grid point on a doubled grid. Only one quadrant (i &lt; Nx, j &lt; Ny) corresponds to to the real potential. &quot;&quot;&quot; def __init__(self, grid, sign=-1.): self.grid = grid new_shape = (2 * self.grid.Nx, 2 * self.grid.Ny) self.rho, self.G = np.zeros(new_shape), np.zeros(new_shape) self.phi = np.zeros(new_shape) def set_grid(self, grid): self.__init__(grid) def compute_greens_function(self, line_charge_density): &quot;&quot;&quot;Compute Green&#39;s function on doubled grid.&quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny Y, X = np.meshgrid(self.grid.x - self.grid.xmin, self.grid.y - self.grid.ymin) self.G[:Nx, :Ny] = np.log(X**2 + Y**2, out=np.zeros_like(X), where=(X + Y &gt; 0)) self.G *= -0.5 * line_charge_density / (2 * pi * epsilon_0) self.G[Nx:, :] = np.flip(self.G[:Nx, :], axis=0) self.G[:, Ny:] = np.flip(self.G[:, :Ny], axis=1) def get_potential(self, rho, line_charge_density): &quot;&quot;&quot;Compute the electric potential on the grid. Parameters - rho : ndarray, shape (Nx, Ny) Charge density at each grid point. line_charge_density : float Longitudinal charge density. Returns - phi : ndarray, shape (Nx, Ny) Electric potential at each grid point. &quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny self.rho[:Nx, :Ny] = rho self.compute_greens_function(line_charge_density) self.phi = ifft2(fft2(self.G) * fft2(self.rho)).real return self.phi[:Nx, :Ny] . Running the algorithm gives the following potential on the doubled grid: . solver = PoissonSolver(grid) phi = solver.get_potential(rho, bunch.line_charge_density) . We can then compute the electric field at every grid point using second-order centered differencing. This gives . $$(E_x)_{i,j} = - frac{ phi_{i+1,j} - phi_{i-1,j}}{2 Delta_x}, tag{15}$$ . $$(E_y)_{i,j} = - frac{ phi_{i,j+1} - phi_{i,j-1}}{2 Delta_y}. tag{16}$$ . Ex, Ey = grid.gradient(-phi) . Finally, the value of the electric field at each particle position can be interpolated from the grid. . Ex_int = grid.interpolate(Ex, bunch.positions) Ey_int = grid.interpolate(Ey, bunch.positions) . Particle mover . All we need to do in this step is integrate the equations of motion. A common method is leapfrog integration in which the position and velocity are integrated out of phase as follows: . $$ m left( frac{ mathbf{v}_{i+1/2} - mathbf{v}_{i-1/2}}{ Delta_t} right) = mathbf{F}( mathbf{x}_i),$$ . $$ frac{ mathbf{x}_{i+1} - mathbf{x}_i}{ Delta_t} = mathbf{v}_{i+1/2}$$ . . Credit: S. LundA different scheme must be used when velocity-dependent forces are present. This is a symplectic integrator, which means it conserves energy. It is also second-order accurate, meaning that its error is proportional to the square of the $ Delta_t$. Finally, it is time-reversible. The only complication is that, because the velocity and position are out of phase, we need to push the velocity back one half-step before starting the simulation, and push it one half-step forward when taking a measurement. . Putting it all together . Simulation loop . We have all the tools to implement the simulation loop. While $s &lt; s_{max}$ we: . Compute the charge density on the grid. | Find the electric potential on the grid. | Interpolate the electric field at the particle positions. | Update the particle positions. | We&#39;ll first create a History class which stores the beam moments or phase space coordinates. . class History: &quot;&quot;&quot;Class to store bunch data over time. Atributes moments : list Second-order bunch moments. Each element is ndarray of shape (10,). coords : list Bunch coordinate arrays. Each element is ndarray of shape (nparts, 4) moment_positions, coord_positions : list Positions corresponding to each element of `moments` or `coords`. &quot;&quot;&quot; def __init__(self, bunch, samples=None): self.X = bunch.X self.moments, self.coords = [], [] self.moment_positions, self.coord_positions = [], [] if samples is None or samples &gt;= bunch.nparts: self.idx = np.arange(bunch.nparts) else: self.idx = np.random.choice(bunch.nparts, samples, replace=False) def store_moments(self, s): Sigma = np.cov(self.X.T) self.moments.append(Sigma[np.triu_indices(4)]) self.moment_positions.append(s) def store_coords(self, s): self.coords.append(np.copy(self.X[self.idx, :])) self.coord_positions.append(s) def package(self, mm_mrad): self.moments = np.array(self.moments) self.coords = np.array(self.coords) if mm_mrad: self.moments *= 1e6 self.coords *= 1e3 . Now we&#39;ll create a Simulation class. . class Simulation: &quot;&quot;&quot;Class to simulate the evolution of a charged particle bunch. Attributes - bunch : Bunch: The bunch to track. length : float Total tracking distance [m]. step_size : float Distance between force calculations [m]. nsteps : float Total number of steps = int(length / ds). steps_performed : int Number of steps performed so far. s : float Current bunch position. positions : ndarray, shape (nsteps + 1,) Positions at which coordinates are updated. history : History object Object storing bunch data at each position. meas_every : dict or int Keys should be &#39;moments&#39; and &#39;coords&#39;. Values correspond to the number of simulations steps between storing these quantities. For example, `meas_every = {&#39;coords&#39;:4, &#39;moments&#39;:2}` will store the moments every 4 steps and the moments every other step. If an int is provided, this will be applied to both. Defaults to start and end of simulation. samples : int Number of bunch particles to store when measuring phase space coordinates. Defaults to the entire coordinate array. mm_mrad : bool If True, use units of mm-mrad. Otherwise use m-rad. ext_foc : callable Function returning external kx and ky at the current position such that u&#39;&#39; = -ku. Call signature is `kx, ky = ext_foc(s)`. &quot;&quot;&quot; def __init__(self, bunch, length, step_size, grid_size, meas_every={}, samples=None, mm_mrad=True, ext_foc=None): self.bunch = bunch self.length, self.ds = length, step_size self.nsteps = int(length / step_size) self.positions = np.linspace(0, length, self.nsteps + 1) self.grid = Grid(size=grid_size) self.solver = PoissonSolver(self.grid) self.fields = np.zeros((bunch.nparts, 2)) self.history = History(bunch, samples) self.ext_foc = ext_foc if type(meas_every) is int: meas_every = {&#39;moments&#39;: meas_every, &#39;coords&#39;:meas_every} meas_every.setdefault(&#39;moments&#39;, self.nsteps) meas_every.setdefault(&#39;coords&#39;, self.nsteps) for key in meas_every.keys(): if meas_every[key] is None: meas_every[key] = self.nsteps self.meas_every = (meas_every[&#39;moments&#39;], meas_every[&#39;coords&#39;]) self.mm_mrad = mm_mrad self.s, self.steps_performed = 0.0, 0 def set_grid(self): &quot;&quot;&quot;Determine grid limits.&quot;&quot;&quot; self.bunch.compute_extremum() self.grid.set_lims(self.bunch.xlim, self.bunch.ylim) self.solver.set_grid(self.grid) def compute_electric_field(self): &quot;&quot;&quot;Compute the self-generated electric field.&quot;&quot;&quot; self.set_grid() rho = self.grid.distribute(self.bunch.positions) rho *= self.bunch.line_charge_density * 4 # unknown origin phi = self.solver.get_potential(rho, self.bunch.line_charge_density) Ex, Ey = self.grid.gradient(-phi) self.fields[:, 0] = self.grid.interpolate(Ex, self.bunch.positions) self.fields[:, 1] = self.grid.interpolate(Ey, self.bunch.positions) def kick(self, ds): &quot;&quot;&quot;Update particle slopes.&quot;&quot;&quot; # Space charge dxp_ds = self.bunch.sc_factor * self.fields[:, 0] dyp_ds = self.bunch.sc_factor * self.fields[:, 1] # External forces if self.ext_foc is not None: kx, ky = self.ext_foc(self.s) dxp_ds -= kx * self.bunch.X[:, 0] dyp_ds -= ky * self.bunch.X[:, 2] self.bunch.X[:, 1] += dxp_ds * ds self.bunch.X[:, 3] += dyp_ds * ds def push(self, ds): &quot;&quot;&quot;Update particle positions.&quot;&quot;&quot; self.bunch.X[:, 0] += self.bunch.X[:, 1] * ds self.bunch.X[:, 2] += self.bunch.X[:, 3] * ds def store(self): &quot;&quot;&quot;Store bunch coordinates or statistics.&quot;&quot;&quot; store_moments = self.steps_performed % self.meas_every[0] == 0 store_coords = self.steps_performed % self.meas_every[1] == 0 if not (store_moments or store_coords): return Xp = np.copy(self.bunch.X[:, [1, 3]]) self.kick(+0.5 * self.ds) # sync positions/slopes if store_moments: self.history.store_moments(self.s) if store_coords: self.history.store_coords(self.s) self.bunch.X[:, [1, 3]] = Xp def run(self): &quot;&quot;&quot;Run the simulation.&quot;&quot;&quot; self.compute_electric_field() self.store() self.kick(-0.5 * self.ds) # desync positions/slopes for i in trange(self.nsteps): self.compute_electric_field() self.kick(self.ds) self.push(self.ds) self.s += self.ds self.steps_performed += 1 self.store() self.history.package(self.mm_mrad) . Demonstration . To demonstrate our method, we need some way of checking its accuracy. Real bunch measurements will be noisy and infrequent along the accelerator, so this could be a pain. Probably the way to go is to compare with other well-tested codes. Alternatively, there is an analytic benchmark available: the Kapchinskij-Vladimirskij (KV) distribution. Without going into any detail, the beam projects to a uniform density ellipse in the $x$-$y$ plane, and the space charge forces produced within this ellipse are linear (in general space charge forces are nonlinear). If we plug the KV distribution into the Vlasov equation, it can be seen that these forces will remain linear for all time if the external focusing forces are also linear. As a consequence, a set of self-consistent differential equations describing the ellipse boundary can be written down. This is a remarkable fact. For now I won&#39;t show these equations; I&#39;ll just integrate them behind the scenes and compare with the calculations. . Now for the simulation. We&#39;ll look at two situations: 1) a drift space and 2) a FODO lattice. 1) is a region in which there are no external fields, so only the bunch&#39;s electric field is present. 2) is one of the basic building blocks of an accelerator, consisting of a series of alternating focusing and defocusing quadrupoles. This was covered in a previous post. Some care must be taken in the choice of simulation parameters; we need a fine enough grid to resolve the hard edge of the beam, enough macroparticles per grid cell to collect good statistics. I found the accuracy did depend on the choices here, and I sort of just played around with the numbers until things looked okay. Below are the simulation parameters we will use. . nparts = int(1e5) bunch_length = 250.0 # [m], circumference of SNS accumulator ring intensity = 40e14 # 25 times that of SNS to show change over short distance step_size = 0.025 # [m] grid_size = (128, 128) . Let&#39;s first track through a 10 meter drift. We&#39;ll compare the KV distribution to the envelope calculation and also to the Gaussian distribution. . def create_bunches(intensity, bunch_length, nparts): bunches = {} for kind in [&#39;kv&#39;, &#39;gauss&#39;]: bunches[kind] = Bunch(intensity=intensity, length=bunch_length) bunches[kind].fill(dg.generate(kind, nparts, eps=25e-6, cut=3)) return bunches bunches = create_bunches(intensity, bunch_length, nparts) tracking_distance = 10.0 # [m] emittance = 25e-6 # [m*rad] meas_every = {&#39;moments&#39;: int(0.1 * tracking_distance / step_size), &#39;coords&#39;: 4} sims = {} for kind, bunch in bunches.items(): sims[kind] = Simulation(bunch, tracking_distance, step_size, grid_size, meas_every=meas_every, samples=10000) sims[kind].run() . 100%|██████████| 400/400 [00:44&lt;00:00, 9.03it/s] 100%|██████████| 400/400 [00:49&lt;00:00, 8.11it/s] . Notice that the beam expands even without space charge because there is a distribution of transverse particle velocities in the initial bunch. The effect of space charge, then, is to increase the rate of expansion. The accuracy seems fairly good. The runtime is also okay; the equivalent PyORBIT simulation is about 10 times faster (it directly calls C++ routines and doesn&#39;t store the bunch coordinates). Below shows the evolution of a sample of 10,000 macroparicles, with the KV distribution on the left and a Gaussian distribution on the right for comparison. . &lt;/input&gt; Once Loop Reflect Now we&#39;ll track through a FODO lattice. The external fields simply add a term proportional to $x$ or $y$ in Eq. (1), so that . $$ x&#39;&#39; + k_x(s)x = frac{q}{mv_s^2 gamma^3} E_x,$$ . and similar for $y$. These fields cause the beam to periodically breath in and out, expanding in one dimension while contracting in the other. We&#39;ll add a quadrupole every 2.5 meters in our 10 meter drift space. . The disagreement with the KV envelope model, although small, seems to grow with time, specifically in the horizontal direction. I&#39;m not totally sure why this is; it could be that we need more macroparticles, or a different grid spacing, or maybe there is a bug in my code. I don&#39;t have the the time or need to investigate further. It is at least somewhat accurate. Here again is the evolution of the bunch in the $x$-$y$ plane. . &lt;/input&gt; Once Loop Reflect Conclusion . This post implemented an electrostatic PIC simulation in Python. If I were to continue development of this code, the next step would be to consider the $ mathbf{v} times mathbf{B}$ rotation caused by magnetic fields. It would also be straightforward to extend the code to 3D. Finally, all the methods used here are applicable to gravitational simulations. Here are some helpful references: . USPAS course | Hockney &amp; Eastwood | Birdsall &amp; Langdon | .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/simulation/space%20charge/2021/02/22/PIC.html",
            "relUrl": "/physics/accelerators/simulation/space%20charge/2021/02/22/PIC.html",
            "date": " • Feb 22, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Coupled parametric oscillators",
            "content": "Introduction . A previous post examined the analytic solutions to the equation of motion describing a parametric oscillator, which is a harmonic oscillator whose physical properties are time-dependent (but not dependent on the state of the oscillator). This problem was motivated by describing the transverse oscillations of a particle in an accelerator. In this post, the treatment will be extended to a coupled parametric oscillator. Basically, we are trying to solve the following equation of motion: . $$x&#39;&#39; + k_{11}(s)x + k_{13}(s)y + k_{14}(s)y&#39;,$$ $$y&#39;&#39; + k_{33}(s)y + k_{31}(s)x + k_{32}(s)x&#39;,$$ . where the prime denotes differentiation with respect to $s$. We also assume that each of the $k_{ij}$ coefficients are periodic, so $k_{ij}(s + L) = k_{ij}(s)$ for some $L$. . Motivation . The previous post discussed dipole and quadrupole magnetic fields, which have the special property that their fields depend linearly on $x$ and $y$, and are also uncoupled. Of course there are many other configurations possible. First, consider a solenoid magnet: . . Credit: brilliant.org The field within the coils points in the longitudinal direction and is approximatly constant ($ mathbf{B}_{sol} = B_0 hat{s}$). Plugging this into the Lorentz force equation we find: . $$ dot{ mathbf{v}} = frac{q}{m} mathbf{v} times mathbf{B} = frac{qB_0}{m} left({v_y hat{x} - v_x hat{y}} right).$$ . This means the motion in $x$ depends on the velocity in $y$, and vice versa, so this will contribute to $k_{14}$ and $k_{32}$. Coupling can also be produced from transverse magnetic fields. We again write the multipole expansion of this field: . $$B_y + iB_x = B_{ref} sum_{n=1}^{ infty} left({B_n + iA_n} right) left( frac{x + iy}{R_{ref}} right)^{n-1}.$$ . . Credit: Jeff Holmes . There will be nonlinear coupling (terms proportional to $x^j y^k$, where $j,k &gt; 1$ when $n &gt; 2$, but we are interested in linear coupling. This occurs when the skew quadrupole term $A_2$ is nonzero, which is true anytime a quadrupole is tilted in the transverse plane. The field couples the motion in one plane to the displacement in the other, contributing to the $k_{13}$ and $k_{31}$ terms. . Approach . Let&#39;s review the approach we took in analyzing the 1D parametric oscillator. We wrote the solution in pseudo-harmonic form, with an amplitude and phase which depended on time. We then found that particles travel along the boundary of an ellipse in 2D phase space, the area of which is a constant of the motion (we will denote this area by $ epsilon_x$). To understand the motion, we just need to know the dimensions and orientation of this ellipse, for which we proposed the parameters $ alpha_x$ and $ beta_x$, as well as the location of the particle on the ellipse boundary, which is determined by the phase $ mu_x$. All the subscripts can be replaced by $y$ to handle the vertical motion. We also wrote a transfer matrix $ mathbf{M}$, which connects the initial and final phase space coordinates after tracking through one period, from the parameters in the following form: . $$ mathbf{M} = mathbf{V} mathbf{P} mathbf{V}^{-1},$$ . where $ mathbf{V}^{-1}$ is a function of $ alpha_x$ and $ beta_x$ and transforms the ellipse into a circle while preserving the area, and $ mathbf{P}$ is a rotation in phase space according to the phase advance $ mu_x$. Basically, $ mathbf{V}$ turns the parametric oscillator into a harmonic oscillator. . This is a very elegant way to describe the motion with a minimal set of parameters. The question is: can we do something similar for coupled motion, in which the phase space is 4D, not 2D? To start, let&#39;s track a particle in a lattice with a nonzero skew quadrupole coefficient, plotting its phase space coordinates at one position after every period. . &lt;/input&gt; Once Loop Reflect The particle traces interesting donut-like shapes in horizontal ($x$-$x&#39;$) and vertical ($y$-$y&#39;$) phase space instead of ellipses. Below shows the shapes after 1000 periods. . There is definitely more than one frequency present, which we see if we plot the $x$ and $y$ position vs period number and take the FFT. . This is typical of a coupled oscillator. Such systems are typically understood as the superposition of normal modes, each of which corresponds to a single frequency. For example, consider two masses connected with a spring. There are two possible ways for the masses to oscillate at the same frequency. The first is a breathing mode in which they move in opposite directions, and the second is a sloshing mode in which they move in the same direction. The motion is simply the sum of these two modes. We will try to do something similar for a coupled parameteric oscillator. . Solution . Transfer matrix eigenvectors . If the phase space coordinate vector $ mathbf{x} = (x, x&#39;, y, y&#39;)^T$ evolves according to . $$ mathbf{x} rightarrow mathbf{Mx},$$ . where $ rightarrow$ represents tracking through one period, it can be shown that $ mathbf{M}$ is symplectic due to the Hamiltonian mechanics of the system. A consequence of the symplecticity condition is that $ mathbf{M}$ is fully described by 10 numbers instead of 16. Our method examines the eigenvectors of $ mathbf{M}$: . $$ mathbf{Mv} = e^{-i mu} mathbf{v}.$$ . The symplecticity condition also causes the eigenvalues and eigenvectors come in two complex conjugate pairs; this gives $ mathbf{v}_1$, $ mathbf{v}_2$, $ mu_1$, $ mu_2$ and their complex conjugates. The seemingly complex motion seen in the last animation is greatly simplified when written in terms of the eigenvectors. We can write any cooridinate vector as a linear combination of the real and imaginary components of $ mathbf{v}_1$ and $ mathbf{v}_2$: . $$ mathbf{x} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i psi_1} + sqrt{ epsilon_2} mathbf{v}_2e^{-i psi_2} right).$$ . We&#39;ve introduced two initial amplitudes ($ epsilon_1$ and $ epsilon_2$) as well as two initial phases ($ psi_1$ and $ psi_2$). Applying the transfer matrix then simply tacks on a phase. Thus, what we are observing are the 2D projections of the real components of these eigenvectors as they rotate in the complex plane. . $$ mathbf{Mx} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i left( psi_1 + mu_1 right)} + sqrt{ epsilon_2} mathbf{v}_2e^{-i( psi_2 + mu_2)} right).$$ . Let&#39;s replay the animation, but this time draw a red arrow for $ mathbf{v}_1$ and a blue arrow for $ mathbf{v}_2$. We&#39;ve chosen $ epsilon_1 = 4 epsilon_2$ and $ psi_2 - psi_1 = pi/2$. . &lt;/input&gt; Once Loop Reflect That really simplifies things! Each eigenvector simply rotates at its frequency $ mu_l$. It also explains why the amplitude in the $x$-$x&#39;$ and $y$-$y&#39;$ planes trade back and forth: it is because the projections of the eigenvectors rotate at different frequencies, sometimes aligning and sometimes anti-aligning. Because of this, the previous invariants $ epsilon_x$ and $ epsilon_y$ are replaced by $ epsilon_1$ and $ epsilon_2$ as the invariants. It is helpful to think of a torus (shown below). The two amplitudes would determine the inner and outer radii of the torus, and the two phases determine the location of a particle on the surface. . . Credit: Wikipedia Parameterization of eigenvectors . We are now going to introduce a set of parameters for these eigenvectors, and in turn the transfer matrix. We already have two phases, so that leaves 8 parameters. Our strategy is to observe that each eigenvector traces an ellipse in both horizontal ($x$-$x&#39;$) and vertical ($y$-$y&#39;$) phase space. Then, we will simply assign an $ alpha$ function and $ beta$ function to each of these ellipses. So, for the ellipse traced by $ mathbf{v}_1$ in the $x$-$x&#39;$ plane, we have $ beta_{1x}$ and $ alpha_{1x}$, and then for the second eigenvector we have $ beta_{2x}$ and $ alpha_{2x}$. The same thing goes for the vertical dimension with $x$ replaced by $y$. . . The actual eigenvectors written in terms of the parameters are . $$ vec{v}_1 = begin{bmatrix} sqrt{ beta_{1x}} - frac{ alpha_{1x} + i(1-u)}{ sqrt{ beta_{1x}}} sqrt{ beta_{1y}}e^{i nu_1} - frac{ alpha_{1y} + iu}{ sqrt{ beta_{1y}}} e^{i nu_1} end{bmatrix}, quad vec{v}_2 = begin{bmatrix} sqrt{ beta_{2x}}e^{i nu_2} - frac{ alpha_{2x} + iu}{ sqrt{ beta_{2x}}}e^{i nu_2} sqrt{ beta_{2y}} - frac{ alpha_{2y} + i(1-u)}{ sqrt{ beta_{2y}}} end{bmatrix}$$ . So in addition to the phases $ mu_1$ and $ mu_2$ we have $ alpha_{1x}$, $ alpha_{2x}$, $ alpha_{1y}$, $ alpha_{2y}$, $ beta_{1x}$, $ beta_{2x}$, $ beta_{1y}$, and $ beta_{2y}$. That&#39;s pretty much it. There are a few other parameters we need to introduce to simplify the notation, but they are not independent. The first is $u$, which, as noted in the figure, determines the areas of the ellipses in one plane relative to the other. The second and third are $ nu_1$ and $ nu_2$, which are phase differences between the $x$ and $y$ components of the eigenvectors (in the animation they are either $0$ or $ pi$). I won&#39;t discuss these here. The last thing to note is that the parameters reduce to their 1D definitions when there is no coupling in the lattice. So we would have $ beta_{1x}, beta_{2y} rightarrow beta_{x}, beta_{y}$ and $ beta_{2x}, beta_{1y} rightarrow 0$, and similar for $ alpha$. The invariants and phase advances would also revert back to their original values: $ epsilon_{1,2} rightarrow epsilon_{x,y}$ and $ mu_{1,2} rightarrow mu_{x,y}$. . Floquet transformation . These eigenvectors can also be used to construct a transformation which removes both the variance in the focusing strength and the coupling between the planes, turning the coupled parametric oscillator into an uncoupled harmonic oscillator. In other words, we seek a matrix $ mathbf{V}$ such that . $$ mathbf{V^{-1} M V} = mathbf{P} = begin{bmatrix} cos{ mu_1} &amp; sin{ mu_1} &amp; 0 &amp; 0 - sin{ mu_1} &amp; cos{ mu_1} &amp; 0 &amp; 0 0 &amp; 0 &amp; cos{ mu_2} &amp; sin{ mu_2} 0 &amp; 0 &amp; - sin{ mu_2} &amp; cos{ mu_2} end{bmatrix} $$We can do this simply by rewriting the following equation (I haven&#39;t yet figured out how to number equations in Jupyter): . $$ mathbf{x} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i psi_1} + sqrt{ epsilon_2} mathbf{v}_2e^{-i psi_2} right)$$ . in matrix form as $ mathbf{x} = mathbf{V} mathbf{x}_n$ with . $$ mathbf{x}_n = begin{bmatrix} sqrt{ epsilon_1} cos{ psi_1} - sqrt{ epsilon_1} sin{ psi_1} sqrt{ epsilon_2} cos{ psi_2} - sqrt{ epsilon_2} sin{ psi_2} end{bmatrix} $$ $$ mathbf{V} = left[{Re( mathbf{v}_1), -Im( mathbf{v}_1), Re( mathbf{v}_2), -Im( mathbf{v}_2)} right]$$ . Let&#39;s observe the motion in these new coordinates $ mathbf{x}_n$. . &lt;/input&gt; Once Loop Reflect The motion is uncoupled after this transformation; i.e., particles move in a circle of area $ varepsilon_1$ in the $x_n$-$x_n&#39;$ plane at frequency $ mu_1$, and in a circle of area $ varepsilon_2$ in the $y_n$-$y_n&#39;$ plane at frequency $ mu_2$. . Conclusion . The method introduced here allows us to describe the evolution of a parametric oscillator using the minimum number of parameters. Our physical motivation was an accelerator lattice with linear, coupled forces, such as when skew quadrupole terms are present in the magnetic fields. There is no agreed upon method to do this among accelerator physicists, but I like (and know) this method the best, and have used it in my research. I&#39;ve left out many details which can be found in the paper by Lebedev and Bogacz. The paper by Ripken is also very helpful. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/2021/01/25/coupled_parametric_oscillators.html",
            "relUrl": "/physics/accelerators/2021/01/25/coupled_parametric_oscillators.html",
            "date": " • Jan 25, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Parametric oscillators",
            "content": "This post presents the solution to a general problem: what is the motion of a particle in one dimension (1D) in the presence of time-dependent linear forces? This amounts to solving the following equation of motion: . $$ frac{d^2x}{dt^2} + k(t)x = 0,$$ . where $k(t + T) = k(t)$ for some $T$. This is a parametric oscillator, a harmonic oscillator whose physical properties are not static. For example, the oscillations of a pendulum (in the small angle approximation) on the surface of a planet whose gravitational pull varies periodically would be described by the above equation. The solution to this equation was derived by George William Hill in 1886 to study lunar motion, and for this reason it is known as Hill&#39;s equation. It also finds application in areas such as condensed matter physics, quantum optics, and accelerator physics. After setting up the physical problem, we will examine the solutions and discuss their relevance to the last application, accelerator physics. . Problem motivation . Accelerator physics . Particle accelerators are machines which produce groups of charged particles (known as beams), increase their kinetic energy, and guide them to a target. These machines are invaluable to modern scientific research. The most famous examples are colliders, such as the LHC, in which two beams are smashed together to generate fundamental particles. A lesser known fact is that the fields of condensed matter physics, material science, chemistry, and biology also benefit tremendously from accelerators; this is due to the effectiveness of scattering experiments in which the deflection of a beam after colliding with a target is used to learn information about the target. The scattered beam is composed of neutrons in spallation neutron sources such as SNS, electrons in electron scattering facilities such as CEBAF, or photons in synchrotron light sources such as APS. In addition to scientific research, accelerators find use in medicine, particularly for cancer treatment, and also in various industrial applications. . . A large detector at an interaction point in the LHC. There are generally a few beam properties which are very important to experimentalists; in colliders it is the energy and luminosity, in spallation sources it is the intensity, and in light sources it is the brightness. There is thus a constant need to push these parameters to new regions. For example, below is the famous Livingston plot which shows the energy achieved by various machines over the past century. . . Note: vertical axis scale is beam energy needed to produce the center of mass energy by collision with a resting proton (credit: Rasmus Ischebeck). There are many physics issues associated with the optimization of these beam parameters. Accelerator physics is a field of applied physics which studies these issues. The task of the accelerator physicist is to understand, control, and measure the journey of the beam from its creation to its final destination. The difficulty of this task has grown over time; the improvement accelerator performance has brought with it a staggering increase in size and complexity. The construction and operation of modern accelerators generally requires years of planning, thousands of scientists and engineers, and hundreds of millions or even billions of dollars. Despite this complexity, the underlying physics principles are quite simple, and the single particle motion in one of these machines can be understood analytically if a few approximations are made. In the end we will arrive at Hill&#39;s equation. . How to build an accelerator . There are three basic tasks an accelerator has to accomplish. First, it must increase the beam energy (acceleration). Second, it must guide the beam along a predetermined path (steering). Third, it must ensure the beam particles remain close together (focusing). It is helpful to use a coordinate system in which the $s$ axis points along the design trajectory, and the $x$ and $y$ axes defined in the plane transverse to $s$. In this way the motion is broken up into transverse and longitudinal dynamics. . . How are these tasks accomplished? Well, particles are charged, and the force on a point charge in an electromagnetic field is given by . $$ mathbf{F} = q left({ mathbf{E} + mathbf{v} times mathbf{B}} right),$$ . where $q$ is the particle charge, $ mathbf{v}$ is the particle velocity, $ mathbf{E}$ is the electric field, and $ mathbf{B}$ is the magnetic field. An accelerator consists of a series of elements, each with their own $ mathbf{E}$ and $ mathbf{B}$; the collection of these elements is called a lattice. We need to determine which electric and magnetic fields to use. . The first task, acceleration, is not the focus of this post; I&#39;ll just mention the basic principles that are used. Acceleration cannot be done with $ mathbf{B}$ fields, since the force they produce is always perpendicular the motion. A simple method is to produce an electric field is to create voltage difference between two conductors, but there is a limit to the field strengths that can be produced in this way. One solution is to create a series of radio-frequency (RF) cavities, each with a time-varying voltage. The positions and lengths of these cavities are chosen so that particles are only within the cavities when the electric field points along the direction of motion, as shown by this fantastic animation from Wikipedia: . . The remaining tasks, steering and focusing, concern the motion in the transverse plane. $ mathbf{B}$ fields, not $ mathbf{E}$ fields, are used since their effect grows with increased particle velocity. Any transverse magnetic field $ mathbf{B} = (B_x, B_y)^T$ can be written using a multipole expansion . $$B_y + iB_x = B_{ref} sum_{n=1}^{ infty} left({B_n + iA_n} right) left( frac{x + iy}{R_{ref}} right)^{n-1}.$$ . $B_{ref}$ and $R_{ref}$ are a reference field strength and radius, respectively; just consider them to be constants. We then have the normal multiple coefficients $B_n$, and the skew multipole coefficients $A_n$. The field lines corresponding to the first few normal multipole coefficients are shown below. . . Credit: Jeff Holmes The dipole term is perfect for steering. The field is constant in magnitude and direction: . $$ mathbf{B}_{dipole} propto hat{y},$$ . producing a force which is proportional to the $x$ position: . $$ mathbf{F}_{dipole} propto - hat{x}.$$ . The quadrupole term is used for focusing. The field takes the following form: . $$ mathbf{B}_{quad} propto y hat{x} + x hat{y},$$ . with the resulting force: . $$ mathbf{F}_{quad} propto -x hat{x} + y hat{y}.$$ . The force from the quadrupole is focusing in the horizontal direction, but defocusing in the vertical direction; however, net focusing is still achieved by alternating the direction of the quadrupoles. This is analogous to a beam of light passing through a series of converging and diverging lenses. If the spacing and curvature of the lenses is correctly chosen, a net focusing can be achieved. . . Focusing (QF) and defocusing (QD) quadrupoles modeled as magnetic lenses. The forces which result from these fields are linear, meaning they are proportional the $x$ or $y$ but not $x^2$, $y^3$, etc., and they are uncoupled, meaning the dynamics in the $x$ and $y$ dimensions are independent. Now, we may ask, can we really produce a perfect dipole or quadrupole field? The answer is no. In reality there will always be higher order multipoles present in the field, but people work very hard to ensure these are much smaller than the desired multipole. This video shows a bit of the construction process for these magnets. . Linearized equation of motion . Making the above approximation of perfect dipole and quadrupole magnets, and ignoring all other elements in the machine, we arrive at the equation of motion for a single particle in the transverse plane: . $$x&#39;&#39; + k(s)x = 0,$$ . where $x&#39; = dx/ds$ and $k(s + L) = k(s)$ for some distance $L$. We could also write a similar equation for $y$. It is conventional to use the slope $x&#39;$ instead of the velocity; this allows us to talk about the position of the particle in the lattice instead of the amount of time which has passed. The period length $L$ could be the entire circumference of a circular machine, or could be a smaller repeated subsection. . Solution . Envelope function . The general solution to Hill&#39;s equation is given by . $$x(s) = sqrt{ epsilon} ,w(s) cos left({ mu(s) + delta} right).$$ . This introduces an amplitude $w(s) = w(s + L)$ which we call the envelope function, as well as a phase $ mu$, both of which depend on $s$. The constants $ epsilon$ and $ delta$ are determined by the initial conditions. Let&#39;s plot this trajectory in a FODO (focus-off-defocus-off) lattice, which consists of evenly spaced focusing and defocusing quadrupoles. Here is the focusing strength within the lattice (QF is the focusing quadrupole and QD is the defocusing quadrupole): . . For now we can think of the lattice as repeating itself forever in the $s$ direction. Each black line below is represents the trajectory for a different initial position and slope; although the individual trajectories look rather complicated, the envelope function has a very simple form. . . Phase space . The particle motion becomes much easier to interpret if we observe it in position-momentum space, aka phase space. The following animation shows the evolution of the particle phase space coordinates at a single position in the lattice. The position shown is $s = nL/4$, where $n$ is the period number, which corresponds to the midpoint between the focusing and defocusing quadrupoles. . . &lt;/input&gt; Once Loop Reflect We see that the particle jumps along the boundary of an ellipse in phase space. The shape and orientation of the ellipse will change if we look at a different position in the lattice, but its area will be the same. So, the motion is determined by the dimensions and oriention of this ellipse throughout the lattice, as well as the location of the paricle on the ellipse boundary. This motivates the definition of the so-called Twiss parameters, which were first introduced by Courant and Snyder in 1958: . $$ beta = w^2, quad alpha = - frac{1}{2} beta&#39;, quad gamma = frac{1 + alpha^2}{ beta}.$$ . The dimensions of the phase space ellipse are nicely described by these parameters: . . The maximum extent of the ellipse is determined by $ beta$ in the $x$ direction and $ gamma$ in the $y$ direction. $ alpha$ is proportional to the slope of the $ beta$ function, and so determines the tilt angle of the ellipse. The position of a particle on the ellipse is given by the phase $ mu$. Finally, the invariant of the motion corresponding to the ellipse area is constructed from the Twiss parameters as . $$ epsilon = beta {x&#39;}^2 + 2 alpha xx&#39; + gamma x^2$$ . for any $x$ and $x&#39;$. The $ beta$ functions and phase advances in both dimensions are extremely important to measure and control in a real machine. Here is an example of the horizontal and vertical $ beta$ functions in the SNS accumulator ring. . . Transfer matrices . A helpful tool to pair with the parameterization we just introduced is the transfer matrix, a matrix which connects the phase space coordinates at two different positions: . $$ begin{bmatrix} x x&#39; end{bmatrix}_{s + L} = mathbf{M} begin{bmatrix} x x&#39; end{bmatrix}_{s}$$ . The transfer matrix can be written as $ mathbf{M} = mathbf{V} mathbf{P} mathbf{V}^{-1}$, where . $$ mathbf{V} = frac{1}{ sqrt{ beta}} begin{bmatrix} beta &amp; 0 - alpha &amp; 1 end{bmatrix}$$ and $$ mathbf{P} = begin{bmatrix} cos mu &amp; sin mu - sin mu &amp; cos mu end{bmatrix} $$ . The effect of $ mathbf{V}^{-1}$ is to deform the phase space ellipse into a circle while preserving its area. $ mathbf{P}$ is then just a rotation in phase space, and $ mathbf{V}$ then transforms back into a tilted ellipse. This is illustrated below. . . $ mathbf{V}$ can be thought of as a time-dependent transformation which removes the variance in the focusing strength, turning the parametric oscillator into a simple harmonic oscillator. Often it is called the Floquet transformation. . Conclusion . We&#39;ve presented the solution to Hill&#39;s equation, which describes a parameteric oscillator. The equation pops up in multiple areas, but we focused on its application in accelerator physics, in which Hill&#39;s equation describes the transverse motion of a single particle in an accelerator with perfectly linear magnetic fields. . The solution is best understood geometrically: particles move around the surface of an ellipse in phase space, the area of which is an invariant of the motion. The dimensions and orientation of the ellipse are determined by $ alpha$ and $ beta$, and the location of the paricle on the ellipse boundary is determined by $ mu$. These parameters can be used to construct a time-dependent transformation ($ mathbf{V}$) which turns the parametric oscillator into a simple harmonic oscillator. . The next post will examine how this treatment can be extended to include coupling between the horizontal and vertical dimensions. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/oscillators/2021/01/21/parametric_oscillators.html",
            "relUrl": "/physics/accelerators/oscillators/2021/01/21/parametric_oscillators.html",
            "date": " • Jan 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am completing my PhD in physics from the University of Tennessee, Knoxille, working as part of the Accelerator Physics group at the Spallation Neutron Source. Previously, I graduated from Wheaton College with a BS in physics. . Publications . Computation of the matched envelope of the Danilov distribution, PRAB, 2021 | . Presentations . Computation of the matched envelope of the Danilov distribution, AP Group Meeting, SNS, 01.29.2021 | Parameterization of coupled motion, AP Group Meeting, SNS, 01.15.2021 | Thesis proposal, University of Tennessee, 09.30.2020 | .",
          "url": "https://austin-hoover.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://austin-hoover.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}