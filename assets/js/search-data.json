{
  
    
        "post0": {
            "title": "Authorship identification",
            "content": "In 2020 I was able to take a few courses in the grad computer science department; the first was Data Mining and the second was Intro to Machine Learning. I thought should write about a few of the things I learned. One application of machine learning which I focused on is the task of authorship identification, that is, to identify the author of an unlabeled document given a list of possible authors and some sample of each author&#39;s writing. To learn about this topic, I attempted to reproduce the results presented in a paper titled N-Gram Feature Selection for Authorship Identification which can be found here. In this post I&#39;ll first motivate the problem of authorship identification, then briefly introduce the relevant statistical methods, and finally summarize and implement the methods in the paper. . . Stylometry . Motivation: the Federalist Papers . The Federalist Papers are an important collection of 85 essays written by Hamilton, Madison, and Jay during 1787 and 1788. They were published under the alias &quot;Plubious&quot; at the time, and although it became well known that the three contributed to papers, the authorship of each individual paper was kept hidden for over a decade. It was actually in the interest Hamilton and Madison, both politicians, to keep the authorship a secret (they had both changed their position on a number of issues and didn&#39;t want their political opponents to use their old arguments against them). Days before his death, however, Hamilton allegedly wrote down who he believed to be the correct author of each essay, claiming over 60 for himself. Madison waited a number of years before publishing his own list, and in the end there were 12 essays to which both Madison and Hamilton claimed authorship. Many interesting details on the controversy can be found in a paper by Adair. . . Alexander Hamilton (left) and James Madison (right). Credit: Wikipedia. There are two ways we could go about resolving this dispute. The first approach is to analyze the actual content of the text. For example, perhaps an essay draws from a reference which only Madison was intimately familiar with, or perhaps an essay is similar to some previous work by Hamilton. This was done many times over the next 150 years, but perhaps the final word on the subject was by Adair in 1944, who concluded that Madison likely wrote all 12 essays. An alternative approach is to determine the authorship using only the words on the page, i.e., to analyze the style of the text. For example, maybe Madison used many more commas than Hamilton. The field of stylometry attempts to statistically quantify these stylistic differences. David Holmes writes the following about stylometry: . At its heart lies an assumption that authors have an unconscious aspect to their style, an aspect which cannot consciously be manipulated but which possesses features which are quantifiable and which may be distinctive. . I think this a valid assumption. The question is which features best characterize the author&#39;s style and which methods are best to use in the analysis of these features. Let&#39;s go back in time a bit to the first attempts at stylometry. . History . Thomas Mendenhall, a physicist, is considered the first to statistically analyze large literary texts. He presented the following interesting idea in an 1887 paper titled The Characteristic Curves of Composition: it is well-known that each element has a unique distribution of wavelengths in the light which it emits when it is heated; perhaps each author has a unique distribution of word lengths in the texts they have written. It&#39;s a really cool idea, and I highly recommend reading his original paper. Mendenhall tallied word lengths by hand for various books, usually in batches of 1000 words or so. Here is Fig. 2 from his paper which shows the characteristic curves for a few excerpts of Oliver Twist. . . Distribution of word lengths in &quot;Oliver Twist&quot;. Each curve is for a different sample of 1000 words. He showed that these curves are very interesting and that they do show similarities between different works by the same author. The use of these statistics for authorship identification was left for future work. . The next significant advance in the statistical analysis of text was made by Zipf in 1932. Zipf found an interesting relationship between an integer $k$ and the frequency $f(k)$ of the $k$th most frequent word. This is often called a rank-frequency relationship where $k$ is the rank. The scaling law can be written as . $$ f(k) propto k^{-1}. tag{1}$$ . The basic idea expressed by this power law is that short words are much more frequent than large words. Surprisingly, the law holds up very well, albeit not perfectly, for most texts. Why this is the case is still unknown; a comprehensive review of the current state of the law can be found here. The law also shows up in other situations such as national GDP: . . Zipf’s Law for National GDPs over time, which appear to be moving toward the prediction by Zipf&#39;s Law (red line). From Ref. [1]. Although not directly applicable to authorship identification, the success of Zipf&#39;s Law was very encouraging and led to a flurry of new mathematical models in the following years. Stylometry had a landmark case in the 1960&#39;s when researchers used the frequency distributions of short function words — words we don&#39;t think about too much like &quot;upon&quot; or &quot;therefore&quot; — to support Adair&#39;s conclusion that Madison wrote the 12 disputed Federalist Papers. . At the end of the day, however, models created in the spirit of Zipf&#39;s Law are probably doomed to fail. The &quot;true&quot; underlying model must be very complex due to its dependence on human psychology. There are now many algorithms available which instead build predictive models directly from data, and these can be readily applied to the problem of authorship identification. Here we focus on the use of the Support Vector Machine (SVM). . Support Vector Machine (SVM) . I include here the basic idea behind the SVM approach. There are, of course, a huge number of resources which go into the details [include the references]. I&#39;ll sort of follow the Wikipedia page since it has a nice short summary. . Maximum margin hyperplane . Consider a linear, binary classifier, i.e., a plane which splits the data into two classes. The equation for a plane in any number of dimensions is . $$ y( mathbf{x}) = mathbf{w}^T mathbf{x} + w_0 = 0 tag{2}. $$ . This plane is called the decision surface; points are assigned to class 1 if $y( mathbf{x}) &gt; 0$ or class 2 when $y( mathbf{x}) &lt; 0$. Suppose the data is linearly separable (able to be completely split in two) and that we&#39;ve found a plane which correctly splits the data. We could then scale the coordinates such that all points with $y( mathbf{x}) ge 1$ belong to class 1 and all points with $y( mathbf{x}) le -1$ belong to class 2. The separating plane then sits in the middle as in the following figure. . . Maximum margin separating plane. Credit: Wikipedia. Notice that the plane could be rotated while still correctly splitting the existing data; the SVM attempts to find the optimal plane by maximizing the orthogonal distance from the decision plane to the closest point. This is known as the margin, and it can be shown that it is inversely proportional to the magnitude of $ mathbf{w}$. Thus, the SVM tries to minimize $| mathbf{w}|^2$ subject to the constraint that all points are correctly categorized. New data is then assigned based on this optimal boundary. . Many datasets won&#39;t be linear separable. In this case there are two options for the SVM. The first is to add a penalty function in order to minimize the number of miscategorized points, so for N samples we minimize . $$ frac{1}{2}| mathbf{w}|^2 + C sum_{i=1}^{N}{ Theta left[{t_i y( mathbf{x}_i)} right]} . tag{3}$$ . where $t_i$ is the true class of point $i$, $ Theta$ is the Heaviside step function, and $C$ is a positive constant. Let&#39;s try this on some toy data. The Python package scikit-learn has a user-friendly interface for the SVM implementation in LIBLINEAR which we use here. . import numpy as np import pandas as pd from sklearn import svm import matplotlib.pyplot as plt import proplot as plot import plotly.graph_objects as go import seaborn as sns . # Create two overlapping Gaussian distributions np.random.seed(0) n = 200 X = np.vstack([np.random.normal(size=(n, 2), loc=[0, 0], scale=2.0), np.random.normal(size=(n, 2), loc=[5, 5], scale=2.5)]) y = n * [1] + n * [-1] # Plot the data fig, ax = plot.subplots() ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y) # Plot decision boundary for different values of C C_vals = [1, 100, 1000] line_x = np.array([min(X[:, 0]), max(X[:, 0])]) for C, ls in zip(C_vals, [&#39;solid&#39;, &#39;dashed&#39;, &#39;dotted&#39;]): clf = svm.LinearSVC(C=C) clf.fit(X, y) (w1, w2), w0 = clf.coef_[0], clf.intercept_ line_y = -(w1 / w2) * line_x - (w0 / w2) ax.plot(line_x, line_y, c=&#39;k&#39;, ls=ls) ax.legend(labels=[&#39;C = {}&#39;.format(C) for C in C_vals], ncols=1, loc=(1.02, 0)) ax.format(xticks=[], yticks=[], xlim=line_x); . /opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. warnings.warn(&#34;Liblinear failed to converge, increase &#34; /opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. warnings.warn(&#34;Liblinear failed to converge, increase &#34; . The three lines have significantly different angles, but all have around the same classification accuracy on the existing data. It can therefore be important to try at least a few different values of $C$, which determines the trade-off between correctly classifying all samples and maximizing the margin, and to observe the effect on the testing data classification accuracy. $C$ is known as a hyperparameter since it isn&#39;t changed by the algorithm during the training process, but is instead a property of the algorithm itself. . Kernel trick . In some cases the linear model is going to be bad; a frequently used example is following &quot;target&quot; dataset. . n = 400 r1 = np.sqrt(np.random.uniform(0.0, 0.2, size=(n,))) r2 = np.sqrt(np.random.uniform(0.5, 1.0, size=(n,))) t1 = np.random.uniform(0, 2*np.pi, size=(n,)) t2 = np.random.uniform(0, 2*np.pi, size=(n,)) X = np.vstack([np.vstack([r1*np.cos(t1), r1*np.sin(t1)]).T, np.vstack([r2*np.cos(t2), r2*np.sin(t2)]).T]) y = n * [1] + n * [-1] fig, ax = plot.subplots() ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y); . Obviously a line won&#39;t work; ideally we would draw a circle around the inner cluster to split the data. A really cool strategy is the kernel trick, which performs a transformation to a higher dimensional space in which the data is linearly separable. For example, consider the transformation . $$ (x_1, x_2) rightarrow (x_1^2, x_2^2, sqrt{2} x_1 x_2) . tag{4}$$ . x1, x2 = X.T u = x1**2 v = np.sqrt(2) * x1 * x2 w = x2**2 fig = go.Figure(data=go.Scatter3d(x=u, y=v, z=w, mode=&#39;markers&#39;, marker=dict(color=y, size=5, opacity=0.5))) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . We can now split the data with a 2D plane! This need not be the transformation used by the SVM — in fact, many transformations can be used — but it clearly demonstrates the idea. Once the decision boundary is found in this transormed space, it can be projected back onto our original space. A quick way to plot this boundary is to make a prediction on a grid of points which covers the entire plot range. . clf = svm.SVC(kernel=&#39;rbf&#39;) clf.fit(X, y) xx, yy = np.meshgrid(np.linspace(-1.1, 1.1, 300), np.linspace(-1.1, 1.1, 300)) Z = np.c_[xx.ravel(), yy.ravel()] y_pred = clf.predict(Z) zz = y_pred.reshape(xx.shape) fig, ax = plot.subplots() ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y) ax.contour(xx, yy, zz, alpha=0.05, cmap=&#39;binary&#39;); . There are, however, several advantages to the linear SVM. First, it is much faster to train, and second, the kernel trick may be unnecessary for high-dimensional data. As we&#39;ll see, the latter applies to the task of authorship identification. . Multi-class . The two-class problem can be extended to multi-class problems. [Overview of method.] . N-grams and feature selection methods . As I mentioned in the introduction, the paper I chose to follow to learn about this topic is called N-Gram Feature Selection for Authorship Identification. In short, the paper used n-gram frequencies (which I&#39;ll define shortly) as features in the classification task and examined how to minimize the number of features used by the classifier. Let&#39;s step through their method. . Data set description . The Reuters Corpus Volume 1 (RCV1) data set is a huge collection of news articles, each manually labeled by topic. Around 100,000 of these have known authors, and there are around 2000 different authors. A specific topic was chosen, and only authors who wrote at least one article which fell under this topic were considered. From this subset of authors, the top 50 in terms of number of articles written were chosen. 100 articles from each author were selected — 5000 in total — and these were evenly split into a training and testing set. The resulting corpus is a good challenge for authorship identification because 1) the genre of all the documents is the same, and 2) the authors write about similar topics. Hopefully this leaves the author&#39;s style as the primary distinguishing factor. Another challenge is that the documents are quite short. The data set can be downloaded here. The files are organized like this: . . The sklearn.feature_extraction.text module has some functions which will efficiently extract the n-grams from the documents; here I&#39;ll do it manually but keep the same approach as in that module. Let&#39;s first create two lists, texts_train and texts_test corresponding to the 2500 training and testing documents. Each document is represented as a single string with spaces and new lines replaced with underscores. We also keep track of the correct author name and class id for each document. . from os import listdir from os.path import join def load_file(filename): string = open(filename, &#39;r&#39;).read() for c in (&#39; &#39;, &#39; n&#39;): string = string.replace(c, &#39;_&#39;) return string def load_files(outer_path): texts, class_ids, class_names = [], [], [] for class_id, folder in enumerate(sorted(listdir(outer_path))): folder_path = join(outer_path, folder) for filename in listdir(folder_path): class_ids.append(class_id) class_names.append(folder) file = open(join(folder_path, filename), &#39;r&#39;) text = file.read() for c in (&#39; n&#39;, &#39; &#39;): text = text.replace(c, &#39;_&#39;) texts.append(text) file.close() return texts, class_ids, class_names texts_train, y_train, authors_train = load_files(&#39;reuters_data/train&#39;) texts_test, y_test, authors_test = load_files(&#39;reuters_data/test&#39;) . . N-grams . An obvious feature candidate is word frequency; a less obvious one is n-gram frequency. A character n-gram is a string of length n. For example, the string red_bike! has the following 3-grams: red, ed_, d_b, _bi, bik, ike, ke!. The idea behind this approach is that these shorter strings capture different aspects of style such as use of punctuation and certain prefixes/suffixes. . We&#39;ll first build a vocabulary of n-grams from the training data. Then we&#39;ll create a feature matrix $X$ using this vocabulary for the training and testing data, where $X_{ij}$ is the frequency of the jth n-gram in the ith document. It is also convenient to add an option to keep only the n-grams which are the most frequent across all documents. . class FeatureExtractor: def __init__(self, nmin=1, nmax=1): self.vocab = {} self.set_ngram_range(nmin, nmax) def set_ngram_range(self, nmin, nmax): self.nmin, self.nmax = nmin, nmax def get_ngrams(self, text): ngrams = [] for n in range(self.nmin, self.nmax + 1): for i in range(n, len(text) + 1): ngrams.append(text[i-n:i]) return ngrams def build_vocab(self, texts): self.vocab, indexer = {}, 0 for text in texts: for ngram in self.get_ngrams(text): if ngram not in self.vocab: self.vocab[ngram] = indexer indexer += 1 def count_ngrams(self, text): counts = {ngram: 0 for ngram in self.vocab.keys()} for ngram in self.get_ngrams(text): if ngram in self.vocab: counts[ngram] += 1 return list(counts.values()) def create_feature_matrix(self, texts, max_features=None): X = np.array([self.count_ngrams(text) for text in texts]) if max_features and max_features &lt; X.shape[1]: idx = np.flip(np.argsort(np.sum(X, axis=0))) X = X[:, idx] X = X[:, :max_features] self.vocab = {k:v for k,v in zip(self.vocab.keys(), idx)} return X . extractor = FeatureExtractor(nmin=3, nmax=3) extractor.build_vocab(texts_train) X_train = extractor.create_feature_matrix(texts_train) X_test = extractor.create_feature_matrix(texts_test) . Here is part of the array X_train: . . Notice that the matrix is very sparse. Now we&#39;re ready to train the SVM and make predictions on the testing data. . clf = svm.LinearSVC(C=1) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) . from sklearn.metrics import accuracy_score, confusion_matrix acc = accuracy_score(y_test, y_pred) cmat = confusion_matrix(y_test, y_pred) fig, ax = plt.subplots(figsize=(4.5, 3.5)) sns.heatmap(cmat, ax=ax, cmap=&#39;Viridis&#39;, cbar_kws=dict(label=&#39;Number of documents&#39;)) ax.set_title(&#39;Confusion Matrix (acc = {:.3f})&#39;.format(acc)) ax.set_ylabel(&#39;True class&#39;); ax.set_xlabel(&#39;Predicted class&#39;); . Feature selection . We achieved a 68% accuracy, which seems quite good (keep in mind that random guessing will lead to 2% accuracy, not 50%). Going forward we&#39;ll focus on the downside of the n-gram approach, that the dimensionality of the feature space can become very large. We extracted nearly 26,0000 3-grams from a relatively small corpus, and this number will grow quickly if we consider multiple values of $n$. For example, including 2, 3, 4, and 5 grams leads to 454,402 features. It is thus advantageous to select a subset of these features which are &quot;best&quot; in some sense. This process is often called feature selection. Beyond the practical benefit of reducing the dimensionality, it is interesting to explore how low the feature number can go while maintaining a good accuracy. . Standard univariate methods . The standard feature selection technique is to use statistical tests which compare one feature to another. The baseline we will use is mutual information (MI). MI measures the amount of information about one random variable that is gained by observing a different variable. In math this reads . $$ M(u, v) = []. tag{5}$$ . [Discuss mutual information (or maybe switch to information gain?).] . LocalMaxs algorithm . One downside of MI is that it doesn&#39;t consider combinations of features. For example, the words Dallas and Texas are very likely to be found next to each other and could probably be treated as a single multi-word unit (MWU). A technique has been developed to quantify this idea. For a character n-gram $C$, we define the glue $g(C)$ as the &quot;power&quot; holding the characters in $C$ together, i.e., the glue is higher if those characters are likely to be in the same string. . [Describe LocalMaxs algorithm]. . Results . Conclusion . I think what I&#39;ll try next is to identify artists from images of their paintings. .",
            "url": "https://austin-hoover.github.io/blog/stylometry/machine%20learning/natural%20language%20processing/2021/04/01/authorship_identification.html",
            "relUrl": "/stylometry/machine%20learning/natural%20language%20processing/2021/04/01/authorship_identification.html",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Nonlinear resonances",
            "content": "Most of us are familiar with the experience of pushing someone else on a playground swing. We intuitively know that we should sync our pushes with the swing oscillation frequency, which appears to be independent of the swing amplitude. This strategy employs the idea of a resonance, which is an increase of the oscillation amplitude of a system for certain driving frequencies. In this post we first review the mathematics of this simple example, then extend the machinery to the nonlinear dynamics in a particle accelerator. My goal here is to write down the main results which are relevant to accelerators in order to improve my own understanding of the topic. . . Linear resonances . Consider a mass on a spring which, if left alone, oscillates at freqency $ omega_0^2$. . . The equation of motion for $x$ is . $$ frac{d^2}{dt^2}{x} + omega_0^2x = 0. tag{1}$$ . Now consider a sinusoidal driving force $f(t) = f_0 cos( omega t)$ as well as a damping term: . $$ frac{d^2}{dt^2}{x} + b dot{x} + omega_0^2x = f_0 cos( omega t).$$ . After doing some work it can be shown that the gravitational, damping, and driving forces initially fight against each other, but in the end the driving force dominates and the position oscillates as . $$ x(t) = A cos( omega t - delta) tag{2}$$ . where . $$A^2 = frac{f_0^2}{( omega - omega_0)^2 + b omega^2}. tag{3}$$ . The figure below shows the squared amplitude as the driving frequency is varied. The maximum amplitude approaches infinity as the damping term goes to zero. . The next step is to consider what happens when the driving force is not a pure sine wave. We&#39;ll only consider periodic driving forces, and any periodic function can be written as a sum of sines and cosines of different frequencies. Assuming $f(t)$ is an even function so that we can drop the sine terms in the Fourier expansion, the equation of motion becomes . $$ ddot{x} + b dot{x} + omega_0^2 x = sum_{n=0}^{ infty} {f_n cos(n omega t)}. tag{4}$$ . The long-term solution is found by just adding up the solutions to each term in the sum: . $$x(t) = sum_{n = 0}^{ infty}{A_n cos{(n omega t - delta_n)}}, tag{5}$$ . where $A_n$ is given by Eq. (3) for the frequency $n omega$. The resonance condition will apply to each of these amplitudes individually, which means that a resonance could be excited if any component of the driving force is near the natural frequency. . Sources of nonlinearity . We&#39;re now going to apply these ideas to a particle accelerator. We&#39;ll assume small transverse oscillations, no acceleration, no deviation from the design momentum, and no particle-particle interactions. Under these assumptions, the transverse equation of motion of a particle with charge $q$ and momentum $p$ in a magnetic field $ mathbf{B} = (B_x, B_y)^T$ is . $$ x&#39;&#39; = - frac{q}{p} B_y(s), tag{6}$$ $$ y&#39;&#39; = + frac{q}{p} B_x(s). $$ . Remember that $x&#39; = dx/ds$, and $s$ is the position in accelerator (from now on we&#39;ll assume a circular accelerator or &quot;ring&quot; of circumference $L$). Any 2D magnetic field can be expanded as the following infinite sum: . $$B_y - iB_x = sum_{n=1}^{ infty} left({b_n - ia_n} right) left( frac{x + iy}{r_0} right)^{n-1}, tag{7}$$ . where $r_0$ is a constant. The $b_n$ and $a_n$ terms are called the multipole coefficients and skew multipole coefficients, respectively. The $n^{th}$ term in the expansion is the field produced by $2n$ symmetrically arranged magnetic poles. . . We can see that terms with $n &gt; 2$ introduce nonlinear powers of $x$ and $y$ on the right side of Eq. (6), while terms with $n le 2$ introduce linear or constant terms. One may ask why we are considering a general magnetic field when in reality we use only dipoles and quadrupoles. The answer is two-fold. First, the best we can do in a real magnet is to make the $n &gt; 2$ terms as small as possible; they aren&#39;t zero and we need to know how they affect the motion. Second, sextupoles (and sometimes even octopoles) can be introduced intentionally. Their primary use is to correct for the fact that not all beam particles have the same momentum. . . An example of a sextupole electromagnet. Credit: CERN. Perturbation analysis . The nonlinear terms in Eq. (6) eliminate any hope of an analytic solution. There are two options in situations such as these: 1) use a computer, or 2) use perturbation theory. The strategy of option 2 is to make approximations until an exact solution can be found, then to add in small nonlinear terms and see how the solution changes. The process can be repeated to solve the problem up to a certain order of accuracy. Usually this is infeasible beyond a few iterations, but it is a helpful tool for gaining intuition and interpreting numerical results. In particular, we&#39;ll be looking for regions where the particle may encounter a resonance. Without many details, let&#39;s try out the perturbation approach. Later on we&#39;ll use a computer and see if our analysis was accurate. . Floquet coordinates . The first step is to find an exact solution under some approximation. We&#39;ll neglect coupling by setting $y = 0$ and focus on one dimension to make things easier. Let&#39;s denote the linear focusing from the lattice by $k$, with all other terms in the field expansion folded into $ Delta B$ (there are still $n = 1$ and $n = 2$ terms in $ Delta B$, but they represent deviations from the design values). We&#39;re also assuming that these variables are normalized by the ratio $q / p$. This results in the equation of motion . $$ x&#39;&#39; + k(s)x = Delta B. tag{8}$$ . This is Hill&#39;s equation with a nonlinear driving term. The stable solution when $ Delta B = 0$ is . $$x(s) = sqrt{ epsilon beta(s)} cos left({ mu(s) + delta} right), tag{9}$$ . with the phase advance is given by . $$ mu(s) = int_{0}^{s}{ frac{ds}{ beta(s)}}. tag{10}$$ . These pseudo-harmonic oscillations are still a bit difficult to visualize, so it&#39;s helpful to perform the Floquet transformation which scales the $x$ coordinate as . $$x(s) rightarrow u(s) = frac{x(s)}{ sqrt{ beta_x(s)}}. tag{11}$$ . Furthermore, it is convenient to replace the $s$ coordinate with . $$ phi(s) = frac{1}{ nu_0} int_{0}^{C}{ frac{ds}{ beta_x(s)}}. tag{12}$$ . Here $ nu_0$ is the number of phase space oscillations per trip around the ring. As a result, the unperturbed equation of motion becomes (with $ dot{x} = dx / d phi$) . $$ ddot{u} + nu_0^2 u = 0. tag{13}$$ . But this is just a harmonic oscillator! The trajectory in phase space is a circle, and the particle revolves once around this circle for every turn around the ring. Finally, we can write $ Delta B$ as a power series in $u$ and derive the equation of motion in Floquet coordinates: . $$ ddot{u} + nu_0^2 u = - nu_0^2 beta^{3/2} Delta B = - nu_0^2 sum_{n=0}^{ infty} left({ beta^{ frac{n + 3}{2}} b_{n+1}} right) u^n. tag{14}$$ . Fourier expansion . The tools to analyze driven harmonic oscillators are now available to us. Similar to Eq. (4), each term on the right hand side can be Fourier expanded, the reason being that $ beta$ (the oscillation amplitude of the unperturbed motion) and $b_n$ (a multipole coefficient) depend only on the position in the ring, so of course they are periodic in $ phi$. Grouping these terms together and performing the expansion gives . $$ ddot{u} + nu_0^2 u = - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} C_{n,k} , u^n , e^{ik phi}. tag{16}$$ . We&#39;re now going to linearize this equation. This means plugging in $u = u_0 + delta u$, where $u_0$ is the unperturbed solution and $ delta_u$ is small, and discarding all higher powers of $ delta_u$. This gives . $$ ddot{ delta u} + nu_0^2 delta u approx - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} C_{n,k} , u_0^n , e^{ik phi} . tag{17}$$ . This equation tells us how the perturbation evolves with time — ideally it remains finite, but at a resonant condition it will grow without bound. The final step is to write $u_0^n$ in a managable form. There is this trick involving the binomial expansion: . $$ u_0^n propto cos^n( nu phi) = frac{1}{2^n} sum_{m=0}^{n} binom{n}{m} e^{i(n-2m) nu_0 phi}. tag{17}$$ . So, we finally arrive at . $$ ddot{ delta u} + nu_0^2 delta u approx - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} sum_{m=0}^{n} {n choose m} frac{C_{n,k}}{2^n} e^{i left[(n - 2m) nu_0 + k right] phi}. tag{18} $$ . There are a lot of indices floating around; $n$ is one less than the multipole coefficient of the magnetic field, $k$ is for the Fourier expansion, and $m$ is just a dummy index we used to binomially expand $u_0^2$. . Resonance diagram . Eq. (18) describes a driven harmonic oscillator like Eq. (5), so we can expect a resonance condition to occur when any of the frequency components of the driving force are close to the natural frequency $ nu_0$. In other words, a resonance could occur when . $$ (n - 2m) nu_0 + k = pm nu_0. tag{19}$$ . If you write out the different cases ($n$ = 0, 1, 2, ...), you&#39;ll find that dipole terms ($n = 0$) forbid integer tunes, quadrupole terms forbid 1/2 integer tunes, sextupole terms forbid 1/3 integer tunes, and so on. The same thing can be done for the vertical dimension. Once coupling is included between $x$ and $y$, we&#39;re lead to the definition of resonance lines: . $$ M_x nu_x + M_y nu_y = N, tag{20}$$ . where $M_x$, $M_y$, and $N$ are integers and $|M_x| + |M_y|$ is the order of the resonance. The reason for calling these resonance lines is because they define lines in $ nu_x$-$ nu_y$ space (tune space). You can click through the following animation to see how the lines fill up the space as higher order resonances are included. . &lt;/input&gt; Once Loop Reflect Resonance strengths tend to decrease with order number, so people generally don&#39;t consider anything beyond order 3 or 4. That being said, the machine tunes $ nu_x$ and $ nu_y$ need to be carefully chosen to avoid all low order resonance lines. Ideally all beam particles occupy this single point in tune space, but space charge complicates things by decreasing the tune by different amounts for each particle, possible placing them on one of the above resonance lines. This effect, called tune spread, places a fundamental limit on the number of particles in the beam. . Numerical exploration of the sextupole . Let&#39;s explore the behavior of a beam under the influence of a sextupole magnet. This section recreates some figures from the book Accelerator Physics by S. Y. Lee. The easiest way to do this is to approximate the multipole as an instantaneous change to the slope of the particle&#39;s trajectory. This is valid if the magnet isn&#39;t too long. . import numpy as np import numpy.linalg as la import scipy class Multipole: &quot;&quot;&quot;Class to apply multipole kick to particle. Adapted from PyORBIT tracking routine in `py-orbit/src/teapotbase.cc`. Attributes - order : int The order of the multipole term (dipole: 1, quadrupole: 2, ...). strength : float Integrated multipole strength [m^-(order - 1)]. skew : bool If True, rotate the magnet 45 degrees. &quot;&quot;&quot; def __init__(self, order, strength, skew=False): self.order, self.strength, self.skew = order, strength, skew def track_part(self, vec): &quot;&quot;&quot;Apply transverse kick to particle slopes. vec : ndarray, shape (4,) Transverse phase space coordinate vector [x, x&#39;, y, y&#39;]. &quot;&quot;&quot; x, xp, y, yp = vec k = self.strength / np.math.factorial(self.order - 1) zn = (x + 1j*y)**(self.order- 1) if self.skew: vec[1] += k * zn.imag vec[3] += k * zn.real else: vec[1] -= k * zn.real vec[3] += k * zn.imag return vec . order = 3 strength = 0.5 multipole = Multipole(order, strength, skew=False) . The situation we&#39;ll consider is a circular lattice which is made of linear uncoupled elements + one thin sextupole. We&#39;ll observe the beam at the location of the sextupole after each turn. A key result of the linear theory is that the details of the rest of the lattice are unimportant for this task. All we need to do is choose the Twiss parameters and tune in each dimension to form the transfer matrix, then we can just track using matrix multiplication. Recall that the transfer matrix is written as $ mathbf{M} = mathbf{V P V^{-1}}$, where $ mathbf{V} = mathbf{V}( alpha_x, alpha_y, beta_x, beta_y)$ performs the Floquet normalization and $ mathbf{P} = mathbf{P}( nu_x, nu_y)$ is a rotation in the $x$-$x&#39;$ and $y$-$y&#39;$ phase spaces by the angle $2 pi nu_x$ and $2 pi nu_y$, respectively. The following class implements this representation of the lattice. . def V_2D(alpha, beta): &quot;&quot;&quot;Floquet normalization matrix in 2D phase space.&quot;&quot;&quot; return np.array([[beta, 0.0], [-alpha, 1.0]]) / np.sqrt(beta) def P_2D(tune): &quot;&quot;&quot;Phase advance matrixmin 2D phase space.&quot;&quot;&quot; phase_advance = 2 * np.pi * tune cos, sin = np.cos(phase_advance), np.sin(phase_advance) return np.array([[cos, sin], [-sin, cos]]) class Lattice: &quot;&quot;&quot;Represents lattice as linear one-turn transfer matrix + multipole kick. Attributes - M : ndarray, shape (4, 4) Linear one-turn transfer matrix. aperture : float Radius of cylindical boundary containing the particles [m]. multipole : Multipole object Must implement `track_part(vec)`, where vec = [x, xp, y, yp]. &quot;&quot;&quot; def __init__(self, alpha_x, alpha_y, beta_x, beta_y, tune_x, tune_y, aperture=0.2): &quot;&quot;&quot;Constructor. Parameters - alpha_x, alpha_y, beta_x, beta_y : float Twiss parameters at the lattice entrance. tune_x, tune_y : float Number of phase space oscillations per turn. &quot;&quot;&quot; self.P = np.zeros((4, 4)) self.V = np.zeros((4, 4)) self.M = np.zeros((4, 4)) self.P[:2, :2] = P_2D(tune_x) self.P[2:, 2:] = P_2D(tune_y) self.V[:2, :2] = V_2D(alpha_x, beta_x) self.V[2:, 2:] = V_2D(alpha_y, beta_y) self.M = la.multi_dot([self.V, self.P, la.inv(self.V)]) self.aperture = aperture self.multipole = None def add_multipole(self, multipole): self.multipole = multipole def track_part(self, vec): &quot;&quot;&quot;Track a single particle through the lattice. vec : ndarray, shape (4,) Transverse phase space coordinate vector [x, x&#39;, y, y&#39;]. &quot;&quot;&quot; vec = np.matmul(self.M, vec) if self.multipole is not None: vec = self.multipole.track_part(vec) return vec def track_bunch(self, X): &quot;&quot;&quot;Track a particle bunch through the lattice. X : ndarray, shape (nparts, 4) Transverse phase space coordinate array. &quot;&quot;&quot; X = np.apply_along_axis(self.track_part, 1, X) return self.collimate(X) def collimate(self, X): &quot;&quot;&quot;Delete particles outside aperture.&quot;&quot;&quot; radii = np.sqrt(X[:, 0]**2 + X[:, 2]**2) return np.delete(X, np.where(radii &gt; self.aperture), axis=0) def get_matched_bunch(self, nparts=2000, emittance=10e-6, cut=3.0): &quot;&quot;&quot;Generate truncated Gaussian distribution matched to the lattice.&quot;&quot;&quot; X = scipy.stats.truncnorm.rvs(a=4*[-cut], b=4*[cut], size=(nparts, 4)) A = np.sqrt(emittance) * np.identity(4) V = self.V X = np.apply_along_axis(lambda vec: np.matmul(A, vec), 1, X) X = np.apply_along_axis(lambda vec: np.matmul(V, vec), 1, X) return X . 1/3 integer resonance . We focus first on the 1/3 integer resonance. Below, a particle is tracked over 100 turns starting from few different initial amplitudes. We set $y = y&#39; = 0$ in all cases. The $x$-$x&#39;$ trajectories should be upright ellipses in the absence of nonlinear elements. Some helper functions are defined in the collapsed cell. . # Define Twiss parameters at the observation point alpha_x = alpha_y = 0.0 beta_x = beta_y = 20.0 def create_lattice(tune_x, tune_y, multipole=None): lattice = Lattice(alpha_x, alpha_y, beta_x, beta_y, tune_x, tune_y) lattice.add_multipole(multipole) return lattice def get_traj(lattice, emittance, nturns=1): &quot;&quot;&quot;Return array of shape (nturns, 4) of tracked single particle coordinates. The vertical coordinate and slope are set to zero. &quot;&quot;&quot; X = np.array([[np.sqrt(emittance * beta_x), 0, 0, 0]]) tracked_vec = [X[0]] for _ in range(nturns): X = lattice.track_bunch(X) if X.shape[0] == 0: # particle was deleted break tracked_vec.append(X[0]) return 1000 * np.array(tracked_vec) # convert from m to mm def compare_traj(tunes_x, tune_y, emittances, nturns=1, multipole=None, limits=(45, 2.5), **kws): &quot;&quot;&quot;Compare trajectories w/ different emittances as horizontal tune is scaled.&quot;&quot;&quot; kws.setdefault(&#39;s&#39;, 1) kws.setdefault(&#39;c&#39;, &#39;pink8&#39;) fig, axes = plot.subplots(nrows=2, ncols=3, figsize=(7.25, 4)) xlim, ylim = (-limits[0], limits[0]), (-limits[1], limits[1]) axes.format(xlabel=&quot;x [mm]&quot;, ylabel=&quot;x&#39; [mrad]&quot;, xlim=xlim, ylim=ylim) for ax, tune_x in zip(axes, tunes_x): lattice = create_lattice(tune_x, tune_y, multipole) for emittance in emittances: tracked_vec = get_traj(lattice, emittance, nturns) ax.scatter(tracked_vec[:, 0], tracked_vec[:, 1], **kws) ax.annotate(r&#39;$ nu_x = {:.3f}$&#39;.format(tune_x), xy=(0.7, 0.9), xycoords=&#39;axes fraction&#39;, bbox=dict(fc=&#39;w&#39;, ec=&#39;k&#39;)) return axes def track_bunch(X, lattice, nturns=1): &quot;&quot;&quot;Track and return list of coordinate array after each turn. Also return the fraction of particles which were lost (exceeded aperture) at each frame.&quot;&quot;&quot; coords, nparts, frac_lost = [X], X.shape[0], [0.0] for _ in range(nturns): X = lattice.track_bunch(X) coords.append(X) frac_lost.append(1 - X.shape[0] / nparts) return [1000*X for X in coords], frac_lost def animate_phase_space(coords, frac_lost=None, limits=(55, 5)): &quot;&quot;&quot;Create animation of turn-by-turn x-x&#39; and y-y&#39; distributions.&quot;&quot;&quot; fig, axes = plot.subplots(ncols=2, figsize=(5, 2.5), wspace = 0.75, sharey=False, sharex=False) xlim, ylim = (-limits[0], limits[0]), (-limits[1], limits[1]) axes.format(xlim=xlim, ylim=ylim) axes[0].format(xlabel=&quot;x [mm]&quot;, ylabel=&quot;x&#39; [mrad]&quot;) axes[1].format(xlabel=&quot;y [mm]&quot;, ylabel=&quot;y&#39; [mrad]&quot;) myplt.despine(axes) plt.close() kws = dict(marker=&#39;.&#39;, c=&#39;steelblue&#39;, ms=3, lw=0, markeredgewidth=0, fillstyle=&#39;full&#39;) line0, = axes[0].plot([], [], **kws) line1, = axes[1].plot([], [], **kws) def update(t): x, xp, y, yp = coords[t].T line0.set_data(x, xp) line1.set_data(y, yp) axes[0].set_title(&#39;Turn {}&#39;.format(t)) if frac_lost: axes[1].set_title(&#39;Frac. lost = {:.3f}&#39;.format(frac_lost[t])) return animation.FuncAnimation(fig, update, frames=len(coords)) . . tunes_x = np.linspace(0.61, 0.66, 6) tune_y = 0.518 emittances = 1e-6 * np.array([10, 30, 60, 90]) nturns = 100 axes = compare_traj(tunes_x, tune_y, emittances, nturns) axes.format(suptitle=&#39;Linear lattice&#39;) . Now turn on the sextupole magnet. . axes = compare_traj(tunes_x, tune_y, emittances, nturns, multipole) axes.format(suptitle=&#39;Linear lattice + sextupole&#39;) . The initially elliptical orbits are morphed into a triangular shape as the tune approaches the resonance condition, and some of the larger orbits become unstable. It turns out that by looking at the Hamiltonian you can find a triangular region defining a separatrix between stable and unstable motion. Particles inside the triangle will oscillate forever, particles at the corner of the triangle are at unstable equilibrium points, and particles outside the triangle will eventually stream outward from the corners. This is easier to see by tracking a bunch of particles. The interesting stuff will be in the horizontal plane, but I&#39;ll plot the vertical plane as well for comparison. . lattice = create_lattice(0.66, tune_y, multipole) X = lattice.get_matched_bunch() coords, frac_lost = track_bunch(X, lattice, nturns=50) animate_phase_space(coords, frac_lost) . &lt;/input&gt; Once Loop Reflect The triangular region of stability is clearly visible at the end of 50 turns. Interestingly, the third order resonance can be used to extract a beam from an accelerator at a much slower rate than normal. To do this, the strength and spacing of sextupole magnets must be carefully chosen to control the shape and orientation of the stability triangle, then tune is slowly moved closer to the 1/3 integer resonance value. The result is that the triangle shrinks as the stable phase space area decreases, and that more and more particles will find themselves in the unstable area and eventually stream out along the vertices. . Integer resonance . The sextupole should also excite the integer resonance. . compare_traj(np.linspace(0.96, 0.976, 6), tune_y, emittances, nturns, multipole, limits=(60, 2.5)); . lattice = create_lattice(0.99, 0.18, multipole) animate_phase_space(*track_bunch(X, lattice, nturns=75)) . &lt;/input&gt; Once Loop Reflect Cool pattern! The separatrix is now shaped like a tear drop. It looks like it&#39;s evolving more slowly because the tune is close to an integer, so the particles almost return to the same location in phase space after a turn. . Higher order resonances . There are also higher order resonances which a sextupole can drive. You can actually find fourth and fifth order resonances if you perform perturbation theory up to second order (at least that&#39;s what I&#39;m told in a textbook... I&#39;d like to avoid carrying out such a procedure). Do these show up using our mapping equations? They are expected to be weaker, so we&#39;ll double the sextupole strength. . emittances = 1e-6 * np.array([1, 2, 7, 10, 25, 50, 100, 150, 200, 250, 350]) compare_traj(np.linspace(0.7496, 0.798, 6), 0.23, emittances, 1000, Multipole(3, 1.0), limits=(150, 6), s=0.1); . These are really interesting plots. The tune near 0.75 (it&#39;s actually 0.7496) is exciting a fourth order resonance, while the tune near 0.8 is exciting a fifth order resonance. In all the plots, the low amplitude orbits are stable ellipses. We then see the behavior change as the amplitude is increased, with the particle jumping between distinct &quot;islands&quot;. Eventually the trajectories once again form closed loops, but in deformed shapes. The motion is unstable at even larger amplitudes. Understanding exactly why the the plots look like they do would take more work. . Conclusion . This post outlined the theory of nonlinear resonances driven by magnetic multipoles. The effect of a sextupole-driven resonance on the phase space trajectory was then examined using mapping equations. I can&#39;t say that I have a great grasp of this huge topic, but taking the time to just write down the equations really helped to make it less mysterious to me. Eq. (20) is referenced all the time in accelerator physics; for example, I talked about it in my thesis proposal. During that proposal, one of my commitee members asked what was actually going on at the resonance condition. I didn&#39;t really know, so I said something like &quot;well... that&#39;s just how it is&quot;. Luckily, one of the other committee members gave a succinct answer about how this is the result of classical perturbation analysis in the presence of nonlinear driving terms. Since then I have been wanting to be able to at least sketch how one would arrive at this equation, so this post wasn&#39;t a total waste of time. Here are a number of helpful references: . Lectures S. Lund, Transverse Particle Resonances with Application to Circular Accelerators | E. Prebys, Resonances and Coupling | . | Textbooks D. Edwards and M. Syphers, An introduction to the Physics of High Energy Accelerators | H. Wiedemann, Particle Accelerator Physics | S. Y. Lee, Accelerator Physics | L. Reichl, The Transition to Chaos — Conservative Classical Systems and Quantum Manifestations | J. Taylor, Classical Mechanics | H. Goldstein, Classical Mechanics | . | .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/nonlinear%20systems/resonances/2021/03/28/nonlinear_resonances.html",
            "relUrl": "/physics/accelerators/nonlinear%20systems/resonances/2021/03/28/nonlinear_resonances.html",
            "date": " • Mar 28, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Particle-in-cell simulation",
            "content": "Many simulation codes exist for beam physics (one example is PyORBIT). A key component of the these simulations is the inclusion of the electromagnetic interactions between particles in the beam, also known as space charge forces. One way to compute space charge forces is the particle-in-cell (PIC) method. This post implements the PIC method in Python. . Theoretical model . We&#39;ll use bunch to refer to a group of particles in three-dimensional (3D) space, and we&#39;ll use a local cartesian coordinate system whose origin moves with the center of the bunch as shown below: . . The $s$ coordinate specifies the position of the bunch in the accelerator, and the path can be curved. Now for a few assumptions and approximations. First, assume all particles in the bunch move at a constant velocity $ beta c$, where $c$ is the speed of light. We then make the paraxial approximation. It&#39;s conventional to use the slope $x&#39; = dx/ds$ instead of the velocity, and the paraxial approximation assumes this slope is very small. Usually we report this slope in milliradians since $tan theta approx theta$ for small angles. Next we assume that the transverse ($x$-$y$) size of the bunch varies slowly along the $s$ axis. If this is true and we look at the electric field in a transverse slice of the bunch, there won&#39;t be much difference between the true field and the field of an infinitely long, uniform density cylinder. Our focus will be on the transverse dynamics of such a slice, so we&#39;ll treat each &quot;particle&quot; as an infinite line of charge. The figure below illustrates this approximation. . . Credit: G. Franchetti Another approximation is to neglect any magnetic fields generated by the beam, which is again valid if the transverse velocities are very small relative to $ beta c$. All this being said, the equations of motion without any external forces, i.e., in free space, can be written as . $$ mathbf{x}&#39;&#39; = frac{q}{mc^2 beta^2 gamma^3} mathbf{E}, tag{1}$$ . where $ mathbf{x} = [x, y]^T$ is the coordinate vector, $ mathbf{E} = [E_x, E_y]^T$ is the self-generated electric field, $m$ is the particle mass, and $ gamma = left({1 - beta^2} right)^{-1/2}$. Let&#39;s first address the factor $ gamma^{-3}$ in the equation of motion, which means that the space charge force goes to zero as the velocity approaches the speed of light. This is because parallel moving charges generate an attractive magnetic force which grows with velocity, completely cancelling the electric force in the limit $v rightarrow c$. . . Credit: OpenStax University PhysicsOne may ask: what about the rest frame in which there is no magnetic field? But special relativity says that electrogmagnetic fields change with reference frame. Using the transformations defined here, you can quickly prove that . $$ mathbf{E}_{lab} = frac{ mathbf{E}_{rest}}{ gamma}. tag{2}$$ . This inverse relationship between velocity and the space charge force has real-life consequences. It tells us that space charge is important if 1) the beam is very intense, meaning there are many particles in a small area, or 2) the beam is very energetic, meaning it is moving extremely fast. For example, space charge can usually be ignored in electron beams, which move near the speed of light for very modest energies due to their tiny mass, but is significant in high-intensity, low-energy hadron accelerators such as FRIB, SNS, and ESS. . We should now address the difficulty in determining the evolution of this system: the force on a particle in an $n$-particle bunch depends on the positions of the other $n - 1$ particles. The approach of statistical mechanics to this problem is to introduce a distribution function $f( mathbf{x}, mathbf{x}&#39;, t)$ which gives the number of particles in an infinitesimal volume of phase space. The Vlasov-Poisson system of equations determines the evolution of $f$ as long as we ignore collisions between particles: . $$ frac{ partial{f}}{ partial{s}} + mathbf{x}&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}}} + mathbf{x}&#39;&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}&#39;}}. tag{3}$$ . We know $ mathbf{x&#39;&#39;}$ from Eq. (1). The electric field is obtained from Poisson&#39;s equation: . $$ nabla cdot mathbf{E} = - nabla^2 phi = frac{ rho}{ varepsilon_0}. tag{4}$$ . Finally, the transverse charge density $ rho$ is determined by . $$ rho = q int{f dx&#39;dy&#39;}. tag{5}$$ . Although these equations are easy to write down, they are generally impossible to solve analytically. We need to turn to a computer for help. . Computational method . The Vlasov equation could be solved directly, but this is difficult, especially in 2D or 3D. On the other end of the spectrum, the notion of a fluid in phase space could be abandoned and each particle could be tracked individually, computing the forces using direct sums. But this is infeasible with current hardware; the time complexity would by $O(n^2)$, where $n$ is the number of particles, and $n$ may be on the order of $10^{14}$. The particle-in-cell (PIC) method is a sort of combination of these two approaches. The idea is to track a group of macroparticles according to Eq. (1), each of which represents a large number of real particles. The fields, however, are solved from Eq. (4). The key step is transforming back and forth between a discrete and continuous representation of the bunch. The simulation loop for the PIC method is shown below. . . In the next sections I will discuss each of these steps and implement them in Python. Here are all the imports needed to run the code. . import numpy as np from scipy.interpolate import RegularGridInterpolator from scipy.fft import fft2, ifft2 from scipy.integrate import odeint from scipy.stats import truncnorm import Cython %load_ext cython . Let&#39;s first create a Bunch class, which is a simple container for the bunch coordinates. . class Bunch: &quot;&quot;&quot;Container for 2D distribution of positive elementary charges. Attributes - intensity : float Number of physical particles in the bunch. length : float Length of the bunch [m]. mass, kin_energy : float Mass [GeV/c^2], charge [C], and kinetic energy [GeV] per particle. nparts : float Number of macroparticles in the bunch. X : ndarray, shape (nparts, 4) Array of particle coordinates. Columns are [x, x&#39;, y, y&#39;]. Units are meters and radians. positions : ndarray, shape (nparts, 2): Just the x and y positions (for convenience). &quot;&quot;&quot; def __init__(self, intensity=1e14, length=250., mass=0.938, kin_energy=1.0): self.intensity, self.length = intensity, length self.mass, self.kin_energy = mass, kin_energy self.gamma = 1 + (kin_energy / mass) # Lorentz factor self.beta = np.sqrt(1 - (1 / self.gamma)**2) # v/c r0 = 1.53469e-18 # classical proton radius [m] self.perveance = 2 * r0 * intensity / (length * self.beta**2 * self.gamma**3) self.nparts = 0 self.compute_macrosize() self.X, self.positions = None, None def compute_macrosize(self): &quot;&quot;&quot;Update the macrosize and macrocharge.&quot;&quot;&quot; self.macrosize = self.intensity // self.nparts if self.nparts &gt; 0 else 0 def fill(self, X): &quot;&quot;&quot;Fill with particles.&quot;&quot;&quot; self.X = X if self.X is None else np.vstack([self.X, X]) self.positions = self.X[:, [0, 2]] self.nparts = self.X.shape[0] self.compute_macrosize() def compute_extremum(self): &quot;&quot;&quot;Get extreme x and y coorinates.&quot;&quot;&quot; self.xmin, self.ymin = np.min(self.positions, axis=0) self.xmax, self.ymax = np.max(self.positions, axis=0) self.xlim, self.ylim = (self.xmin, self.xmax), (self.ymin, self.ymax) . Weighting . Starting from a group of macroparticles, we need to produce a charge density $ rho_{i,j}$ on a grid. The most simple approach is the nearest grid point (NGP) method, which, as the name suggests, assigns the full particle charge to the closest grid point. This is commonly called zero-order weighting; although it is very fast and easy to implement, it is not commonly used because it can lead to significant noise. A better method called cloud-in-cell (CIC) treats each particle as a rectangular, uniform density cloud of charge with dimensions equal to the grid spacing. A fractional part of the charge is assigned based on the fraction of the cloud overlapping with a given cell. This can be thought of as first-order weighting. To get a sense of what these methods are doing (in 1D), we can slide a particle across a cell and plot the resulting density of the cell at each position, thus giving an effective particle shape. . The NGP method leads to a discontinuous boundary while the CIC method leads to a continous boundary (but discontinous derivative). There are also higher order methods which lead to a smooth boundary, but I don&#39;t cover those here. . We also need to perform the inverse operation: given the electric field at each grid point, interpolate the value at each particle position. The same method applies here. NGP just uses the electric field at the nearest grid point, while CIC weights the four nearest grid points. The following Grid class implements the CIC method. Notice that Cython is used in the for-loop in the distribute method. I couldn&#39;t figure out a way to perform this operation with the loop, and in pure Python it took about 90% of the runtime for a single simulation step. Using Cython gave a significant performance boost. . %%cython import numpy as np from scipy.interpolate import RegularGridInterpolator class Grid: &quot;&quot;&quot;Class for 2D grid. Attributes - xmin, ymin, xmax, ymax : float Minimum and maximum coordinates. Nx, Ny : int Number of grid points. dx, dy : int Spacing between grid points. x, y : ndarray, shape (Nx,) or (Ny,) Positions of each grid point. cell_area : float Area of each cell. &quot;&quot;&quot; def __init__(self, xlim=(-1, 1), ylim=(-1, 1), size=(64, 64)): self.xlim, self.ylim = xlim, ylim (self.xmin, self.xmax), (self.ymin, self.ymax) = xlim, ylim self.size = size self.Nx, self.Ny = size self.dx = (self.xmax - self.xmin) / (self.Nx - 1) self.dy = (self.ymax - self.ymin) / (self.Ny - 1) self.cell_area = self.dx * self.dy self.x = np.linspace(self.xmin, self.xmax, self.Nx) self.y = np.linspace(self.ymin, self.ymax, self.Ny) def set_lims(self, xlim, ylim): &quot;&quot;&quot;Set the min and max grid coordinates.&quot;&quot;&quot; self.__init__(xlim, ylim, self.size) def zeros(self): &quot;&quot;&quot;Create array of zeros with same size as the grid.&quot;&quot;&quot; return np.zeros((self.size)) def distribute(self, positions): &quot;&quot;&quot;Distribute points on the grid using the cloud-in-cell (CIC) method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. Returns - rho : ndarray, shape (Nx, Ny) The value rho[i, j] gives the number of macroparticles in the i,j cell. &quot;&quot;&quot; # Compute area overlapping with 4 nearest neighbors (A1, A2, A3, A4) ivals = np.floor((positions[:, 0] - self.xmin) / self.dx).astype(int) jvals = np.floor((positions[:, 1] - self.ymin) / self.dy).astype(int) ivals[ivals &gt; self.Nx - 2] = self.Nx - 2 jvals[jvals &gt; self.Ny - 2] = self.Ny - 2 x_i, x_ip1 = self.x[ivals], self.x[ivals + 1] y_j, y_jp1 = self.y[jvals], self.y[jvals + 1] _A1 = (positions[:, 0] - x_i) * (positions[:, 1] - y_j) _A2 = (x_ip1 - positions[:, 0]) * (positions[:, 1] - y_j) _A3 = (positions[:, 0] - x_i) * (y_jp1 - positions[:, 1]) _A4 = (x_ip1 - positions[:, 0]) * (y_jp1 - positions[:, 1]) # Distribute fractional areas rho = self.zeros() cdef double[:, :] rho_view = rho cdef int i, j for i, j, A1, A2, A3, A4 in zip(ivals, jvals, _A1, _A2, _A3, _A4): rho_view[i, j] += A4 rho_view[i + 1, j] += A3 rho_view[i, j + 1] += A2 rho_view[i + 1, j + 1] += A1 return rho / self.cell_area def interpolate(self, grid_vals, positions): &quot;&quot;&quot;Interpolate values from the grid using the CIC method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. grid_vals : ndarray, shape (n, 2) Scalar value at each coordinate point. Returns - int_vals : ndarray, shape (nparts,) Interpolated value at each position. &quot;&quot;&quot; int_func = RegularGridInterpolator((self.x, self.y), grid_vals) return int_func(positions) def gradient(self, grid_vals): &quot;&quot;&quot;Compute gradient using 2nd order centered differencing. Parameters - grid_vals : ndarray, shape (Nx, Ny) Scalar values at each grid point. Returns - gradx, grady : ndarray, shape (Nx, Ny) The x and y gradient at each grid point. &quot;&quot;&quot; return np.gradient(grid_vals, self.dx, self.dy) . It should also be mentioned that the field interpolation method should be the same as the charge deposition method; if this is not true, it is possible for a particle to exert a force on itself! Let&#39;s test the method on a Gaussian distribution of 100,000 macroparticles in the $x$-$y$ plane, truncated at three standard devations. We&#39;ll choose the number of grid points to be $N_x = N_y = 64$. . # Create coordinate array nparts = 100000 cut = 3.0 X = truncnorm.rvs(a=4*[-cut], b=4*[cut], size=(nparts, 4)) X *= 10e-6 # Create bunch bunch = Bunch() bunch.fill(X) bunch.compute_extremum() # Distribute bunch particles on grid grid = Grid(bunch.xlim, bunch.ylim, size=(64, 64)) rho = grid.distribute(bunch.positions) . Field solver . The workhorse in the simulation loop is the field solver. We need to solve Poisson&#39;s equation: . $$ left({ frac{ partial^2}{ partial x^2} + frac{ partial^2}{ partial y^2}} right) = - frac{ rho left(x, y right)}{ varepsilon_0}. tag{6}$$ . The discretized version of the equation reads . $$ frac{ phi_{i+1,j} -2 phi_{i,j} + phi_{i-1,j}}{{ Delta_x}^2} + frac{ phi_{i,j+1} -2 phi_{i,j} + phi_{i,j-1}}{{ Delta_y}^2} = - frac{ rho_{i,j}}{ varepsilon_0} tag{7}$$ . for a grid with spacing $ Delta_x$ and $ Delta_y$. There are multiple paths to a solution; we will focus on the method implemented in PyORBIT which utilizes the Fourier convolution theorem. Let&#39;s briefly go over this method. The potential from an infinite line of elementary charges at the origin with number density $ lambda$ is . $$ phi( mathbf{x}) = - frac{ lambda e}{2 pi varepsilon_0} ln{| mathbf{x}|} = - frac{ lambda e}{2 pi varepsilon_0} int{ ln{| mathbf{x} - mathbf{y}|} delta( mathbf{y})d mathbf{y}}. tag{8}$$ . Note that $ mathbf{y}$ is just a dummy variable. By letting $G( mathbf{x} - mathbf{y}) = - ln{| mathbf{x} - mathbf{y}|}$ and $ rho( mathbf{x}) = delta( mathbf{x})$, then up to a scaling factor we have . $$ phi( mathbf{x}) = int{G( mathbf{x} - mathbf{y}) rho( mathbf{y})d mathbf{y}} = G( mathbf{x}) * rho( mathbf{x}). tag{9}$$ . In this form the potential is a convolution (represented by $*$) of the charge density $ rho$ with $G$, which is called the Green&#39;s function. On the grid this will look like . $$ phi_{i, j} = sum_{k,l ne i,j}{G_{i-k, j-l} rho_{k, l}}. tag{11}$$ . This solves the problem in $O(N^2)$ time complexity for $N$ grid points. This is already much faster than a direct force calculation but could still get expensive for fine grids. We can speed things up by exploiting the convolution theorem, which says that the Fourier transform of a convolution of two functions is equal to the product of their Fourier transforms. The Fourier transform is defined by . $$ hat{ phi}( mathbf{k})= mathcal{F} left[ phi( mathbf{x}) right] = int_{- infty}^{ infty}{e^{-i mathbf{k} cdot mathbf{x}} phi( mathbf{x}) d mathbf{x}}. tag{12}$$ . The convolution theorem then says $$ mathcal{F} left[ rho * G right] = mathcal{F} left[ rho right] cdot mathcal{F} left[G right]. tag{13}$$ . For the discrete equation this gives . $$ hat{ phi}_{n, m} = hat{ rho}_{n, m} hat{G}_{n, m}, tag{14}$$ . where the hat represents the discrete Fourier transform. The time complexity can be reduced to $O left(N log N right)$ with the FFT algorithm at our disposal. . There is a caveat to this method: Eq. (11) must be a circular convolution in order to use the FFT algorithm, which means $G$ must be periodic. But the beam is in free space (we&#39;ve neglected any conducting boundary), so this is not true. We can make it true by doubling the grid size in each dimension. We then make $G$ a mirror reflection in the new quadrants so that it is periodic, and also set the charge density equal to zero in these regions. After running the method on this larger grid, the potential in the new quadrants will be unphysical; however, the potential in the original quadrant will be correct. There are also some tricks we can play to reduce the space complexity, and in the end doubling the grid size is not much of a price to pay for the gain in speed. The method is implemented in the PoissonSolver class. . class PoissonSolver: &quot;&quot;&quot;Class to solve Poisson&#39;s equation on a 2D grid. Attributes - rho, phi, G : ndarray, shape (2*Nx, 2*Ny) The density (rho), potential (phi), and Green&#39;s function (G) at each grid point on a doubled grid. Only one quadrant (i &lt; Nx, j &lt; Ny) corresponds to to the real potential. &quot;&quot;&quot; def __init__(self, grid, sign=-1.): self.grid = grid new_shape = (2 * self.grid.Nx, 2 * self.grid.Ny) self.rho, self.G = np.zeros(new_shape), np.zeros(new_shape) self.phi = np.zeros(new_shape) def set_grid(self, grid): self.__init__(grid) def compute_greens_function(self): &quot;&quot;&quot;Compute Green&#39;s function on doubled grid.&quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny Y, X = np.meshgrid(self.grid.x - self.grid.xmin, self.grid.y - self.grid.ymin) self.G[:Nx, :Ny] = -0.5 * np.log(X**2 + Y**2, out=np.zeros_like(X), where=(X + Y &gt; 0)) self.G[Nx:, :] = np.flip(self.G[:Nx, :], axis=0) self.G[:, Ny:] = np.flip(self.G[:, :Ny], axis=1) def get_potential(self, rho): &quot;&quot;&quot;Compute the scaled electric potential on the grid. Parameters - rho : ndarray, shape (Nx, Ny) Number of macroparticles at each grid point. Returns - phi : ndarray, shape (Nx, Ny) Scaled electric potential at each grid point. &quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny self.rho[:Nx, :Ny] = rho self.compute_greens_function() self.phi = ifft2(fft2(self.G) * fft2(self.rho)).real return self.phi[:Nx, :Ny] . Running the algorithm gives the following potential on the doubled grid: . solver = PoissonSolver(grid) phi = solver.get_potential(rho) . We can then approximate the gradient of the potential using second-order centered differencing. This gives . $$( nabla phi)_{i,j} = frac{ phi_{i+1,j} - phi_{i-1,j}}{2 Delta_x} hat{x} + frac{ phi_{i,j+1} - phi_{i,j-1}}{2 Delta_y} hat{y}. tag{15}$$ . Ex, Ey = grid.gradient(-phi) . Finally, the value of the electric field at each particle position can be interpolated from the grid. . Ex_int = grid.interpolate(Ex, bunch.positions) Ey_int = grid.interpolate(Ey, bunch.positions) . Particle mover . All we need to do in this step is integrate the equations of motion. A common method is leapfrog integration in which the position and velocity are integrated out of phase as follows: . $$ m left( frac{ mathbf{v}_{i+1/2} - mathbf{v}_{i-1/2}}{ Delta_t} right) = mathbf{F}( mathbf{x}_i), tag{16}$$ . $$ frac{ mathbf{x}_{i+1} - mathbf{x}_i}{ Delta_t} = mathbf{v}_{i+1/2} tag{17}$$ . . Credit: S. LundA different scheme must be used when velocity-dependent forces are present. This is a symplectic integrator, which means it conserves energy. It is also second-order accurate, meaning that its error is proportional to the square of the $ Delta_t$. Finally, it is time-reversible. The only complication is that, because the velocity and position are out of phase, we need to push the velocity back one half-step before starting the simulation, and push it one half-step forward when taking a measurement. . Putting it all together . Simulation loop . We have all the tools to implement the simulation loop. While $s &lt; s_{max}$ we: . Compute the charge density on the grid. | Find the electric potential on the grid. | Interpolate the electric field at the particle positions. | Update the particle positions. | We&#39;ll first create a History class which stores the beam moments or phase space coordinates. . class History: &quot;&quot;&quot;Class to store bunch data over time. Atributes moments : list Second-order bunch moments. Each element is ndarray of shape (10,). coords : list Bunch coordinate arrays. Each element is ndarray of shape (nparts, 4) moment_positions, coord_positions : list Positions corresponding to each element of `moments` or `coords`. &quot;&quot;&quot; def __init__(self, bunch, samples=&#39;all&#39;): self.X = bunch.X self.moments, self.coords = [], [] self.moment_positions, self.coord_positions = [], [] if samples == &#39;all&#39; or samples &gt;= bunch.nparts: self.idx = np.arange(bunch.nparts) else: self.idx = np.random.choice(bunch.nparts, samples, replace=False) def store_moments(self, s): Sigma = np.cov(self.X.T) self.moments.append(Sigma[np.triu_indices(4)]) self.moment_positions.append(s) def store_coords(self, s): self.coords.append(np.copy(self.X[self.idx, :])) self.coord_positions.append(s) def package(self): self.moments = np.array(self.moments) self.coords = np.array(self.coords) . Now we&#39;ll create a Simulation class. . class Simulation: &quot;&quot;&quot;Class to simulate the evolution of a charged particle bunch in free space. Attributes - bunch : Bunch: The bunch to track. distance : float Total tracking distance [m]. step_size : float Distance between force calculations [m]. nsteps : float Total number of steps = int(length / ds). steps_performed : int Number of steps performed so far. s : float Current bunch position. history : History object Object storing historic bunch data. meas_every : dict Dictionary with keys: &#39;moments&#39; and &#39;coords&#39;. Values correspond to the number of simulations steps between storing these quantities. For example, `meas_every = {&#39;coords&#39;:4, &#39;moments&#39;:2}` will store the moments every 4 steps and the moments every other step. Defaults to storing only the initial and final positions. samples : int Number of bunch particles to store when measuring phase space coordinates. Defaults to the entire coordinate array. &quot;&quot;&quot; def __init__(self, bunch, distance, step_size, grid_size, meas_every={}, samples=&#39;all&#39;): self.bunch = bunch self.distance, self.step_size = distance, step_size self.nsteps = int(distance / step_size) self.grid = Grid(size=grid_size) self.solver = PoissonSolver(self.grid) self.fields = np.zeros((bunch.nparts, 2)) self.history = History(bunch, samples) self.s, self.steps_performed = 0.0, 0 self.meas_every = meas_every self.meas_every.setdefault(&#39;moments&#39;, self.nsteps) self.meas_every.setdefault(&#39;coords&#39;, self.nsteps) self.sc_factor = bunch.perveance / bunch.nparts def set_grid(self): &quot;&quot;&quot;Set grid limits from bunch size.&quot;&quot;&quot; self.bunch.compute_extremum() self.grid.set_lims(self.bunch.xlim, self.bunch.ylim) self.solver.set_grid(self.grid) def compute_electric_field(self): &quot;&quot;&quot;Compute self-generated electric field.&quot;&quot;&quot; self.set_grid() rho = self.grid.distribute(self.bunch.positions) phi = self.solver.get_potential(rho) Ex, Ey = self.grid.gradient(-phi) self.fields[:, 0] = self.grid.interpolate(Ex, self.bunch.positions) self.fields[:, 1] = self.grid.interpolate(Ey, self.bunch.positions) def kick(self, step_size): &quot;&quot;&quot;Update particle slopes.&quot;&quot;&quot; self.bunch.X[:, 1] += self.sc_factor * self.fields[:, 0] * step_size self.bunch.X[:, 3] += self.sc_factor * self.fields[:, 1] * step_size def push(self, step_size): &quot;&quot;&quot;Update particle positions.&quot;&quot;&quot; self.bunch.X[:, 0] += self.bunch.X[:, 1] * step_size self.bunch.X[:, 2] += self.bunch.X[:, 3] * step_size def store(self): &quot;&quot;&quot;Store bunch data.&quot;&quot;&quot; store_moments = self.steps_performed % self.meas_every[&#39;moments&#39;] == 0 store_coords = self.steps_performed % self.meas_every[&#39;coords&#39;] == 0 if not (store_moments or store_coords): return Xp = np.copy(self.bunch.X[:, [1, 3]]) self.kick(+0.5 * self.step_size) # sync positions/slopes if store_moments: self.history.store_moments(self.s) if store_coords: self.history.store_coords(self.s) self.bunch.X[:, [1, 3]] = Xp def run(self, meas_every={}): &quot;&quot;&quot;Run the simulation.&quot;&quot;&quot; self.store() self.compute_electric_field() self.kick(-0.5 * self.step_size) # desync positions/slopes for i in trange(self.nsteps): self.compute_electric_field() self.kick(self.step_size) self.push(self.step_size) self.s += self.step_size self.steps_performed += 1 self.store() self.history.package() . Demonstration . We need some way of checking our method&#39;s accuracy. Luckily there is an analytic benchmark available: the Kapchinskij-Vladimirskij (KV) distribution. Without going into any detail, the beam projects to a uniform density ellipse in the $x$-$y$ plane, and the space charge forces produced within this ellipse are linear (in general space charge forces are nonlinear). If we plug the KV distribution into the Vlasov equation, it can be seen that these forces will remain linear for all time if the external focusing forces are also linear. As a consequence, a set of self-consistent differential equations describing the evolution of the ellipse boundary can be written down. If we consider the beam to be an upright ellipse with semi-axis $a$ along the $x$ axis and $b$ along the $y$ axis, then without external fields the equations read: . $$ a&#39;&#39; = frac{2Q}{a + b} + frac{ varepsilon_x}{a^3}, $$ $$ b&#39;&#39; = frac{2Q}{a + b} + frac{ varepsilon_y}{b^3}. tag{18}$$ . These are known as the KV envelope equations or simply envelope equations. $Q$, called the perveance, is a dimensionless number which is proportional to the beam intensity but reduced by the beam energy. We can think of this constant as a measure of the space charge strength. The $ varepsilon_x$ and $ varepsilon_y$ terms are called the emittances and determine the area occupied by the beam in $x$-$x&#39;$ and $y$-$y&#39;$ phase space. For example, a beam with all particles sitting perfectly still in the $x$-$y$ plane has no emittance, but a beam which is instead spreading out has a nonzero emittance. These emittances will also be conserved for the KV distribution. The following function integrates the envelope equations. . def track_env(X, positions, perveance=0.0): &quot;&quot;&quot;Track beam moments (assuming KV distribution) through free space. Parameters - X : ndarray, shape (nparts, 4) Transverse bunch coordinate array. positions : list List of positions at which to evaluate the equations. perveance : float The dimensionless space charge perveance. Returns - ndarray, shape (len(positions), 4) Each row gives [a, a&#39;, b, b&#39;], where a and b are the beam radii in the x and y dimension, respectively. &quot;&quot;&quot; Sigma = np.cov(X.T) a, b = np.sqrt(Sigma[0, 0]), np.sqrt(Sigma[2, 2]) ap, bp = Sigma[0, 1] / a, Sigma[2, 3] / b epsx = np.sqrt(np.linalg.det(Sigma[:2, :2])) epsy = np.sqrt(np.linalg.det(Sigma[2:, 2:])) def derivs(env, s): a, ap, b, bp = env envp = np.zeros(4) envp[0], envp[2] = ap, bp envp[1] = 0.5 * perveance/(a + b) + epsx**2 / a**3 envp[3] = 0.5 * perveance/(a + b) + epsy**2 / b**3 return envp return odeint(derivs, [a, ap, b, bp], positions, atol=1e-14) . Some care must be taken in the choice of simulation parameters; we need a fine enough grid to resolve the hard edge of the beam and enough macroparticles per grid cell to collect good statistics. I chose what I thought was reasonable: 128,000 macroparticles, a step size of 2.5 cm, and a $128 times 128$ grid. . nparts = 128000 bunch_length = 250.0 # [m] intensities = [0.0, 10e14, 20e14, 40e14] # Simulation parameters distance = 10.0 # [m] step_size = 0.025 # [m] grid_size = (128, 128) samples = 10000 meas_every = {&#39;moments&#39;: int(0.1 * distance/step_size), &#39;coords&#39;: 4} . Below we create and track four identical KV distributions, each with a different intensity. . # Create KV bunch in normalized coordinates (surface of 4D unit sphere) X = np.random.normal(size=(nparts, 4)) # 4D Gaussian X = np.apply_along_axis(lambda row: row/np.linalg.norm(row), 1, X) # normalize rows # Scale by emittance eps_x, eps_y = 10e-6, 10e-6 A = 2 * np.sqrt(np.diag([eps_x, eps_x, eps_y, eps_y])) X = np.apply_along_axis(lambda row: np.matmul(A, row), 1, X) # Scale beam size and divergence relative to emittance alpha_x, alpha_y = 0.0, 0.0 beta_x, beta_y = 20.0, 20.0 V = np.zeros((4, 4)) V[:2, :2] = np.sqrt(1/beta_x)* np.array([[beta_x, 0], [alpha_x, 1]]) V[2:, 2:] = np.sqrt(1/beta_y)* np.array([[beta_y, 0], [alpha_y, 1]]) X = np.apply_along_axis(lambda row: np.matmul(V, row), 1, X) # Create and track bunches sims = [] for intensity in intensities: bunch = Bunch(intensity, bunch_length) bunch.fill(np.copy(X)) sim = Simulation(bunch, distance, step_size, grid_size, meas_every=meas_every, samples=samples) sim.run() sims.append(sim) . 100%|██████████| 400/400 [00:56&lt;00:00, 7.02it/s] 100%|██████████| 400/400 [00:55&lt;00:00, 7.26it/s] 100%|██████████| 400/400 [00:56&lt;00:00, 7.02it/s] 100%|██████████| 400/400 [00:53&lt;00:00, 7.45it/s] . This plot shows the horizontal and vertical beam size over time for each of the four chosen beam intensities. The solid lines are the result of integrating the envelope equations, while the black dots are the result of the PIC calculation. Notice that the beam expands on its own due to the nonzero emittance and that the effect of space charge is to increase the expansion rate. It seems to be quite accurate over this distance, and the runtime is acceptable for my purposes. Here is the evolution of a sample of 10,000 of the macroparicles as well as an ellipse showing the KV envelope. . &lt;/input&gt; Once Loop Reflect Conclusion . This post implemented an electrostatic PIC solver in Python. I learned quite a bit from doing this and was happy to see my calculations agree with the theoretical benchmark. One extension of this code would be to consider the velocity-dependent force from magnetic fields. It would also be straightforward to extend the code to 3D. Finally, all the methods used here are applicable to gravitational simulations. Here are some helpful references: . USPAS course | Hockney &amp; Eastwood | Birdsall &amp; Langdon | .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/simulation/space%20charge/2021/02/22/PIC.html",
            "relUrl": "/physics/accelerators/simulation/space%20charge/2021/02/22/PIC.html",
            "date": " • Feb 22, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Coupled parametric oscillators",
            "content": "Introduction . A previous post examined the analytic solutions to the equation of motion describing a parametric oscillator, which is a harmonic oscillator whose physical properties are time-dependent (but not dependent on the state of the oscillator). This problem was motivated by describing the transverse oscillations of a particle in an accelerator. In this post, the treatment will be extended to a coupled parametric oscillator. Basically, we are trying to solve the following equation of motion: . $$x&#39;&#39; + k_{11}(s)x + k_{13}(s)y + k_{14}(s)y&#39;,$$ $$y&#39;&#39; + k_{33}(s)y + k_{31}(s)x + k_{32}(s)x&#39;,$$ . where the prime denotes differentiation with respect to $s$. We also assume that each of the $k_{ij}$ coefficients are periodic, so $k_{ij}(s + L) = k_{ij}(s)$ for some $L$. . Motivation . The previous post discussed dipole and quadrupole magnetic fields, which have the special property that their fields depend linearly on $x$ and $y$, and are also uncoupled. Of course there are many other configurations possible. First, consider a solenoid magnet: . . Credit: brilliant.org The field within the coils points in the longitudinal direction and is approximatly constant ($ mathbf{B}_{sol} = B_0 hat{s}$). Plugging this into the Lorentz force equation we find: . $$ dot{ mathbf{v}} = frac{q}{m} mathbf{v} times mathbf{B} = frac{qB_0}{m} left({v_y hat{x} - v_x hat{y}} right).$$ . This means the motion in $x$ depends on the velocity in $y$, and vice versa, so this will contribute to $k_{14}$ and $k_{32}$. Coupling can also be produced from transverse magnetic fields. We again write the multipole expansion of this field: . $$B_y + iB_x = B_{ref} sum_{n=1}^{ infty} left({B_n + iA_n} right) left( frac{x + iy}{R_{ref}} right)^{n-1}.$$ . . Credit: Jeff Holmes . There will be nonlinear coupling (terms proportional to $x^j y^k$, where $j,k &gt; 1$ when $n &gt; 2$, but we are interested in linear coupling. This occurs when the skew quadrupole term $A_2$ is nonzero, which is true anytime a quadrupole is tilted in the transverse plane. The field couples the motion in one plane to the displacement in the other, contributing to the $k_{13}$ and $k_{31}$ terms. . Approach . Let&#39;s review the approach we took in analyzing the 1D parametric oscillator. We wrote the solution in pseudo-harmonic form, with an amplitude and phase which depended on time. We then found that particles travel along the boundary of an ellipse in 2D phase space, the area of which is a constant of the motion (we will denote this area by $ epsilon_x$). To understand the motion, we just need to know the dimensions and orientation of this ellipse, for which we proposed the parameters $ alpha_x$ and $ beta_x$, as well as the location of the particle on the ellipse boundary, which is determined by the phase $ mu_x$. All the subscripts can be replaced by $y$ to handle the vertical motion. We also wrote a transfer matrix $ mathbf{M}$, which connects the initial and final phase space coordinates after tracking through one period, from the parameters in the following form: . $$ mathbf{M} = mathbf{V} mathbf{P} mathbf{V}^{-1},$$ . where $ mathbf{V}^{-1}$ is a function of $ alpha_x$ and $ beta_x$ and transforms the ellipse into a circle while preserving the area, and $ mathbf{P}$ is a rotation in phase space according to the phase advance $ mu_x$. Basically, $ mathbf{V}$ turns the parametric oscillator into a harmonic oscillator. . This is a very elegant way to describe the motion with a minimal set of parameters. The question is: can we do something similar for coupled motion, in which the phase space is 4D, not 2D? To start, let&#39;s track a particle in a lattice with a nonzero skew quadrupole coefficient, plotting its phase space coordinates at one position after every period. . &lt;/input&gt; Once Loop Reflect The particle traces interesting donut-like shapes in horizontal ($x$-$x&#39;$) and vertical ($y$-$y&#39;$) phase space instead of ellipses. Below shows the shapes after 1000 periods. . There is definitely more than one frequency present, which we see if we plot the $x$ and $y$ position vs period number and take the FFT. . This is typical of a coupled oscillator. Such systems are typically understood as the superposition of normal modes, each of which corresponds to a single frequency. For example, consider two masses connected with a spring. There are two possible ways for the masses to oscillate at the same frequency. The first is a breathing mode in which they move in opposite directions, and the second is a sloshing mode in which they move in the same direction. The motion is simply the sum of these two modes. We will try to do something similar for a coupled parameteric oscillator. . Solution . Transfer matrix eigenvectors . If the phase space coordinate vector $ mathbf{x} = (x, x&#39;, y, y&#39;)^T$ evolves according to . $$ mathbf{x} rightarrow mathbf{Mx},$$ . where $ rightarrow$ represents tracking through one period, it can be shown that $ mathbf{M}$ is symplectic due to the Hamiltonian mechanics of the system. A consequence of the symplecticity condition is that $ mathbf{M}$ is fully described by 10 numbers instead of 16. Our method examines the eigenvectors of $ mathbf{M}$: . $$ mathbf{Mv} = e^{-i mu} mathbf{v}.$$ . The symplecticity condition also causes the eigenvalues and eigenvectors come in two complex conjugate pairs; this gives $ mathbf{v}_1$, $ mathbf{v}_2$, $ mu_1$, $ mu_2$ and their complex conjugates. The seemingly complex motion seen in the last animation is greatly simplified when written in terms of the eigenvectors. We can write any cooridinate vector as a linear combination of the real and imaginary components of $ mathbf{v}_1$ and $ mathbf{v}_2$: . $$ mathbf{x} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i psi_1} + sqrt{ epsilon_2} mathbf{v}_2e^{-i psi_2} right).$$ . We&#39;ve introduced two initial amplitudes ($ epsilon_1$ and $ epsilon_2$) as well as two initial phases ($ psi_1$ and $ psi_2$). Applying the transfer matrix then simply tacks on a phase. Thus, what we are observing are the 2D projections of the real components of these eigenvectors as they rotate in the complex plane. . $$ mathbf{Mx} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i left( psi_1 + mu_1 right)} + sqrt{ epsilon_2} mathbf{v}_2e^{-i( psi_2 + mu_2)} right).$$ . Let&#39;s replay the animation, but this time draw a red arrow for $ mathbf{v}_1$ and a blue arrow for $ mathbf{v}_2$. We&#39;ve chosen $ epsilon_1 = 4 epsilon_2$ and $ psi_2 - psi_1 = pi/2$. . &lt;/input&gt; Once Loop Reflect That really simplifies things! Each eigenvector simply rotates at its frequency $ mu_l$. It also explains why the amplitude in the $x$-$x&#39;$ and $y$-$y&#39;$ planes trade back and forth: it is because the projections of the eigenvectors rotate at different frequencies, sometimes aligning and sometimes anti-aligning. Because of this, the previous invariants $ epsilon_x$ and $ epsilon_y$ are replaced by $ epsilon_1$ and $ epsilon_2$ as the invariants. It is helpful to think of a torus (shown below). The two amplitudes would determine the inner and outer radii of the torus, and the two phases determine the location of a particle on the surface. . . Credit: Wikipedia Parameterization of eigenvectors . We are now going to introduce a set of parameters for these eigenvectors, and in turn the transfer matrix. We already have two phases, so that leaves 8 parameters. Our strategy is to observe that each eigenvector traces an ellipse in both horizontal ($x$-$x&#39;$) and vertical ($y$-$y&#39;$) phase space. Then, we will simply assign an $ alpha$ function and $ beta$ function to each of these ellipses. So, for the ellipse traced by $ mathbf{v}_1$ in the $x$-$x&#39;$ plane, we have $ beta_{1x}$ and $ alpha_{1x}$, and then for the second eigenvector we have $ beta_{2x}$ and $ alpha_{2x}$. The same thing goes for the vertical dimension with $x$ replaced by $y$. . . The actual eigenvectors written in terms of the parameters are . $$ vec{v}_1 = begin{bmatrix} sqrt{ beta_{1x}} - frac{ alpha_{1x} + i(1-u)}{ sqrt{ beta_{1x}}} sqrt{ beta_{1y}}e^{i nu_1} - frac{ alpha_{1y} + iu}{ sqrt{ beta_{1y}}} e^{i nu_1} end{bmatrix}, quad vec{v}_2 = begin{bmatrix} sqrt{ beta_{2x}}e^{i nu_2} - frac{ alpha_{2x} + iu}{ sqrt{ beta_{2x}}}e^{i nu_2} sqrt{ beta_{2y}} - frac{ alpha_{2y} + i(1-u)}{ sqrt{ beta_{2y}}} end{bmatrix}$$ . So in addition to the phases $ mu_1$ and $ mu_2$ we have $ alpha_{1x}$, $ alpha_{2x}$, $ alpha_{1y}$, $ alpha_{2y}$, $ beta_{1x}$, $ beta_{2x}$, $ beta_{1y}$, and $ beta_{2y}$. That&#39;s pretty much it. There are a few other parameters we need to introduce to simplify the notation, but they are not independent. The first is $u$, which, as noted in the figure, determines the areas of the ellipses in one plane relative to the other. The second and third are $ nu_1$ and $ nu_2$, which are phase differences between the $x$ and $y$ components of the eigenvectors (in the animation they are either $0$ or $ pi$). I won&#39;t discuss these here. The last thing to note is that the parameters reduce to their 1D definitions when there is no coupling in the lattice. So we would have $ beta_{1x}, beta_{2y} rightarrow beta_{x}, beta_{y}$ and $ beta_{2x}, beta_{1y} rightarrow 0$, and similar for $ alpha$. The invariants and phase advances would also revert back to their original values: $ epsilon_{1,2} rightarrow epsilon_{x,y}$ and $ mu_{1,2} rightarrow mu_{x,y}$. . Floquet transformation . These eigenvectors can also be used to construct a transformation which removes both the variance in the focusing strength and the coupling between the planes, turning the coupled parametric oscillator into an uncoupled harmonic oscillator. In other words, we seek a matrix $ mathbf{V}$ such that . $$ mathbf{V^{-1} M V} = mathbf{P} = begin{bmatrix} cos{ mu_1} &amp; sin{ mu_1} &amp; 0 &amp; 0 - sin{ mu_1} &amp; cos{ mu_1} &amp; 0 &amp; 0 0 &amp; 0 &amp; cos{ mu_2} &amp; sin{ mu_2} 0 &amp; 0 &amp; - sin{ mu_2} &amp; cos{ mu_2} end{bmatrix} $$We can do this simply by rewriting the following equation (I haven&#39;t yet figured out how to number equations in Jupyter): . $$ mathbf{x} = Re left( sqrt{ epsilon_1} mathbf{v}_1e^{-i psi_1} + sqrt{ epsilon_2} mathbf{v}_2e^{-i psi_2} right)$$ . in matrix form as $ mathbf{x} = mathbf{V} mathbf{x}_n$ with . $$ mathbf{x}_n = begin{bmatrix} sqrt{ epsilon_1} cos{ psi_1} - sqrt{ epsilon_1} sin{ psi_1} sqrt{ epsilon_2} cos{ psi_2} - sqrt{ epsilon_2} sin{ psi_2} end{bmatrix} $$ $$ mathbf{V} = left[{Re( mathbf{v}_1), -Im( mathbf{v}_1), Re( mathbf{v}_2), -Im( mathbf{v}_2)} right]$$ . Let&#39;s observe the motion in these new coordinates $ mathbf{x}_n$. . &lt;/input&gt; Once Loop Reflect The motion is uncoupled after this transformation; i.e., particles move in a circle of area $ varepsilon_1$ in the $x_n$-$x_n&#39;$ plane at frequency $ mu_1$, and in a circle of area $ varepsilon_2$ in the $y_n$-$y_n&#39;$ plane at frequency $ mu_2$. . Conclusion . The method introduced here allows us to describe the evolution of a parametric oscillator using the minimum number of parameters. Our physical motivation was an accelerator lattice with linear, coupled forces, such as when skew quadrupole terms are present in the magnetic fields. There is no agreed upon method to do this among accelerator physicists, but I like (and know) this method the best, and have used it in my research. I&#39;ve left out many details which can be found in the paper by Lebedev and Bogacz. The paper by Ripken is also very helpful. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/oscillators/coupling/2021/01/25/coupled_parametric_oscillators.html",
            "relUrl": "/physics/accelerators/oscillators/coupling/2021/01/25/coupled_parametric_oscillators.html",
            "date": " • Jan 25, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Parametric oscillators",
            "content": "This post presents the solution to a general problem: what is the motion of a particle in one dimension (1D) in the presence of time-dependent, linear, periodic forces? This amounts to solving the following equation of motion: . $$ frac{d^2x}{dt^2} + k(t)x = 0,$$ . where $k(t + T) = k(t)$ for some $T$. This is a parametric oscillator, a harmonic oscillator whose physical properties are not static. For example, the oscillations of a pendulum (in the small angle approximation) on the surface of a planet whose gravitational pull varies periodically would be described by the above equation. The solution to this equation was derived by George William Hill in 1886 to study lunar motion, and for this reason it is known as Hill&#39;s equation. It also finds application in areas such as condensed matter physics, quantum optics, and accelerator physics. After setting up the physical problem, we will examine the solutions and discuss their relevance to the last application, accelerator physics. . Problem motivation . Accelerator physics . Particle accelerators are machines which produce groups of charged particles (known as beams), increase their kinetic energy, and guide them to a target. These machines are invaluable to modern scientific research. The most famous examples are colliders, such as the LHC, in which two beams are smashed together to generate fundamental particles. A lesser known fact is that the fields of condensed matter physics, material science, chemistry, and biology also benefit tremendously from accelerators; this is due to the effectiveness of scattering experiments in which the deflection of a beam after colliding with a target is used to learn information about the target. The scattered beam is composed of neutrons in spallation neutron sources such as SNS, electrons in electron scattering facilities such as CEBAF, or photons in synchrotron light sources such as APS. In addition to scientific research, accelerators find use in medicine, particularly for cancer treatment, and also in various industrial applications. . . A large detector at an interaction point in the LHC. There are generally a few beam properties which are very important to experimentalists; in colliders it is the energy and luminosity, in spallation sources it is the intensity, and in light sources it is the brightness. There is thus a constant need to push these parameters to new regions. For example, below is the famous Livingston plot which shows the energy achieved by various machines over the past century. . . Note: vertical axis scale is beam energy needed to produce the center of mass energy by collision with a resting proton (credit: Rasmus Ischebeck). There are many physics issues associated with the optimization of these beam parameters. Accelerator physics is a field of applied physics which studies these issues. The task of the accelerator physicist is to understand, control, and measure the journey of the beam from its creation to its final destination. The difficulty of this task has grown over time; the improvement accelerator performance has brought with it a staggering increase in size and complexity. The construction and operation of modern accelerators generally requires years of planning, thousands of scientists and engineers, and hundreds of millions or even billions of dollars. Despite this complexity, the underlying physics principles are quite simple, and the single particle motion in one of these machines can be understood analytically if a few approximations are made. In the end we will arrive at Hill&#39;s equation. . How to build an accelerator . There are three basic tasks an accelerator has to accomplish. First, it must increase the beam energy (acceleration). Second, it must guide the beam along a predetermined path (steering). Third, it must ensure the beam particles remain close together (focusing). It is helpful to use a coordinate system in which the $s$ axis points along the design trajectory, and the $x$ and $y$ axes defined in the plane transverse to $s$. In this way the motion is broken up into transverse and longitudinal dynamics. . . How are these tasks accomplished? Well, particles are charged, and the force on a point charge in an electromagnetic field is given by . $$ mathbf{F} = q left({ mathbf{E} + mathbf{v} times mathbf{B}} right),$$ . where $q$ is the particle charge, $ mathbf{v}$ is the particle velocity, $ mathbf{E}$ is the electric field, and $ mathbf{B}$ is the magnetic field. An accelerator consists of a series of elements, each with their own $ mathbf{E}$ and $ mathbf{B}$; the collection of these elements is called a lattice. We need to determine which electric and magnetic fields to use. . The first task, acceleration, is not the focus of this post. The remaining tasks, steering and focusing, concern the motion in the transverse plane. $ mathbf{B}$ fields, not $ mathbf{E}$ fields, are used since their effect grows with increased particle velocity. Any transverse magnetic field $ mathbf{B} = (B_x, B_y)^T$ can be written using a multipole expansion . $$B_y + iB_x = B_{ref} sum_{n=1}^{ infty} left({B_n + iA_n} right) left( frac{x + iy}{R_{ref}} right)^{n-1}.$$ . $B_{ref}$ and $R_{ref}$ are a reference field strength and radius, respectively; just consider them to be constants. We then have the normal multiple coefficients $B_n$, and the skew multipole coefficients $A_n$. The field lines corresponding to the first few normal multipole coefficients are shown below. . . Credit: Jeff Holmes The dipole term is perfect for steering. The field is constant in magnitude and direction: . $$ mathbf{B}_{dipole} propto hat{y},$$ . producing a force which is proportional to the $x$ position: . $$ mathbf{F}_{dipole} propto - hat{x}.$$ . The quadrupole term is used for focusing. The field takes the following form: . $$ mathbf{B}_{quad} propto y hat{x} + x hat{y},$$ . with the resulting force: . $$ mathbf{F}_{quad} propto -x hat{x} + y hat{y}.$$ . The force from the quadrupole is focusing in the horizontal direction, but defocusing in the vertical direction; however, net focusing is still achieved by alternating the direction of the quadrupoles. This is analogous to a beam of light passing through a series of converging and diverging lenses. If the spacing and curvature of the lenses is correctly chosen, a net focusing can be achieved. . . Focusing (QF) and defocusing (QD) quadrupoles modeled as magnetic lenses. The forces which result from these fields are linear, meaning they are proportional the $x$ or $y$ but not $x^2$, $y^3$, etc., and they are uncoupled, meaning the dynamics in the $x$ and $y$ dimensions are independent. Now, we may ask, can we really produce a perfect dipole or quadrupole field? The answer is no. In reality there will always be higher order multipoles present in the field, but people work very hard to ensure these are much smaller than the desired multipole. This video shows a bit of the construction process for these magnets. . Linearized equation of motion . Making the above approximation of perfect dipole and quadrupole magnets, and ignoring all other elements in the machine, we arrive at the equation of motion for a single particle in the transverse plane: . $$x&#39;&#39; + k(s)x = 0,$$ . where $x&#39; = dx/ds$ and $k(s + L) = k(s)$ for some distance $L$. We could also write a similar equation for $y$. It is conventional to use the slope $x&#39;$ instead of the velocity; this allows us to talk about the position of the particle in the lattice instead of the amount of time which has passed. The period length $L$ could be the entire circumference of a circular machine, or could be a smaller repeated subsection. . Solution . Envelope function . The general solution to Hill&#39;s equation is given by . $$x(s) = sqrt{ epsilon} ,w(s) cos left({ mu(s) + delta} right).$$ . This introduces an amplitude $w(s) = w(s + L)$ which we call the envelope function, as well as a phase $ mu$, both of which depend on $s$. The constants $ epsilon$ and $ delta$ are determined by the initial conditions. Let&#39;s plot this trajectory in a FODO (focus-off-defocus-off) lattice, which consists of evenly spaced focusing and defocusing quadrupoles. Here is the focusing strength within the lattice (QF is the focusing quadrupole and QD is the defocusing quadrupole): . . For now we can think of the lattice as repeating itself forever in the $s$ direction. Each black line below is represents the trajectory for a different initial position and slope; although the individual trajectories look rather complicated, the envelope function has a very simple form. . . Phase space . The particle motion becomes much easier to interpret if we observe it in position-momentum space, aka phase space. The following animation shows the evolution of the particle phase space coordinates at a single position in the lattice. The position shown is $s = nL/4$, where $n$ is the period number, which corresponds to the midpoint between the focusing and defocusing quadrupoles. . . &lt;/input&gt; Once Loop Reflect We see that the particle jumps along the boundary of an ellipse in phase space. The shape and orientation of the ellipse will change if we look at a different position in the lattice, but its area will be the same. So, the motion is determined by the dimensions and oriention of this ellipse throughout the lattice, as well as the location of the paricle on the ellipse boundary. This motivates the definition of the so-called Twiss parameters, which were first introduced by Courant and Snyder in 1958: . $$ beta = w^2, quad alpha = - frac{1}{2} beta&#39;, quad gamma = frac{1 + alpha^2}{ beta}.$$ . The dimensions of the phase space ellipse are nicely described by these parameters: . . The maximum extent of the ellipse is determined by $ beta$ in the $x$ direction and $ gamma$ in the $y$ direction. $ alpha$ is proportional to the slope of the $ beta$ function, and so determines the tilt angle of the ellipse. The position of a particle on the ellipse is given by the phase $ mu$. Finally, the invariant of the motion corresponding to the ellipse area is constructed from the Twiss parameters as . $$ epsilon = beta {x&#39;}^2 + 2 alpha xx&#39; + gamma x^2$$ . for any $x$ and $x&#39;$. The $ beta$ functions and phase advances in both dimensions are extremely important to measure and control in a real machine. Here is an example of the horizontal and vertical $ beta$ functions in the SNS accumulator ring. . . Transfer matrices . A helpful tool to pair with the parameterization we just introduced is the transfer matrix, a matrix which connects the phase space coordinates at two different positions: . $$ begin{bmatrix} x x&#39; end{bmatrix}_{s + L} = mathbf{M} begin{bmatrix} x x&#39; end{bmatrix}_{s}$$ . The transfer matrix can be written as $ mathbf{M} = mathbf{V} mathbf{P} mathbf{V}^{-1}$, where . $$ mathbf{V} = frac{1}{ sqrt{ beta}} begin{bmatrix} beta &amp; 0 - alpha &amp; 1 end{bmatrix}$$ and $$ mathbf{P} = begin{bmatrix} cos mu &amp; sin mu - sin mu &amp; cos mu end{bmatrix} $$ . The effect of $ mathbf{V}^{-1}$ is to deform the phase space ellipse into a circle while preserving its area. $ mathbf{P}$ is then just a rotation in phase space, and $ mathbf{V}$ then transforms back into a tilted ellipse. This is illustrated below. . . $ mathbf{V}$ can be thought of as a time-dependent transformation which removes the variance in the focusing strength, turning the parametric oscillator into a simple harmonic oscillator. Often it is called the Floquet transformation. . Conclusion . We&#39;ve presented the solution to Hill&#39;s equation, which describes a parameteric oscillator. The equation pops up in multiple areas, but we focused on its application in accelerator physics, in which Hill&#39;s equation describes the transverse motion of a single particle in an accelerator with perfectly linear magnetic fields. . The solution is best understood geometrically: particles move around the surface of an ellipse in phase space, the area of which is an invariant of the motion. The dimensions and orientation of the ellipse are determined by $ alpha$ and $ beta$, and the location of the paricle on the ellipse boundary is determined by $ mu$. These parameters can be used to construct a time-dependent transformation ($ mathbf{V}$) which turns the parametric oscillator into a simple harmonic oscillator. . The next post will examine how this treatment can be extended to include coupling between the horizontal and vertical dimensions. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/oscillators/2021/01/21/parametric_oscillators.html",
            "relUrl": "/physics/accelerators/oscillators/2021/01/21/parametric_oscillators.html",
            "date": " • Jan 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am completing my PhD in physics from the University of Tennessee, Knoxille, working as part of the Accelerator Physics group at the Spallation Neutron Source. Previously, I graduated from Wheaton College with a BS in physics. . Publications . Computation of the matched envelope of the Danilov distribution, PRAB, 2021 | . Presentations . Computation of the matched envelope of the Danilov distribution, AP Group Meeting, SNS, 01.29.2021 | Parameterization of coupled motion, AP Group Meeting, SNS, 01.15.2021 | Thesis proposal, University of Tennessee, 09.30.2020 | .",
          "url": "https://austin-hoover.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://austin-hoover.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}