{
  
    
        "post0": {
            "title": "Arguing About Gods (part 3b)",
            "content": "Leibnizian cosmological arguments conclude that there is a necessary being (something that must exist) which explains the existence of contingent beings (things that do not have to exist). Such arguments address the question “Why does anything exist?” and generally utilize a principle of sufficient reason (PSR): every contingent fact has an explanation. Some recent formulations use a weaker explanatory principle: every contingent concrete being has an explanation, where concrete means “possibly causes something”, or even a modal version: every contingent concrete being possibly has an explanation. . . 1. The Principle of Sufficient Reason (PSR) . The PSR can be used to run an argument for the existence of a necessary being. Here is the basic form of such an argument [1]: . A contingent being (a being such that if it exists, it could have not-existed) exists. | All contingent beings have a sufficient cause of or fully adequate explanation for their existence. | The sufficient cause of or fully adequate explanation for the existence of contingent beings is something other than the contingent being itself. | The sufficient cause of or fully adequate explanation for the existence of contingent beings must either be solely other contingent beings or include a non-contingent (necessary) being. | Contingent beings alone cannot provide a sufficient cause of or fully adequate explanation for the existence of contingent beings. | Therefore, what sufficiently causes or fully adequately explains the existence of contingent beings must include a non-contingent (necessary) being. | Therefore, a necessary being (a being such that if it exists, it cannot not-exist) exists. | The universe, which is composed of only contingent beings, is contingent. | Therefore, the necessary being is something other than the universe. | Premise 2 is the PSR. Oppy suggests that the PSR, as it is usually formulated, is unacceptably strong and that defensible versions of the PSR are “so weak that it is implausible to suppose that they possess serious metaphysical bite” [2].1 In this section, I will discuss several points raised by Pruss in [3] in defense of the PSR. . 1.1. Support for the PSR . The PSR is a kind of rock-bottom principle that is difficult to arrive at from other principles; thus, one option is to accept the PSR in the absence of any cogent counterarguments. One argument for the PSR is that the denial of the PSR leads to extreme skepticism: if the PSR is false, then my sensory inputs could exist for no reason, meaning that I do not have any knowledge [4]. And since to assign a probability is to explain a frequency in terms of an underlying regularity, no probability can be assigned to this scenario [3]. We might go further and claim that the PSR is assumed whenever we scientifically inquire. Although there is debate about the fundamentality of causation in physics, it seems to me that physics is at least in search of explanations. If the PSR is false, then the following “explanation” is available for any fact: there is no explanation. Again, one might argue that there is no meaningful probability that can be assigned to this “explanation”. . 1.2. Objections to the PSR . Surely, those who reject the PSR are doing so because they are skeptical of applying it to every contingent fact; if there is just one unexplained contingent fact, then the PSR is false. Therefore, one way to object to the PSR is to identify an unexplained, or potentially unexplained, fact. . 1.2.1. Imagination . Hume suggested that the PSR is false because one can imagine an exception to it. This objection is not so interesting to me, so I will move on. . 1.2.2. Chance . Quantum mechanics leaves open the possibility of chancy — i.e., random — events [5].2 If an event is chancy, then it seems that there is no explanation for it. This would violate the PSR. . However, we might think that quantum events are not unexplained even if they are chancy: the laws of quantum mechanics + the system indeterministically produce — and thus explain — the measured state. This may be a less-than-ideal explanation since only patterns in groups of measurements are explained instead of individual measurements, but it is an explanation nonetheless; we are providing a background or framework on which the measurement is not surprising. (In [6], Alex Malpass notes that, in his opinion, it is an open question whether such an explanation is relevant to cosmological arguments.) . 1.2.3. Free will . Here is Oppy’s definition of libertarian free will: &quot;If an agent $X$ acts freely in performing action $A$ in circumstances $C$ at time $T$ in world $W$, then it is not made true by the truth-making core of the world $W$ prior to $T$ that agent $X$ will do $A$ in circumstances $C$.&quot; In other words, there is a possible world that shares the same history as the actual world, but in which the agent acted differently. The PSR is false if there is no explanation for why one world was actualized instead of the other world. . One solution to this problem is to reject the libertarian conception of free will.3 Another solution is to provide an explanation of libertarian free choices. We will have to resort to some sort of non-deterministic explanation, as in the previous section. Pruss presents a hypothesis in [3] that I have reorganized/paraphrased below. (The terms in brackets can be exchanged with the preceding unbracketed terms in each line.) . Hypothesis: Free choices are made based on reasons that one is “impressed by”, i.e., that one takes into consideration in making the decision. | Suppose agent $X$ has a binary choice between $A$ and $B$. | Let $S {T }$ be a subset of the reasons that favor $A {B }$ over $B {A }$. | If $X$ freely chooses $A {B }$, it is because $X$ is making a free choice between $A$ and $B$ while impressed by the reasons in $S {T }$; $X$ is also impressed by $T {S }$, but only acts on the impressive reasons in $S {T }$. 4 | . My initial thought is that it seems plausible for libertarian free choices to be explained in this way or a similar way. The strategy seems analogous to the previous attempt to explain chancy events. . 1.2.4. Modal fatalism . Suppose that $C$ is the conjunction of every contingent proposition and that $E$ explains $C$. $E$ cannot be contingent because $C$ would then contain $E$ and $E$ would explain itself, which is impossible; $E$ cannot be necessary because a necessary proposition cannot explain a contingent proposition; $E$ cannot exist. . This objection points out the difficulty in forming an explanatory link between the necessary and the contingent. Pruss pushes back in a few ways. First, in this objection, we have assumed that if $p$ is necessary and $p$ explains $q$, then $p$ entails $q$. He suggests that there are counterexamples, such as statistical/non-deterministic explanations, and that our normal concept of explanation does not involve entailment. Second, he argues that the PSR survives if libertarian free will is possible and if God — a necessary being — freely chose to instantiate $C$. The existence of God would then explain $C$ without entailing $C$. . There are several difficulties with this approach. One difficulty is that libertarian free will may be impossible. Another difficulty is that it may be the case that God’s choices are not contingent. Most theists maintain that God chose the actual world out of the possible worlds. For example, since God is morally perfect, it seems that God would not instantiate a world in which only evil occurred. In fact, it seems that God would instantiate the best possible world (I think there is debate about whether there is such a thing). The question is whether it is possible for God to have chosen differently. If it is not possible for God to have chosen differently, then maybe God’s choices are necessary, and therefore $C$ is necessary. This would render the PSR false. I suspect this is just an attack on the idea of libertarian free will and that the theist can probably escape without harm. . 1.2.5. Infinite chains . We’re now going to switch gears and focus on infinite causal/explanatory chains.5 Each element in the chain is explained by its predecessor — this is an internal explanation — but the question arises whether some external explanation is needed for the chain.6 . I’ve found that my intuitions change depending on whether the chain extends infinitely into the past. Let’s start with a chain that extends finitely into the past. Consider a particle $p_0$ that comes into existence at time $t = 1$ for no reason. Maybe we take issue with this. Now consider a particle $p_0$ that exists at time $t = 1$ and was created in the following way: for every integer $n ge 0$, particle $p_{n + 1}$ decayed into particle $p_{n}$ at $t = 2^{-n}$. Thus, there are an infinite number of causes within a finite time interval. Although the existence of each particle is explained by its parent, it will seem to many that the result is the same: the particle came into existence uncaused. Therefore, Pruss says, if we demand a cause for one particle, we should demand a cause for an infinite series of particles. . He then argues that our same intuitions should apply if the infinite number of causes are not squeezed into a finite time interval but are instead spread out over an infinite past. In other words, each cause in the interval (0, 1] is mapped to a negative time while preserving the ordering: $1 rightarrow 1$, $ frac{1}{2} rightarrow 0$, $ frac{1}{4} rightarrow -1$, $ frac{1}{8} rightarrow -2$, $ dots$. It appears that nothing has changed, so we should either accept or reject the possibility of both scenarios. . However, it is difficult to imagine what an external cause — a reason why the particles are not different particles, or shoes, or books, or why they exist at all — would look like since there is no first time. (The same can be said of the (0, 1] scenario if we assume that time began so that there is no time $t le 0$.) We must abandon the idea that this external cause explains only the first member of the chain, since there is no first member, and that the cause is temporally prior to its effect, since there is no first time. Pruss says: . Kant’s example of a metal ball continually causing a depression in a soft material shows that simultaneous causation is conceivable. And apart from full or partial reductions of the notion of causation to something like Humean regularity and temporal precedence, I do not think there is much reason to suppose that the cause of a temporal effect must even be in time. [3] . (I am not sure exactly what is meant by the last line.) Thus, the cause would have to, in some way, “support” the chain such that if the cause did not exist, the chain would not exist. This is confusing and deserves more thought. . 2. Weaker explanatory principles . Pruss &amp; Rasmussen utilize a more modest explanatory principle to argue for the existence of a necessary being in their book Necessary Existence. They start with the traditional argument from contingency [7]. . For any contingent concrete things, there is an explanation of the fact that those things exist. | Considering all the contingent concrete things that exist, if there is an explanation of the fact that those things exist, then there is a necessary concrete thing. | (Therefore) There is a necessary concrete being. | Here, “concrete” means “possibly causes something”, i.e., not an abstract object like the number two. This explanatory principle is not as strong as the PSR because it is restricted to facts about existence, so it is not affected by, say, the possibility of unexplained free actions.7 . The authors note a few strengths of this argument:it is defensible against traditional objections from Hume [8] and Kant [9], it is adaptable to a variety of metaphysical frameworks, and the basic reasoning behind the argument is simple and intuitive [7]. The authors also note a few weaknesses of the argument: it does not allow explanatory loops, it does not allow completely internal explanations, and it does not allow any exceptions to the explanatory principle. This last weakness is the most concerning due to the possibility of chancy events. Although indeterministic/statistical explanations are on the table, some people may not think that these are adequate explanations and will insist that, if there are chancy events, chancy events are unexplained. To overcome these weaknesses, the authors introduce several modal arguments. Recall from the discussion of ontological arguments that it is mostly accepted that “possibly necessary” is equivalent to “necessary”. Thus, if it can be shown that a necessary being possibly exists, then it follows that a necessary being exists. We will focus only on the first argument in the book, which Leon [10] rewrites as: . Normally, things that can begin to exist can have a cause of the beginning of their existence. | Contingent concrete reality can begin to exist. | Therefore, there can be a cause of the beginning of contingent concrete reality’s existence. | If there can be a cause of the beginning of contingent concrete reality’s existence, then a necessary being exists. | Therefore, a necessary being exists. | The primary advantage of this argument is the weakening of the explanatory principle (first premise). This premise is more difficult to reject. It allows for exceptions such as uncaused contingent beings, explanatory loops, and internal explanations. As Leon notes, this allows the principle to be used as a defeasible rule of thumb. Referring to Koons [11], Leon writes “To avoid the demands of a well-supported defeasible principle, one must give principled grounds for thinking that it admits of an exception in the particular case at stake.” [10]. (There is debate about whether the PSR should be used as a defeasible rule of thumb — see [12], [13], and [14].) . The second premise is more controversial. In support of this premise, Rasmussen &amp; Pruss suggest three approaches. First, one might argue that contingent concrete reality began to exist; this is the task of the Kalam argument. Second, one might argue only that it is plausible that contingent concrete reality began to exist; for example, by finding a viable cosmological model in which time begins or by showing that causal finitism is possibly true. Third, one might argue that it is conceivable that contingent concrete reality began to exist, which provides defeasible evidence of possibility. . The primary critique of the second premise is that it is false if origin essentialism is true. On origin essentialism, if we look at our universe, the possible worlds shrink: in our universe, it is either true in all possible worlds, or false in all possible worlds, that contingent concrete reality began to exist. Thus, the word “can” should be removed from the second premise. The authors give several responses to this objection. The first response is a short argument against origin essentialism. The second response is a possible workaround, maintaining origin essentialism. Since origin essentialism is a new concept to me, I will have to return to this discussion some other time. . The fourth premise follows from possibly necessary $ rightarrow$ necessary. Leon offers an alternative to a necessary being; he raises the possibility of factually necessary beings: . Perhaps matter-energy (or whatever matter-energy is ultimately composed) is a factually necessary being. According to such a scenario, the contingent dependent beings (e.g., rocks, trees, planets, you and I, etc.) come into being when two or more contingent independent beings (i.e., factually necessary beings) are combined, and the contingent dependent beings cease to exist when they decompose into their elements. However, the fundamental elements of which contingent dependent beings are composed (i.e., the contingent independent beings/factually necessary beings) cannot pass away, for they are at least de facto indestructible—i.e., nothing in the actual world has what it takes to knock them out of existence. Nor can they be created, for they are eternal, existentially independent, and (assuming origin essentialism and their being uncaused at the actual world) essentially uncaused. [10] . I am not sure the reason for this distinction. Why not just say that matter-energy is necessary? . Another worry for any of these modal arguments is the existence of parody arguments.8 Oppy constructs a parody argument for every argument in Necessary Existence in his review of the book [15]. . 3. Conclusion . Oppy has a nice summary of the stances one can take about the foundations/origins of the universe: . Some philosophers suppose that every possible world shares laws and initial causal history with the actual world. Those philosophers divide into two camps:those who suppose that there is just one possible world; and those who suppose that there are many possible worlds. Philosophers in the first camp suppose that causal laws are deterministic; philosophers in the second camp suppose that causal laws are not deterministic. One thing that these philosophers have in common is that they suppose that, if there is an initial causal state, then that initial causal state, and anything that exists in that initial causal state, is necessary: if these philosophers are naturalists, then they suppose that the initial natural state is necessary; if these philosophers are theists, then they suppose that the initial divine state is necessary. Of course, other philosophers suppose that, if there is an initial causal state, then that initial causal state is contingent; and these philosophers divide further on the question of whether there is anything that exists in the initial state that is necessary. It is to be expected that what philosophers have to say about the kinds of arguments that Pruss and Rasmussen discuss is determined by their background views about modality, causation, ontology, epistemology, axiology, and so forth. [15] . Intuitions clash. For example, physicist Sean Carroll says, &quot;I think that brute facts are things we need to accept; the universe is probably one of them.&quot; [16]. Perhaps all these options are equally strange, but the idea that the only explanation of the universe is brute contingency seems the most strange to me. . Non-theists can claim that there is a concrete necessary being (or beings); possible candidates are the universe, fields, particles, etc. In this sense, theists and non-theists can agree on some aspects of the fundamental structure reality. Theists have an additional belief: that there is only one concrete necessary being that has properties consistent with those traditionally ascribed to God. The question thus arises: is there any way to choose between these views? This question needs to be treated on its own, separate from the cosmological argument. . . 1. We are referred to Oppy’s book Philosophical Perspectives on Infinity for a detailed treatment of the PSR. Many of Oppy’s thoughts on infinity are also apparently found in Philosophical Perspectives on Infinity. I should have read this book before Arguing About Gods.↩ . 2. Some interpretations of quantum mechanics are deterministic (such as the pilot wave theory) and others are indeterministic. The correct interpretation of quantum mechanics is unknown. (See Tim Maudlin here).↩ . 3. Many of those that do not accept the PSR also do not accept the libertarian account of free will, in which case this objection is irrelevant. Although I haven’t studied free will in any detail, I am open to the idea that my choices are not determined by the initial conditions of the universe. It is, of course, a difficult question.↩ . 4. My wife, who is not familiar with (or very interested in) this discussion, came up with essentially the same answer when I presented the dilemma to her.↩ . 5. The most famous explanatory regress is the following explanation for why the Earth does not fall: it sits on the back of a turtle, which sits on the back of a turtle, which sits on the back of a turtle, ... it remains unexplained why the entire stack of turtles does not fall, why the stack is comprised of turtles instead of giraffes, etc.↩ . 6. One way to rule out infinite chains is to accept finitism (the denial of the existence of actual infinities in the real world) or causal finitism (the denial of the existence of infinite causal chains). I like causal finitism, but for now, we are assuming that both finitism and causal finitism are false.↩ . 7. Also note that this is closer to a modal version of the Kalam argument since we are dealing with causes and things beginning to exist.↩ . 8. Recall the ontological argument (It is possible that a necessary being exists; therefore, a necessary being exists.) and its parody (It is possible that a necessary being does not exist; therefore, a necessary being does not exist.). The parody argument is taken to show that the modal ontological argument is unsound. We may as well replace the argument with its conclusion: a necessary being exists.↩ . [1] B. Reichenbach, &quot;Cosmological Argument,&quot; In E.N. Zalta,ed., The Stanford Encyclopedia of Philosophy, Winter 2021, (Metaphysics Research Lab, Stanford University, 2021). . [2] G. Oppy, Philosophical Perspectives on Infinity (Cambridge University Press, 2006). . [3] A. Pruss, &quot;Leibnizian Cosmological Arguments,&quot; In W.L. Craig, &amp; J.P. Moreland,eds., Blackwell Companion to Natural Theology (Blackwell Publishing Ltd). . [4] R. C. Koons &amp; A. R. Pruss, &quot;Skepticism and the principle of sufficient reason,&quot; Philosophical Studies 178:1079–1099 (2021). https://doi.org/10.1007/s11098-020-01482-3. . [5] T. Maudlin, &quot;What Bell did,&quot; Journal of Physics A: Mathematical and Theoretical 47:424010 (2014). https://doi.org/10.1088/1751-8113/47/42/424010. . [6] Friction, &quot;Alex Malpass on the principle of sufficient reason,&quot; https://www.youtube.com/watch?v=9pN7wgX5_R0 (2021). . [7] A. R. Pruss &amp; J. L. Rasmussen, Necessary Existence (Oxford University Press, 2018). . [8] D. Hume, Dialogues Concerning Natural Religion (1779). . [9] I. Kant, Critique of Pure Reason (1787). . [10] F. Leon, &quot;Causation and Sufficient Reason,&quot; In G. Oppy, &amp; J.W. Koterski,eds., Theism and Atheism: Opposing Viewpoints in Philosophy (MacMillan Reference, 2019). . [11] R. Koons, &quot;Defeasible Reasoning,&quot; In E.N. Zalta,ed., The Stanford Encyclopedia of Philosophy, Fall 2021, (Metaphysics Research Lab, Stanford University, 2021). . [12] R. Koons, &quot;A New Look at the Cosmological Argument,&quot; American Philosophical Quarterly 34 (1997). . [13] G. Oppy, &quot;Koons’ Cosmological Argument,&quot; Faith and Philosophy 16:378–389 (1999). https://doi.org/10.5840/faithphil199916335. . [14] R. Koons, &quot;Defeasible Reasoning, Special Pleading and the Cosmological Argument: A Reply to Oppy,&quot; Faith and Philosophy 18:192–203 (2001). https://doi.org/10.5840/faithphil20011823. . [15] G. Oppy, &quot;Alexander R. Pruss and Joshua L. Rasmussen. textitNecessary Existence,&quot; Journal of Analytic Theology 7:765–771 (2019). https://doi.org/10.12978/jat.2019-7.061712141524. . [16] C. T. Truth, &quot;Sean Carroll - Why There is &quot;Something&quot; rather than &quot;Nothing&quot;,&quot; https://www.youtube.com/watch?v=c-QkJUxcGt8 (2016). .",
            "url": "https://austin-hoover.github.io/blog/theism/cosmological%20arguments/2022/03/22/arguing_about_gods_3b.html",
            "relUrl": "/theism/cosmological%20arguments/2022/03/22/arguing_about_gods_3b.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Some figures to illustrate beam injection and accumulation",
            "content": "Charge-exchange injection . One method to create high-intensity hadron beams is charge-exchange injection: negatively charged particles are repeatedly accelerated in a linear accelerator (linac) and injected into a circular accelerator (ring), accumulating charge in the ring over time. I created a simple animation to visualize this process. First some imports. . import numpy as np from scipy.stats import truncnorm from matplotlib import animation from matplotlib import pyplot as plt import proplot as pplt import seaborn as sns pplt.rc[&#39;animation.html&#39;] = &#39;jshtml&#39; pplt.rc[&#39;animation.ffmpeg_path&#39;] = &#39;/usr/local/bin/ffmpeg&#39; pplt.rc[&#39;cmap.discrete&#39;] = False pplt.rc[&#39;figure.facecolor&#39;] = &#39;None&#39; pplt.rc[&#39;grid&#39;] = False pplt.rc[&#39;savefig.dpi&#39;] = 150 . And some helper functions. . def rotation_matrix(angle): &quot;&quot;&quot;Clockwise rotation matrix.&quot;&quot;&quot; cs, sn = np.cos(angle), np.sin(angle) return np.array([[cs, sn], [-sn, cs]]) def apply(M, X): &quot;&quot;&quot;Apply matrix `M` to each row of `X`.&quot;&quot;&quot; return np.apply_along_axis(lambda row: np.matmul(M, row), 1, X) . The Bunch class stores the $x$, $x&#39;$, and $z$ coordinates, which are measured relative to the bunch centroid. It also stores $s$, the location of the bunch center along the beam line. To transport the beam, we just increase $s$. We&#39;ll let the particles perform harmonic oscillations in the transverse plane. The only tricky part is to convert to an aerial view and to bend the circulating beam around the ring. . class Bunch: &quot;&quot;&quot;Class to store bunch coordinates. Attributes - X : ndarray, shape (n, 3) The bunch coordinates (x, x&#39;, z). s : The location of the bunch center along the beamline. &quot;&quot;&quot; def __init__(self, n, zrms=0.1, aspect=0.1): &quot;&quot;&quot;Constructor. n : int The number of particles in the bunch. zrms : float The rms width of the z distribution. aspect : float The rms width of the x and x&#39; distribution relative to `zrms`. &quot;&quot;&quot; self.n, self.zrms, self.aspect = n, zrms, aspect self.X = np.random.normal( scale=[aspect * zrms, aspect * zrms, zrms], size=(n, 3), ) self.s = 0.0 def transport(self, distance, period_length): &quot;&quot;&quot;Propagate the bunch along the beamline. Parameters - distance : float The distance along the beamline. period_length : float The period length of transverse focusing. The particles perform transverse harmonic oscillations. &quot;&quot;&quot; self.X[:, :2] = apply( rotation_matrix(2.0 * np.pi * (distance / period_length)), self.X[:, :2], ) self.s += distance def global_coords_line(self, x0=0.0, y0=0.0, angle=0.0): &quot;&quot;&quot;Return global (top-view) coordinates for straight-line transport. Parameters - x0, y0 : float Initial bunch centroid coordinates. angle : float The angle of the beamline. Returns - ndarray, same shape as self.X The top-view coordinates. &quot;&quot;&quot; # The initial bunch is vertical when viewed from above. Point it # in the right direction. R = rotation_matrix(angle + 0.5 * np.pi) x, y = apply(R, self.X[:, [0, 2]]).T # Slide along the beamline to the correct location. x += x0 + self.s * np.cos(angle) y += y0 + self.s * np.sin(angle) coords = np.vstack([x, y]).T return coords def global_coords_ring(self, ring_length): &quot;&quot;&quot;Return global (top-view) coordinates for circular transport. Parameters - ring_length : float The circumference of the ring. We assume it is centered on the origin. Returns - ndarray, same shape as self.X The top-view coordinates. &quot;&quot;&quot; # The initial bunch is vertical when viewed from above. Make it # horizontal. R = rotation_matrix(0.5 * np.pi) x, y = apply(R, self.X[:, [0, 2]]).T # Put the bunch at the top of the ring and account for ring curvature. ring_radius = ring_length / (2.0 * np.pi) y += np.sqrt(ring_radius**2 - x**2) # Slide along the beamline to the correct location. phi = 2.0 * np.pi * (self.s % ring_length) / ring_length R = rotation_matrix(phi) coords = np.vstack([x, y]).T coords = apply(R, coords) return coords . To run the &quot;simulation&quot;, we will create two bunches: the first circulates in the ring, and the second is repeatedly stepped through the linac. After one revolution, the second bunch gives its particles to the first bunch. . # Settings n_turns = 6 n_steps = 50 # steps per turn. ring_length = 2.0 * np.pi step_length = ring_length / n_steps period_length = ring_length / 3.18 n = 50 # size of minipulse x0, y0 = (-ring_length, 1.0) # start of the linac zrms = 0.19 # rms bunch length aspect = 0.09 # horizontal/vertical aspect ratio # Calculate and store the bunch coordinates. np.random.seed(0) lbunch = Bunch(n, zrms, aspect) rbunch = Bunch(n, zrms, aspect) lcoords = [np.copy(lbunch.global_coords_line(x0, y0))] rcoords = [np.copy(rbunch.global_coords_ring(ring_length))] for turn in range(n_turns): for step in range(n_steps): for bunch in [lbunch, rbunch]: bunch.transport(step_length, period_length) lcoords.append(lbunch.global_coords_line(x0, y0)) rcoords.append(rbunch.global_coords_ring(ring_length)) rbunch.X = np.vstack([rbunch.X, lbunch.X]) lbunch.__init__(n, zrms, aspect) . Here is the animation: . fig, ax = pplt.subplots(figwidth=6.7, figheight=3.6, aspect=1.0) kws = dict(zorder=0, color=&#39;black&#39;, lw=0.75, alpha=0.06) ax.plot([-ring_length, 0.0], [1.0, 1.0], **kws) psi = np.linspace(0.0, 2.0 * np.pi, 1000) ax.plot(np.cos(psi), np.sin(psi), **kws) ax.format( xlim=(-0.5 * ring_length, 1.2), ylim=(-1.2, 1.2), xticks=[], yticks=[], yspineloc=&#39;neither&#39;, xspineloc=&#39;neither&#39; ) plt.close() plot_kws = dict(marker=&#39;.&#39;, lw=0, ms=4.0, alpha=0.3, ec=&#39;None&#39;) line1, = ax.plot([], [], color=&#39;black&#39;, **plot_kws) line2, = ax.plot([], [], color=&#39;pink9&#39;, **plot_kws) def update(i): line1.set_data(rcoords[i][:, 0], rcoords[i][:, 1]) line2.set_data(lcoords[i][:, 0], lcoords[i][:, 1]) frames = n_steps * n_turns + 1 fps = 0.5 * (frames / n_turns) anim = animation.FuncAnimation(fig, update, frames=frames, interval=(1000.0 / fps)) . . . It could be interesting to display the results of an actual simulation in this way — usually they are displayed in the frame of the beam centroid. . Phase space painting . If there is significant overlap between the injected and circulating beams, a huge charge density will be created and the beam size will blow up due to the electric forces between the particles. To avoid this, the relative distance and angle between the beams is varied over time. The idea is to inject new particles into unoccupied regions of the transverse phase space ($x$-$x&#39;$-$y$-$y&#39;$) of the circulating beam. This process is called phase space painting, or simply painting. . There are many possible painting methods. The two most common methods are correlated painting and anti-correlated painting. In correlated painting, initial particles are injected directly onto the closed-orbit in the ring, then moved away from the origin at a rate proportional to the square root of time. The angle between the beams is always zero. In other words, . $$ begin{aligned} x(t) &amp;= x_{max} sqrt{t}, y(t) &amp;= y_{max} sqrt{t}, x&#39;(t) &amp;= 0, y&#39;(t) &amp;= 0. end{aligned} tag{1} $$ Here $t$ is time, normalized to the range [0, 1]. Anti-correlated painting reverses this process in one of the planes: . $$ begin{aligned} x(t) &amp;= x_{max} sqrt{t}, y(t) &amp;= y_{max} sqrt{1 - t}, x&#39;(t) &amp;= 0, y&#39;(t) &amp;= 0 end{aligned} tag{2} $$ Realistic painting simulations take a long time. I sometimes just want to see the turn-by-turn distribution without particle interactions or nonlinear effects. In this case, propagating the beam through the ring reduces to matrix multiplication, and if we view the distribution in normalized $x$-$x&#39;$ and $y$-$y&#39;$ phase space, the transfer matrix reduces to a rotation matrix in those planes. The rotation angle is $2 pi nu_x$ in the $x$-$x&#39;$ plane and $2 pi nu_y$ in the vertical plane, where $ nu_{x}$ and $ nu_y$ are the number of phase space oscillations per turn in either plane. . This can still take a while if many particles are used — the circulating beam needs to be rotated in the transverse plane on each turn. We can reduce the time in the following way. Assume we want to know the distribution on turn $t$. First, generate $t$ minipulses from the linac and put them at the origin. Second, slide each minipulse to its final amplitude using Eq.$~$(1) or Eq.$~$(2). Third, rotate each minipulse in $x$-$x&#39;$ and $y$-$y&#39;$ according to when it was injected: the first minipulse does $t$ turns, the second does $t - 1$ turns, etc. until the last minipulse, which doesn&#39;t move. The following Painter class performs these steps. . class Painter: &quot;&quot;&quot;Class to illustrate phase space painting. (Linear approximation, non-interacting particles, normalized phase space.) &quot;&quot;&quot; def __init__(self, n_turns=3000, n_inj=500, inj_rms=0.15, inj_cut=3.0): &quot;&quot;&quot;Constructor. n_turns : int The number of turns before accumulation is complete. n_inj : int The number of particles injected per turn. inj_rms, inj_cut : float The transverse rms width and cut-off of each Gaussian minipulse. &quot;&quot;&quot; self.n_inj = n_inj self.n_turns = n_turns self.inj_rms = inj_rms self.inj_cut = np.repeat(inj_cut, 4) self.times = np.linspace(0.0, 1.0, n_turns) self.set_xmax() self.is_initialized = False def set_xmax(self, xmax=None): &quot;&quot;&quot;Set the final injection point in phase space (x, x&#39;, y, y&#39;).&quot;&quot;&quot; if xmax is None: xmax = [1.0, 0.0, 1.0, 0.0] self.xmax = np.array(xmax) def inj_point(self, turn, method=&#39;correlated&#39;): &quot;&quot;&quot;Return the phase space coordinates of the injection point.&quot;&quot;&quot; t = self.times[turn] if method == &#39;correlated&#39;: return self.xmax * np.sqrt(t) elif method == &#39;anti-correlated&#39;: tau1 = np.sqrt(t) tau2 = np.sqrt(1.0 - t) return np.multiply(self.xmax, [tau1, tau1, tau2, tau2]) else: raise ValueError(&#39;Invalid method&#39;) def generate_minipulse(self): &quot;&quot;&quot;Generate a minipulse at the origin.&quot;&quot;&quot; X = truncnorm.rvs( scale=self.inj_rms, size=(self.n_inj, 4), a=-self.inj_cut, b=+self.inj_cut, ) return X def initialize(self): &quot;&quot;&quot;Initialize all minipulses at the origin.&quot;&quot;&quot; self.coords0 = [self.generate_minipulse() for _ in range(self.n_turns)] self.is_initialized = True def paint(self, nux, nuy, turns=1, method=&#39;correlated&#39;): &quot;&quot;&quot;Return the painted distribution. Parameters - nux, nux : float Horizontal and vertical tunes (oscillations per turn). turns : int Paint this many turns. method : {&#39;correlated&#39;, &#39;anti-correlated&#39;} Correlated painting paints min-to-max in both x-x&#39; and y-y&#39;. Anti-correlated painting paints min-to-max in x-x&#39; and max-to-min in y-y&#39;. Returns - X : ndarray, shape (turns * n, 4) The painted phase space distribution. &quot;&quot;&quot; if not self.is_initialized: self.initialize() coords0 = np.copy(self.coords0[:turns]) # Move each minipulse to its final amplitude. for turn in range(turns): coords0[turn] += self.inj_point(turn, method) # Rotate each minipulse by the appropriate amount of turns. X = [] for turn, X0 in enumerate(coords0): M = np.zeros((4, 4)) _turns = turns - turn + 1 M[:2, :2] = rotation_matrix(2. * np.pi * nux * _turns) M[2:, 2:] = rotation_matrix(2. * np.pi * nuy * _turns) X.append(apply(M, X0)) return np.vstack(X) . We&#39;ll also need a method to plot the 2D projections of the 4D phase space distribution during injection. . def plot_projections(coords, method, painter): limits = 4 * [(-1.5, 1.5)] indices = [(0, 2), (0, 1), (2, 3), (0, 3), (2, 1), (1, 3)] line_kws = dict(color=&#39;black&#39;, lw=0.4, alpha=0.1) fig, axes = pplt.subplots( nrows=len(coords), ncols=6, figwidth=6.7, space=0, spanx=False, spany=False, sharex=False, sharey=False, ) axes.format(xticks=[], yticks=[], xticklabels=[], yticklabels=[]) for col in range(axes.shape[1]): i, j = indices[col] axes[:, col].format(xlim=limits[i], ylim=limits[j]) dims = {0:&quot;x&quot;, 1:&quot;x&#39;&quot;, 2:&quot;y&quot;, 3:&quot;y&#39;&quot;} axes[0, col].set_title(&#39;{}-{}&#39;.format(dims[i], dims[j])) for row, (turn, X) in enumerate(zip(turns_list, coords)): xinj = painter.inj_point(turn, method) time = painter.times[turn] axes[row, 0].set_ylabel(&#39;t = {:.2f}&#39;.format(time), fontsize=&#39;small&#39;) for col in range(axes.shape[1]): ax = axes[row, col] i, j = indices[col] x, y = X[:, i], X[:, j] bins = 125 if row == 0 else &#39;auto&#39; sns.histplot(ax=ax, x=x, y=y, ec=&#39;None&#39;, cmap=&#39;mono&#39;, binrange=(limits[i], limits[j]), bins=bins) ax.axvline(xinj[i], **line_kws) ax.axhline(xinj[j], **line_kws) return axes . Let&#39;s first look at correlated painting. We&#39;ll assume that the lattice is uncoupled and has unequal tunes. . nux = 0.1810201 nuy = nux - 0.143561 n_turns = 3000 turns_list = np.linspace(100, n_turns - 1, 5).astype(int) painter = Painter(n_turns) painter.set_xmax([1.0, 0.0, 0.0, 1.0]) method = &#39;correlated&#39; coords = [painter.paint(nux, nuy, turns, method) for turns in turns_list] axes = plot_projections(coords, method, painter) axes.format(suptitle=&#39;Correlated painting&#39;) . . All plots are shown in the rest frame of the circulating beam at a fixed position in the ring; new particles are injected at the intersection of the faint horizontal and vertical lines. The reason for the square root time-dependence was to create the uniform density ellipses in $x$-$x&#39;$ and $y$-$y&#39;$. The $x$-$y$ distriubtion is rectangular because of the unequal tunes. Interestingly, there are nonzero higher-order correlations between $x$ and $y$, as seen in the higher density cross. The distribution will not look exactly like this during real injection since space charge and other nonlinear effects will perturb the particle oscillations; however, this large peak density is not ideal. For example, at the Spallation Neutron Source (SNS), it is important to minimize the peak density of the beam when it collides with the liquid mercury target. Also, the electric field within the beam in the figure has a nonlinear dependence on the particle coordinates, leading to other undesirable effects. Therefore, the SNS uses correlated painting with an initial offset, creating a hollow beam that eventually fills in during accumulation. The final beam has a more uniform density. . Let&#39;s now look at anti-correlated painting. . method = &#39;anti-correlated&#39; coords = [painter.paint(nux, nuy, turns, method) for turns in turns_list] axes = plot_projections(coords, method, painter) axes.format(suptitle=&#39;Anti-correlated painting&#39;) . . Incredibly, although the distribution has a strange shape in the $x$-$y$ plane throughout injection, the end result is a uniform density ellipse. In fact, it is an approximate KV distribution in which particles are uniformly spaced on a 4D ellipsoid in phase space. Since the KV distribution linearizes the space charge force, this painting method seems very attractive. But when space charge is included during injection, the nonlinear forces at early times during injection will not preserve the KV structure. Nonetheless, anti-correlated painting can have benefits over correlated painting. . We have been studying an alternative painting method called elliptical painting. In elliptical painting, the injected coordinates are scaled along an eigenvector of the ring transfer matrix. . $$ mathbf{x}(t) = Re { sqrt{2 J_l} mathbf{v}_l e^{-i psi_l} } sqrt{t}. $$ Here $ mathbf{x} = (x, x&#39;, y, y&#39;)^T$, $ mathbf{v}_l$ is the eigenvector, $J_l$ is an amplitude, $ psi_l$ is a phase, $Re {z }$ selects the real component of $z$, and $l = 1,2$. This method takes advantage of coupled motion in the ring and generates a Danilov distribution, which has many of the same attractive properties as the KV distribution. The main difference is that it has angular momentum. . There is more to say about this method, but for now, let&#39;s just look at the distribution it creates in the linear approximation. One way to perform elliptical painting is by setting equal tunes in the ring. In this case, one eigenvector is $ mathbf{v} = (x_{max}, 0, 0, y_{max}&#39;)^T$. . method = &#39;correlated&#39; painter.set_xmax([1.0, 0.0, 0.0, 1.0]) coords = [painter.paint(nux, nux, turns, method) for turns in turns_list] axes = plot_projections(coords, method, painter) . . The big change is that we are now varying the angle between the beams — $y&#39;$ in this case — not just the distance. This produces rotation; the beam is rigidly rotating counterclockwise in the above figure. Again, the end result is a uniform density ellipse in the $x$-$y$ plane. But notice the difference from anti-correlated painting: the uniform density ellipse is maintained at all times. In fact, the Danilov structure is maintained at all times. For this reason, an approximate Danilov distribution would be produced even with space charge. In fact, realistic simulations predict that this would remain true even with realistic nonlinear effects. This is good news because the Danilov distribution has even more attractive properties than the KV distribution. We are in the midst of testing this prediction at the SNS. .",
            "url": "https://austin-hoover.github.io/blog/accelerators/plotting/2022/02/11/some_figures.html",
            "relUrl": "/accelerators/plotting/2022/02/11/some_figures.html",
            "date": " • Feb 11, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Tomographic reconstruction in four dimensions",
            "content": "Introduction . A beam of particles in an accelerator is characterized by its distribution in phase space — the space of positions $ {x, y, z }$ and momenta $ {x&#39;, y&#39;, z&#39; }$. I&#39;m currently working on a project in which we&#39;d like to measure the four-dimensional (4D) phase space distribution $f(x, x&#39;, y, y&#39;)$ of a fully-accumulated beam in the Spallation Neutron Source (SNS). . A direct way to do this is to use a series of slits to block all particles outside a small region $ mathbf{x} pm Delta mathbf{x}$, where $ mathbf{x} = (x, x&#39;, y, y&#39;)$, and move $ mathbf{x}$ through phase space. This method is accurate but slow and not available for our beam ($10^{14}$ protons moving at 90% the speed of light). The best we can do is to measure projections of the distribution; i.e., lower-dimensional views such as . $$ f(x) = int_{- infty}^{ infty} int_{- infty}^{ infty} int_{- infty}^{ infty}{f(x, x&#39;, y, y&#39;) dx&#39; dy dy&#39;} $$and estimate the distribution from these projections. This is called tomographic reconstruction. . Tomographic reconstruction is used in a wide variety of fields, particularly in medical imaging where X-rays are used to generate 1D projections of a 2D slice of organic material at various angles. The same idea can be applied to a phase space distribution. Consider $f(x, x&#39;)$. It&#39;s straightforward to measure a 1D projection along the $x$ axis by sweeping a vertical conducting wire across the beam path; the problem is that this only corresponds to one projection angle. The trick is to approximate motion in an accelerator as a series of linear transformations of the phase space coordinates: shearing, scaling, and rotation. Thus, the projection of a beam at one location onto the $x$ axis is a scaled projection of a projection onto a rotated axis at a different location. The projection angle can be controlled by varying the electromagnetic fields between the two locations. . It&#39;s more challenging to find $f(x, x&#39;, y, y&#39;)$. It&#39;s simple to reconstruct the $4 times 4$ covariance matrix from 1D projections; I&#39;ve implemented this method in the SNS and won&#39;t discuss it here. To obtain the 4D distribution, we generally need at least 2D projections. I&#39;m interested in this approach because the final destination of the SNS beam — the target — is coated with a luminescent material which gives the 2D projection onto the $x$-$y$ plane. The idea that I&#39;ll begin to explore in this post is whether these 2D projections can be used to reconstruct the 4D phase space distribution. I&#39;ll first go over the common reconstruction algorithms in 2D and mention if they could be extended to 4D. . Reconstruction algorithms in 2D . import numpy as np import matplotlib.pyplot as plt import proplot as pplt from skimage import transform . I&#39;ll use the following distribution to test the reconstruction algorithms. . a = 1.75 n = 300000 X = np.vstack([ np.random.normal(scale=[1.0, 1.0], loc=[0.0, 0.0], size=(n, 2)), np.random.normal(scale=0.6, loc=[+a, +a], size=(n//5, 2)), np.random.normal(scale=0.6, loc=[+a, -a], size=(n//5, 2)), np.random.normal(scale=0.6, loc=[-a, +a], size=(n//5, 2)), np.random.normal(scale=0.6, loc=[-a, -a], size=(n//5, 2)), ]) n_bins = 60 xmax = 5.0 limits = [(-xmax, xmax), (-xmax, xmax)] Z_true, xedges, yedges = np.histogram2d(X[:, 0], X[:, 1], n_bins, limits, density=True) xcenters = 0.5 * (xedges[:-1] + xedges[1:]) ycenters = 0.5 * (yedges[:-1] + yedges[1:]) fig, ax = pplt.subplots() ax.pcolormesh(xcenters, ycenters, Z_true.T) ax.format(xlabel=&quot;x&quot;, ylabel=&quot;x&#39;&quot;) plt.show() . We now simulate the measurements. Assume we have the ability to rotate the distribution without any shearing or scaling. We then rotate the distribution by different angles and project it onto the $x$ axis. It will turn out that measurement time limits the number of projections we can use; for now, we&#39;ll limit ourselves to 15 projections. . def rotation_matrix(angle): cs, sn = np.cos(angle), np.sin(angle) return np.array([[cs, sn], [-sn, cs]]) n_proj = 15 angles = np.linspace(0., 180., n_proj, endpoint=False) transfer_matrices = [rotation_matrix(np.radians(angle)) for angle in angles] projections = np.zeros((n_bins, n_proj)) for k in range(n_proj): M = transfer_matrices[k] X_meas = np.apply_along_axis(lambda row: np.matmul(M, row), 1, X) projections[:, k], _ = np.histogram(X_meas[:, 0], n_bins, limits[0], density=True) fig, ax = pplt.subplots() ax.pcolormesh(projections.T) ax.format(xlabel=&#39;Projection axis&#39;, ylabel=&#39;Projection number&#39;, ytickminor=False) . We&#39;ll also need some helper functions. . def apply(M, X): return np.apply_along_axis(lambda row: np.matmul(M, row), 1, X) def project(Z, indices): if type(indices) is int: indices = [indices] axis = tuple([k for k in range(4) if k not in indices]) return np.sum(Z, axis=axis) def normalize(Z, bin_volume=1.0): count = np.sum(Z) if count == 0.0: return Z return Z / count / bin_volume def get_bin_volume(limits, n_bins): if type(n_bins) is int: n_bins = len(limits) * [n_bins] return np.prod([((lim[1] - lim[0]) / n) for lim, n in zip(limits, n_bins)]) def process(Z, keep_positive=False, density=False, limits=None): if keep_positive: Z = np.clip(Z, 0.0, None) if density: bin_volume = 1.0 if limits is not None: bin_volume = get_bin_volume(limits, Z.shape) Z = normalize(Z, bin_volume) return Z def plot_rec(Z, Z_true, suptitle=&#39;&#39;): fig, axes = pplt.subplots(ncols=3, figsize=(7.25, 2.5)) axes[0].pcolormesh(xcenters, ycenters, Z.T, cmap=&#39;dusk_r&#39;) axes[1].pcolormesh(xcenters, ycenters, Z_true.T, cmap=&#39;dusk_r&#39;) axes[2].pcolormesh(xcenters, ycenters, (Z - Z_true).T, colorbar=True, colorbar_kw=dict(width=0.075, ticklabelsize=8)) axes.format(xticks=[], yticks=[], suptitle=suptitle) for ax, title in zip(axes, [&#39;Reconstructed&#39;, &#39;Original&#39;, &#39;Error&#39;]): ax.set_title(title) plt.show() . Filtered Back-Projection (FPB) . In filtered back-projection (FBP) each projection is Fourier transformed and then smeared back across the $x$-$x&#39;$ plane. The Fourier transform acts as a filter and creates a sharper image. . Z = transform.iradon(projections, theta=-angles).T Z = process(Z, keep_positive=True, density=True, limits=limits) plot_rec(Z, Z_true, &#39;FBP&#39;) . FBP generally requires a high number of projections to avoid these streaking artifacts. Let&#39;s see what happens when the number of projections is varied. In each case, we distribute the angles evenly over 180 degrees. . . We were also careful to choose an angular range close to 180 degrees to maximize the information carried by the projections. . . The angular range in an accelerator is determined by the amount of control we have over the optics between the reconstruction location and the measurement location. It&#39;s possible to run into problems here; for example, varying a magnet too far from its design value could make the beam unacceptably large at a downstream location. Magnets also have limited strengths, and sometimes they can&#39;t be controlled independently. Additionally, achieving a specific projection angle is not always straightforward; usually an optimizer is used to find the correct settings, and this takes time. I&#39;ll come back to these points later. . FBP can be generalized to 3D reconstruction from 2D projections, but it doesn&#39;t seem straightforward. I don&#39;t know if it could work for a 4D reconstruction from 2D projections. I&#39;m just not sure what back-projection would mean in that case... . Algebraic Reconstruction (ART) . Algebraic reconstruction (ART) algorithms are simpler than FBP. The reconstructed $x$-$x&#39;$ distribution will be defined on a grid. Let $ rho^{(k)}$ be a vector containing the $k$th projection, and let $ psi$ be a vector of the phase space density at the reconstruction location ($N^2$ elements for an $N times N$ grid). Since we know the linear transformation connecting the two points, we can write the following matrix equation: . $$ rho^{(k)} = P^{(k)} psi, $$ . Stack these equations for all the projections to get . $$ rho = P psi. $$ . All we need to do is invert these equations. The problem is that $P$ could be huge. If there are $K$ projections, $P$ will be have $K N times N^2$ elements. Inverting such a matrix could be a pain, so people have developed iterative methods to find the answer. Scikit-image has one ready to go called simultaneous algebraic reconstruction (SART). . Z = transform.iradon_sart(projections, theta=-angles).T Z = process(Z, keep_positive=True, density=True, limits=limits) plot_rec(Z, Z_true, &#39;SART (1 iteration)&#39;) . The SART algorithm looks like it does better with fewer projections. SART is known to produce a good answer in only one iteration. Running it again with the original reconstruction as an initial estimate can sharpen image but may increase noise. In this case, it looks like running SART again leads to a better reconstruction without increasing noise. . Z = transform.iradon_sart(projections, theta=-angles).T Z = transform.iradon_sart(projections, theta=-angles, image=Z.T).T Z = process(Z, keep_positive=True, density=True, limits=limits) plot_rec(Z, Z_true, &#39;SART (2 iterations)&#39;) . ART easily generalizes to 4D, but it could become expensive since $P$ would have $K N^2 times N^4$ elements. This has been carried out by Wolski (2020) with $N = 69$. I&#39;ll leave investigation of 4D ART for a future post. . MENT . There is another approach to the problem based on information theory. The idea is that the most probable distribution should be chosen, i.e., find the $f( mathbf{x})$ that maximizes the entropy . $$ S = - int f( mathbf{x}) log{f( mathbf{x})} d mathbf{x}$$ . with the constraint that the projections of $f( mathbf{x}$ are consistent with measurements. The theory behind this algorithm is layed out in Minerbo (1972). The benefit to MENT is that it has a goal in mind and can do very well with only a few projections where ART might introduce significant streaking artifacts. The downside of MENT is that although it can work well with few projections, there is no guarantee, and it may struggle to converge when many projections are used. Unfortunately, this algorithm is a bit tricky to implement; I&#39;m working on it. . MENT can also work in 4D with 2D projections or 1D projections; the math is pretty much the same. . Grid-based methods . There is a final method that I should mention: using multi-particle simulation to find the answer. Start with a bunch of particles and transport them to the measurement location. Throw away particles that landed in bins in which the measured projection is zero. Now, generate new particles in the viscinity of the old ones with the number of new particles proportional to the measured distribution in that bin. Repeat until convergence. . The advantage of this approach is that it works for any number of dimesions and can also include nonlinear effects in the simulation such as space charge. I&#39;m wondering, though, if this just reduces to a version of ART if the transport is linear; our beam is high energy, which means that space charge can mostly be ignored over short distances. Again, I&#39;ll leave this method for a future post. . 4D reconstruction . I&#39;ve mentioned that at least three methods (ART, MENT, grid-based) will generalize to 4D reconstruction from 2D projections. But what I&#39;d like to look at now is a different, hopefully easier method described in Hock (2013). The method uses 2D projections but only uses 2D reconstruction methods like those described in this post. It&#39;s sort of like a CT scan: 1D projections are used to reconstruct a 2D slice of a human body, and then the slice is moved along the body to reconstruct the 3D image. This would be nice because the conditions for a good 2D reconstruction are well-understood, but might not be clear for higher-dimensional versions of the algorithms. . First, we need a 4D distribution. . X = np.random.normal(size=(400000, 4)) X = np.apply_along_axis(lambda row: row / np.linalg.norm(row), 1, X) X[:, 3] = +X[:, 0] X[:, 1] = -X[:, 2] # Change the x-y phase difference. R = np.zeros((4, 4)) R[:2, :2] = rotation_matrix(np.pi / 4) R[2:, 2:] = rotation_matrix(0.0) X = np.apply_along_axis(lambda row: np.matmul(R, row), 1, X) # Add some noise. X += np.random.normal(scale=0.4, size=X.shape) # Plot the 2D projections. n_bins = 80 axes = myplt.corner(X, figsize=(6, 6), bins=n_bins) plt.show() # Store the limits for each dimension. limits = [ax.get_xlim() for ax in axes[-1, :]] labels = [&quot;x&quot;, &quot;x&#39;&quot;, &quot;y&quot;, &quot;y&#39;&quot;] . We can get the &quot;true&quot; distribution by binning the particles on a 4D grid. . Z_true, edges = np.histogramdd(X, n_bins, limits, density=True) centers = [] for _edges in edges: centers.append(0.5 * (_edges[:-1] + _edges[1:])) bin_volume = get_bin_volume(limits, n_bins) . Description of Hock&#39;s method . Now for the method. Suppose we can independently rotate the $x$-$x&#39;$ and $y$-$y&#39;$ projections of the phase space distribution. Let the angles in $x$-$x&#39;$ be $ left { mu_{x_1}, dots, mu_{x_k}, dots, mu_{x_K} right }$ and the angles in $y$-$y&#39;$ be $ left { mu_{y_1}, dots, mu_{y_l}, dots, mu_{y_L} right }$. For each combination of $ mu_x$ and $ mu_y$, we measure the beam intensity on a screen. In other words, we create a matrix $S$ such that $S_{ijkl}$ gives the intensity at point $(x_i, y_j)$ on the screen for phase advances (angles) $ mu_{x_k}$ and $ mu_{y_l}$. . K = 15 # number of angles in x dimension L = 15 # number of angles in y dimension muxx = muyy = np.linspace(0., 180., K, endpoint=False) xx_list = [] for mux in muxx: Mx = rotation_matrix(np.radians(mux)) xx_list.append(apply(Mx, X[:, :2])[:, 0]) yy_list = [] for muy in muyy: My = rotation_matrix(np.radians(muy)) yy_list.append(apply(My, X[:, 2:])[:, 0]) S = np.zeros((n_bins, n_bins, K, L)) for k, xx in enumerate(xx_list): for l, yy in enumerate(yy_list): S[:, :, k, l], _, _ = np.histogram2d(xx, yy, n_bins, (limits[0], limits[2])) . . The above animation shows a few steps in the scan; since the transfer matrices are rotation matrices, only the cross-plane correlation changes. . We can immediately reconstruct the 3D projection the 4D phase space distribution using this data. Consider one row of the beam image — the intensity along the row gives a 1D projection onto the $x$ axis for a vertical slice of the beam. We have a set of these 1D projections at different $ mu_k$ which we can use to reconstruct the $x$-$x&#39;$ distribution at this vertical slice using any 1D $ rightarrow$ 2D reconstruction method. I&#39;ll use SART since this seemed to work well with 15 projections. We repeat this at each slice. We thus have an array $D$ such that $D_{j,l,r,s}$ gives the density at $x = x_r$, $x&#39; = x&#39;_s$ for $y = y_j$ and $ mu_y = mu_{y_l}$. . D = np.zeros((n_bins, L, n_bins, n_bins)) for j in range(n_bins): for l in range(L): _Z = transform.iradon_sart(S[:, j, :, l], theta=-muxx).T _Z = transform.iradon_sart(S[:, j, :, l], theta=-muxx, image=_Z.T).T D[j, l, :, :] =_Z . I should also mention that the reconstruction grid doesn&#39;t need to be the same size as the measurement grid. We can now do a similar thing in the vertical plane. For each bin in the reconstructed x-x&#39; grid, D[:, :, r, s] gives the projections of $y$-$y&#39;$ at each $ mu_{y_l}$; thus, $y$-$y&#39;$ can be reconstructed at each $x_r$ and $x&#39;_s$, and we have an array $Z$ such that $Z_{r,s,t,u}$ gives the density at $x = x_r$, $x&#39; = x&#39;_s$, $y = y_t$, $y&#39; = y&#39;_u$. . Z = np.zeros((n_bins, n_bins, n_bins, n_bins)) for r in range(n_bins): for s in range(n_bins): _Z = transform.iradon_sart(D[:, :, r, s], theta=-muyy).T _Z = transform.iradon_sart(D[:, :, r, s], theta=-muyy, image=_Z.T).T Z[r, s, :, :] = _Z . Accuracy . ART could make some bins negative, so set those to zero. We then need to normalize Z for comparison with Z_true. . Z = process(Z, keep_positive=True, density=True, limits=limits) print(&#39;min(Z) = {}&#39;.format(np.min(Z))) print(&#39;max(Z) = {}&#39;.format(np.max(Z))) print(&#39;sum(Z) * bin_volume = {}&#39;.format(np.sum(Z) * bin_volume)) print() print(&#39;min(Z_true) = {}&#39;.format(np.min(Z_true))) print(&#39;max(Z_true) = {}&#39;.format(np.max(Z_true))) print(&#39;sum(Z_true) * bin_volume = {}&#39;.format(np.sum(Z_true) * bin_volume)) . min(Z) = 0.0 max(Z) = 0.16912337022978005 sum(Z) * bin_volume = 0.9999999999999988 min(Z_true) = 0.0 max(Z_true) = 0.9743877999183824 sum(Z_true) * bin_volume = 0.9999999999999872 . I&#39;m not sure the best way to quantify the difference between the distributions. My initial thought is to subtract Z_true from Z, take the absolute value, and divide by the number of bins; this would give the average absolute error over the bins. . avg_abs_err_per_bin = np.sum(np.abs(Z - Z_true)) / Z.size print(&#39;Average absolute error per bin = {}&#39;.format(avg_abs_err_per_bin)) . Average absolute error per bin = 0.0019781220132513776 . Lots of bins are empty so this might not be the most meaningful number. For now, it&#39;s probably more helpful to view the differences between the projections. . fig, axes = pplt.subplots(ncols=4, figsize=(6, 2), spanx=False) for i in range(4): axes[i].plot(centers[i], project(Z_true, i), color=&#39;black&#39;, label=&#39;True&#39;) axes[i].plot(centers[i], project(Z, i), color=&#39;red8&#39;, ls=&#39;dotted&#39;, label=&#39;Reconstructed&#39;) axes[i].set_xlabel(labels[i]) axes[0].legend(loc=(0.0, 1.02), framealpha=0.0, ncol=1); . . indices = [(0, 1), (2, 3), (0, 2), (0, 3), (2, 1), (1, 3)] fig, axes = pplt.subplots(nrows=6, ncols=3, figsize=(7.0, 12.5), sharex=False, sharey=False) for row, (i, j) in enumerate(indices): _Z_true = project(Z_true, [i, j]) _Z = project(Z, [i, j]) axes[row, 0].pcolormesh(centers[i], centers[j], _Z.T, cmap=&#39;dusk_r&#39;) axes[row, 1].pcolormesh(centers[i], centers[j], _Z_true.T, cmap=&#39;dusk_r&#39;) axes[row, 2].pcolormesh(centers[i], centers[j], (_Z - _Z_true).T, colorbar=True, colorbar_kw=dict(width=0.075)) axes[row, 0].annotate(&#39;{}-{}&#39;.format(labels[i], labels[j]), xy=(0.02, 0.92), xycoords=&#39;axes fraction&#39;, color=&#39;white&#39;) for ax, title in zip(axes[0, :], [&#39;Reconstructed&#39;, &#39;True&#39;, &#39;Error&#39;]): ax.set_title(title) axes.format(xticks=[], yticks=[]); . . In the second plot, the error is given in number of particles. It looks like the method worked well to reconstruct this distribution. . Feasibility . Target imaging system . The SNS target is a steel vessel containing liquid mercury. At the beginning of this post, I mentioned that the SNS has a target imaging system. Here is a diagram showing how the target imaging system works... . . ... and an example of an image. . . To reduce noise, the image was averaged over five beam pulses and a Gaussian blur was applied with $ sigma = 3$. There are four fiducial markers that are visible as dark spots on the corner of the beam. The black ellipse represents a measurement from wire-scanners upstream of the target. Collecting this image is easy. . Optics control . I&#39;ve assumed that the transfer matrices connecting the coordinates are simple rotation matrices. Although that isn&#39;t true in reality, there is a trick we can play. Any transfer matrix $M$ can be writen as the product $M = V R V^{-1}$, where $V$ provides shearing + scaling and $R$ is a rotation by the phase advances $ mu_x$ and $ mu_y$. Applying $V^{-1}$ to the coordinates is called normalizing the coordinates. What we can do is normalize (scale) the measured profiles, perform the reconstruction in normalized phase space, then unnormalize the reconstructed distribution. . In normalized phase space, the transfer matrices are just rotation matrices ($R$) and the projection angles are the phase advances. This is an advantage because the phase advances are easier to control than the true projection angles. Also, the reconstructed $x$-$x&#39;$ and $y$-$y&#39;$ projections will be somewhat circular in this space, which should reduce errors. . The (un)normalization steps will involve interpolation since we&#39;re performing a transformation on an image, and I&#39;m not yet sure how this will affect the reconstruction error. Anyhow, we don&#39;t have to reconstruct in normalized phase space if this turns out to be an issue. . So, the question is whether we can independently vary the $x$ and $y$ phase advances at the target, and if so, by how much. There are five independent quadrupoles before the target — three focusing and two defocusing — QH26, QV27, QH28, QV29 and QH30. Before that, there are eight quadrupoles — four focusing and four defocusing — that share two powers supplies. One power supply controls QH18, QH20, QH22, and QH24, and another controls QV19, QV21, QV23, and QV25. . There are also a constraints. First, the beam size far away from the target should remain small. We measure the beam size using the $ beta$ function, which should remain below 35 [m/rad]. The beam becomes much larger close to the target; there, the $ beta$ function should remain below 100 [m/rad]. Finally, the SNS target has tight constraints on the beam size and the beam density. It&#39;s safest to keep the $ beta$ functions within 10 or 20 percent of the design value. . I plugged all this into an optimizer and asked to vary the phase advances in a 180 degree range. After fideling with the starting and ending phases, I was able to more-or-less do this. I used 15 phase advances in $x$ and 15 phase advances in $y$, just like in this post. Each optimization took around 16 seconds, so the script took around an hour to run. Below are the computed phase advances at each step as well as the strengths of the seven magnet power supplies. . . There are a few steps near the end of the scan in which the phase advances aren&#39;t exactly correct, but they are close. I can always decrease the phase coverage slighly, or maybe these steps just need a slightly different initial guess. Here are the beta functions for all steps in one plot. The plot ends at the target. . . The bottom plot is just the integral of the inverse of the top plot. The beta functions remain within their limits. . Outlook . Initial tests of Hock&#39;s 4D reconstruction method are promising. There are a number of things to investigate. Ideally, the minimum number of projections is used. First, I should play with the number of projections using ART to see how the reconstruction error scales. I would also like to implement MENT, which should do better with fewer projections. I should then test the &quot;direct&quot; 4D reconstruction methods such as ART, MENT, or grid-based and compare them to Hock&#39;s method. I should also investigate how the methods respond to noise, changes in the grid size, and more complex distributions. . The script to perform the data collection for this method is pretty simple: compute the correct optics, update the live magnet strengths accordingly, send a beam to the target, and save the target image. The biggest issue is execution time. In our next study at the SNS, we&#39;ll have plenty of time to run the script in addition to our other tasks. Hopefully this will be a helpful diagnostic. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/tomography/2021/10/16/tomographic_reconstruction_in_four_dimensions.html",
            "relUrl": "/physics/accelerators/tomography/2021/10/16/tomographic_reconstruction_in_four_dimensions.html",
            "date": " • Oct 16, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Arguing About Gods (part 3a)",
            "content": "1. Introduction . These are my notes on chapter 3 of Oppy’s Arguing About Gods. This chapter is about cosmological arguments. These arguments conclude that there is a cause or explanation of the existence of the universe. . The first step in a cosmological argument is to assert a Causal Principle (CP) — that all things of a certain kind require a cause — or a Principle of Sufficient Reason (PSR) — that all things of a certain kind require an explanation. The next step is to show that the universe fits within the scope of the chosen CP or PSR. A bonus step is to identify the cause or explanation of the universe as God. . These arguments raise difficult questions about time, causation, infinity, physics, etc., and it’s not realistic to have a firm grasp on all these topics without further study. My goal here is to create a sort of roadmap, identifying the main points of contention. . Pruss notes four main hurdles for cosmological arguments: . Glendower Problem: How wide should the scope of the CP or PSR be? | Regress Problem: Which types of infinite regresses are possible? | Taxicab Problem: What caused the first cause? | Gap Problem: How does one identify the first cause with God? | . The arguments can be classified according to how they address these problems. The Kalam argument uses a CP to rule out an infinite past; the Thomistic argument allows for an infinite past but uses a CP to rule out infinite “vertical” causal chains; the Leibnizian argument allows for infinite causal chains but uses a PSR to demand an external explanation for such chains. I decided to dedicate one post to each of these arguments, with a final post on the Gap Problem. . We’ll start with the Kalam, which is simply expressed: . Everything that beings to exist has a cause. | The universe began to exist. | Therefore, the universe has a cause. | 2. The universe began to exist . Let’s say we’re undecided about whether the past had a beginning. We might ask modern physical theories for input on this question. We might also think “from the armchair” about whether a beginningless past is even possible. We’ll start with the former approach. . 2.1. Cosmology . Consider the Standard Model (SM) of cosmology: with the assumption of an isotropic homogeneous mass distribution, general relativity produces an expanding universe solution. The solution diverges as $t rightarrow 0$, where $t = 0$ is some finite time in the past. This implies that the universe expanded from a very dense state at a finite time in the past. The SM is empirically supported for times sufficiently far from $t = 0$. (The spectrum of leftover radiation from the early universe, the abundances of the light elements, and the measured Hubble Constant all agree with SM predictions.) . Things are less clear as $t rightarrow 0$. The singularity predicted by general relativity is taken by many to be unphysical. Quantum effects are expected to be important at these scales, and there is currently no complete theory of quantum gravity and no known way to experimentally test such a theory. Thus, the SM is undecided on whether the universe began to exist. . Still, we can speculate about which model of the early universe is most probable. Some of them support an infinite past and others do not. Since I’m not a cosmologist, I don’t understand these models in depth. It does seem that classical physics gives some indication that the past is finite (see BGV theorem), but future physics may turn the tide. Thus, we should be cautious when using physics to support a strong claim about whether the universe began to exist. . 2.2. Finitism . 2.2.1. Hilbert&#8217;s Hotel . We now move to philosophical arguments against an infinite past. The first view to discuss is finitism, the idea that infinity never shows up in the real world. The benefit of finitism is that it rules out some oddities like Hilbert’s Hotel — a hotel with infinitely many rooms. Even if all the rooms are occupied, the hotel can always accept an additional guest. If all guests in odd-numbered rooms leave, of which there are an infinite number, then an infinite number of guests remain. If all guests in room numbers &gt; 7 leave, of which there are an infinite number, then exactly 7 guests remain. . The question is whether this story precludes the existence of Hilbert’s Hotel. It’s not clear that it does; it might just describe the strange rules the hotel would obey if it existed. The key issue is that a subset of an infinite set is another infinite set. Since the operations done on Hilbert’s Hotel can be done on the natural numbers as well, while Hilbert’s Hotel might be strange, it’s not clear why the reasons for ruling out its possibility wouldn’t also apply to infinite mathematical sets. . 2.2.2. An endless future? . Showing that a completed infinite is impossible to instantiate is only helpful to the Kalam if the series of events in a beginningless past is a completed infinite. The answer to this question depends on the relationship between the past, present, and future. In my understanding, there are three main views: all times exist (four-dimensionalism), present times exists (presentism), or past and present times exist (growing-block). The set of future events in an endless future would form an actual infinite on four-dimensionalism, which would then be ruled out by finitism, which is bad. The Kalam gains nothing on presentism since there is only ever one time that exists. I’m not sure which theory of time is correct, but I would initially lean toward presentism or four-dimensionalism. . There is causal asymmetry between an infinite future and an infinite past. An infinite past allows an infinite number of past events to affect the state of the world at a given time, while there is no such problem in an infinite future. (This will be discussed in the next section on causal finitism.) . 2.2.3. Counting to infinity . Suppose finitism is false. The following argument could then be run: (i) the collection of temporal events is formed by successive addition; (ii) a collection formed by successive addition cannot be an actual infinite; (iii) the temporal series of events cannot be an actual infinite. . The idea is that getting to “now” in a beginningless universe is like traversing an infinite set, which is an impossible task like counting all the negative integers: …, -3, -2, -1, 0. And this counting example raises a question: suppose I counted all the negative integers; why did I finish when I did? I should have finished an infinite time ago. . These counting tasks assume there was a time at which I began counting, i.e., counting all the positive integers starting from zero. But there was no time at which I began counting. I counted 0 today, -1 yesterday, etc. So, it seems to be a coherent story. Yet even though it’s a coherent story, it feels uncomfortable to be left with this infinite regress of explanations. This kind of consideration is central to the Leibnizian argument. . 2.3. Causal finitism . Causal finitism is the idea that every event has a finite causal history.1 Causal finitism’s advantage over finitism is that it doesn’t touch abstract mathematical objects since they can’t cause anything. The case for causal finitism given by Pruss in Infinity, Causation, and Paradox is that causal finitism provides a unified way to kill a wide range of paradoxes; this section looks at a few of these. . 2.3.1. Grim Reapers . Thompson’s lamp is off at t = 0. I turn it on at t = ½, off at t = ¾, on at t = 7/8, and so on until t = 1. Is the lamp on or off at t = 1? It seems there is no way to answer this question. But although this situation is strange, it’s hard to get a real paradox without appeal to a PSR. . The Grim Reaper paradox is more troublesome. I’m alive at $t = 0$ along with an infinite number of sleeping Grim Reapers (GR). Each GR has an alarm set to a time $0 le t le 1$; when a GR’s alarm goes off, it wakes up and kills me if I’m alive, otherwise it goes back to sleep. . Let’s label the alarm time for GR $n$ as $t_n$, where $n$ can be any natural number. Suppose $t_n = 1 / 2^n$. I’ll clearly be dead at all $t &gt; 0$: I couldn’t be alive at $t = 1$ because GR 1 would have already killed me, I couldn’t be alive at $t = 1/2$ because GR 2 would have already killed me, and so on. But none of the GRs killed me: for each GR that could have killed me, there was always a GR that came before. Since my well-being at $t = 1$ is caused by infinitely many GRs, causal finitism kills the paradox. . One way to resolve the paradox is to say that the sum of the GRs killed me; however, the sum of all GRs doing nothing is nothing. We could also say that my death was uncaused; this will be discussed with the first premise of the Kalam. Or we could say that time is discrete. (If time is discrete, the paradox can remain alive if the GRs are spread out at equal intervals into the eternal past: t = -1, t = -2, etc. Again, no GR killed me, but I must be dead at t = 0. The paradox is a bit different since there is no time at which I was alive. Beginningless sequences such as these will discussed with the Leibnizian argument.) . The best way to resolve the paradox without causal finitism is the Unsatisfiable Pair Diagnosis (UPD). The UPD basically says that the situation is impossible because it leads to a paradox. There are two conditions: (A) there is a beginningless sequence, and (B) E occurs at n iff E has not occurred before n. We’re then claiming that A and B can&#39;t both be true at the same time. One example is that “Austin is taller than Paris” and “Paris is taller than Austin” could be true individually but not together. . The crucial question here is whether it’s possible to reach the GR scenario from nearby unparadoxical scenarios; this is known as rearrangement. For example, there is no problem if $t_n = 1 – 1 / 2^n$ since I would die at t = 0.5 and remain dead, and if the alarms could be set to these values, why not the original scenario? Or the original scenario could be modified by adding a GR at $t_0 le 0$: I would be killed by GR 0 at $t_0$. In this case, we only need to remove GR 0 to get back to the paradox. The proponent of the UPD is going to have to call into question the possibility of rearrangement in these cases. . 2.3.2. Newtonian universes . Newtonian physics is false, but it seems there would be nothing inconsistent about a world that obeys Newtonian physics. For example, space could be infinite in extent and an infinite number of particles could collectively cause the motion of another particle, violating causal finitism. An example is a collection of particles spread evenly over an infinite plane. But variation of the initial conditions leads to bizarre results. Pruss uses the example of particles spread evenly over “half” of an infinite plane: the force on a particle on the edge of the distribution will be infinite, so it will literally be nowhere as soon as any time passes. There are other fun examples. Causal finitism obviously kills these paradoxes. . 2.3.3. Infinite lotteries . An infinite (fair) lottery is a lottery with an infinite number of tickets, each of which has zero or infinitesimal chance of winning. The claim is that infinite lotteries are absurd, but possible on causal infinitism. . Let’s start with the absurdity of an infinite lottery. We can label each ticket with a natural number. Let’s say I draw ticket N but don’t look at the number. Then, for each natural number $n$, I guess whether $N &gt; n$. I get a dollar if I’m right, but I lose a dollar if I’m wrong. I should always guess that $N &gt; n$, but obviously I’ll lose an infinite number of times with this strategy. . Or suppose there are $10^{10^10}$coins flipped and I’m asked to guess whether any coins came up heads. Before I guess, I’m also given a random number $n$. If any of the coins came up tails, $n$ was generated from an infinite fair lottery; if none of the coins came up tails, $n$ was generated from a lottery in which the probability of drawing $n$ is $p_n = 1 / 2^n$. Since $1 / 2^n$ is always larger than an infinitesimal, I should always guess that $n$ didn’t come from the infinite lottery, and hence that no coins came up heads. . It’s also possible to raise the winning probability of every ticket by replacing the infinite fair lottery with the $p_n = 1 / 2^n$ lottery. There are other examples. . The next claim to investigate is whether causal finitism can rule out infinite lotteries. There is a technical section in the book related to this question, but here I’ll mention the simplest case: random walker Bob. For every day in a beginningless past, Bob takes one step in a randomly chosen direction — left or right — and writes down his position on a piece of paper. On a random day, Bob writes also writes “winner” on the piece of paper. Thus, an infinite fair lottery has been generated when Bob arrives at today. (The lottery is fair because the probability of any given position being the winner is infinitesimal.) Causal finitism rules out this story because the position of Bob at any point in the story is caused by an infinite number of previous positions. . The most promising way to kill the paradoxes without causal finitism is to note that human reasoning shouldn’t be expected to work with infinities; that’s fine, but causal finitism might be more attractive because it can also rule out paradoxes that don’t involve human reasoning. This whole discussion is pretty mathy and would take some time for me to understand it well. . 2.3.4. Decisions . Every minute in a beginningless past, a die is rolled. On each roll, I’m asked to guess whether the die landed on four. The penalty for a wrong answer is an electric shock. Obviously, I should always guess “no”. Now suppose I have access to the infinite number of previous rolls, that I know the game will end at some point, and that only finitely many non-fours have been rolled in the past. I now know with certainty that there will only be finitely many non-fours rolled for the rest of the game, so I can guess “yes” from now on and ensure a finite number of total shocks as opposed to the infinite number of total shocks I would normally receive. The paradox is that each roll is independent, so knowledge of previous rolls shouldn’t improve your future guesses. Causal finitism wouldn’t allow decisions to be made based on an infinite number of previous rolls. . 2.3.5. The Axiom of Choice . [There is a chapter in the book devoted to paradoxes involving the Axiom of Choice. I’ve avoided this chapter until now because it looks like it would take some time to digest, but maybe I will read it at some point and replace this bracketed text with a short summary.] . 2.3.6. Summary . There are several issues to resolve in relation to causal finitism. First, if it were true, it might raise the probability that spacetime is discrete. Second, its usefulness for killing paradoxes will depend on the nature of causation. . Causal finitism leads to the existence of at least one uncaused cause — just trace each causal chain back to its origin. (It does allow for an infinite past in which different regions of an infinitely large universe are causally isolated; however, each of these isolated regions would need a first cause.) The details of the connection between causal finitism and the second premise of the Kalam are explored by Koons in his article linked at the bottom of the post. What remains is the Gap Problem: what is the nature of the first cause(s)? . Okay, to my knowledge, those are some of the main arguments for the second premise of the Kalam. . 3. Everything that begins to exist has a cause . The first premise of the Kalam says that everything that begins to exist has a cause. I’d like to accept this Causal Principle (CP). It’s intuitive and seems to be supported by empirical evidence. Oppy pushes back in two ways. First, the intuitive support for the premise can be questioned. There is much debate about causation; some models take causation to be fundamental to physics, others see causation as a useful fiction, and others do away with it altogether. For example, while causation looks to be fundamental when billiard balls collide, it’s not so clear in the case of interacting quantum fields. (My own intuition tells me that causation is fundamental to physics.) . Second, the empirical support for the CP can be questioned. The pushback is that we only observe things that began to exist at $t &gt;0$, while the universe began to exist at $t = 0$ (if there was a first moment of time), and that we can’t generalize observations from non-initial times to the initial time. I understand this worry, but I also think that the empirical support for the CP is strong enough that I could just treat the CP as a default rule unless I have strong reasons to think that it’s false in a certain case. In this case, I don’t see an obvious reason why the collection of things (the universe) violates the CP while each individual thing does not. . Now, one might offer some reason why the collection of things (the universe) violates the CP while each individual thing does not. For example, if time began, then the universe didn’t “pop into existence”; instead, it always exists in the sense that it exists at every time. I think what people are getting at here is the idea that there could be no thing external to or prior to the universe, and since the cause of the universe would be external to it and causally prior to it, it is meaningless to ask what caused the universe. This feels a bit tricky to me, though. The beginning of the universe is an even in time, and of every event we can ask its cause. So, if the universe began to exist, then the beginning of the universe was either an uncaused event or it was a caused event. . 4. Conclusion . There are difficult subjects at play in the Kalam cosmological argument like causation, infinity, and cosmology. I’m tempted to refuse to evaluate the argument I have a better handle on these subjects, but I should probably evaluate the premises to the best of my ability right now, else risk never finishing this book. . The universe began to exist. . Modern cosmology is undecided on this premise. | The philosophical arguments given by Craig aren’t super convincing. | Some version of causal finitism is probably true. | It’s strange to imagine that space and time began to exist, but its less strange than an infinite past. | Evaluation: Probably true. | . Whatever begins to exist has a cause. . It’s intuitive. | I have no strong reasons to abandon this when it comes to the universe, but does anything change when something begins to exist at the beginning of time as opposed to some later time? | The principle “from nothing, nothing comes” is valid. | It might be more helpful to talk about the PSR and CP together. | Evaluation: Probably true, but need to think more about edge cases (quantum mechanics, free will). | . 5. References . These articles helped me while thinking about the Kalam argument. . Causality Aristotle On Causality (SEP) | Causation as Folk Science (Norton) | Caustion in Physics (SEP) | . | Cosmology The case for the relativistic hot Big Bang cosmology (Peebles et. al.) | Philosophy of Cosmology (SEP) | Philosophy and Theology (SEP) | . | General Cosmological Argument (SEP | Causation and Sufficient Reason (Leon) | Infinity (SEP) | . | Causal finitism Infinity, Causation, and Paradox (Pruss) | The Form of the Benardete Dichotomy (Shackel) | The Grim Reaper Kalam Argument (Koons) | Supertasks (SEP) | Yablo’s paradox and beginningless time (Luna) | . | Craig&#39;s Kalam Craig on the Actual Infinite (Morriston) | Craig’s Contradictory Kalam: Trouble at the Moment of Creation (Wielenberg) | No Trouble: A Reply to Wielenberg (Craig) | . | . . 1. Causal finitism also rules out an infinite number of causes all acting at the same time.↩ .",
            "url": "https://austin-hoover.github.io/blog/theism/cosmological%20arguments/2021/09/15/arguing_about_gods_3a.html",
            "relUrl": "/theism/cosmological%20arguments/2021/09/15/arguing_about_gods_3a.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Arguing About Gods (part 2)",
            "content": "These are my notes on chapter 2 of Oppy’s Arguing About Gods. This chapter is about ontological arguments — arguments that seek to demonstrate the existence of God by appealing to reason alone. The first and most famous ontological argument for the existence of God was presented by Anselm in 1078 in his work, Proslogion (translated: Discourse on the Existence of God). Since then, many different philosophers have addressed Anselm’s argument, either attacking it, defending it, or reformulating it. This will be a short post; whether ontological arguments succeed is not very important to me. . Anselm&#8217;s argument . Here&#39;s the argument: . There is, in the understanding at least, a being than which no greater being can be conceived. | If it is even in the understanding alone, it can be thought to be in reality as well. | Which would be greater. | (Therefore) There exists, both in the understanding and in reality, a being than which no greater being can be thought. | One assumption is that an object&#39;s greatness can depend on whether it exists or not. To me, it doesn’t make sense to compare the greatness of an existing thing and a non-existing thing. For example, let’s say an unwritten book $B$ exists only in my understanding. Maybe the greatness of a book is a function of the words written on its pages, its length, its binding, etc. Now consider $B’$, the physical realization of $B$. Is $B’ &gt; B$? I think not; when I thought of $B$, I thought of all the properties it would have if it were to exist. . One might also take issue with (1), claiming that infinity is not an understandable concept, at least not in the way Anselm intends. (I’m not sure what I think about that.) But the most obvious problem, as noted by Oppy, is that we should be able to entertain the possibility of a thing without being committed to the existence of that thing. It appears we’ve presented a version of God in which, by the very definition of God, God exists. And that’s not very helpful in determining whether God exists. . Anselm’s argument is still philosophically interesting, though. Consider the following parodies: . Gaunilo’s Parody (The Perfect Island) . There is, in the understanding at least, an island than which no greater island can be thought. | If it is even in the understanding alone, it can be thought to be in reality as well. | Which would be greater. | (Therefore) There exists, both in the understanding and in reality, an island than which no greater island can be thought. | The Devil Corollary . There is, in the understanding at least, a being than which no worse being can be thought. | If it is even in the understanding alone, it can be thought to be in reality as well. | Which would be worse. | (Therefore) There exists, both in the understanding and in reality, a being than which no worse being can be thought. | The No-Devil Corollary . There is, in the understanding at least, a being than which no worse being can be thought. | If it exists in the understanding and in reality, it can be thought to exist in the understanding alone. | Which would be still worse. | (Therefore) There does not exist in reality a being than which no worse being can be thought. | The Extreme No-Devil Corollary . Suppose there is, in the understanding at least, a being than which no worse being can be thought. | If it exists in the understanding, then it is possible that it not exist in the understanding. | Which would be still worse. | (Therefore) There does not exist in the understanding a being than which no worse being can be thought. | The most well-known is the Perfect Island parody. There is a difficult-to-read discussion in the book about the validity/non-validity and logical structure of each parody, and about the relationships between the parodies and Anselm’s argument; for example, could there be such a thing as a maximally great island in the same way that there could be a maximally great being? I won’t summarize that discussion here; I just wanted to mention the parodies because they’re fun to think about. . The modal ontological argument . There’s a more recent ontological argument for the existence of God that is much stronger at first glance. First, some vocabulary. A possible world is a way things could be. In other words, a possible world is the conjunction of a bunch of propositions: ($p_1$ &amp; $p_2$ &amp; $ dots$). For instance, (“dogs bark” &amp; “cats meow”) is true in one possible world, and (“dogs meow” &amp; “cats bark”) is true in a different possible world. This notion of possible worlds is just a useful tool to talk about possibilities — if $p$ is contingently true, then $p$ is true in some, but not all, possible worlds; if $p$ is necessarily true, then p is true in all possible worlds; if $p$ is possibly true, then $p$ is true in at least one possible world. The modal ontological argument attempts to use these ideas, along with the definition of God as a necessary being, to show that God exists: . If God exists, then God exists in all possible worlds. (God would exist necessarily.) | God exists in some possible world. (It’s possible that God exists.) | If God exists in some possible world, then God exists in all possible worlds. | If God exists in all possible worlds, then God exists in the actual world. | Therefore, God exists. | Premise (1) is just a definition of necessary existence, and (3) follows from the idea that “possibly necessary” is the same as “necessary”. There are questions about what it means for a person to accept (2). If I say, “I could be wrong, but I don’t think God exists…” am I granting that there’s a possible world in which God exists? I don’t think so. Rather, to accept (2), I need to grant the metaphysical possibility of the existence of God. So, perhaps the argument doesn’t do much work: if I accept (2), then I likely already think that God exists in the actual world! Nonetheless, I didn’t see any obvious problem with the argument on the first pass. The issue is the following parallel argument: . If God exists, then God exists in all possible worlds. (God would exist necessarily.) | God exists in some, but not all possible worlds. (It’s possible that God does not exist.) | If God exists in some, but not all, possible worlds, then God exists in no possible worlds. | If God exists in no possible worlds, then God does not exist in the actual world. | Therefore, God does not exist. | I’m not aware of a way to break this symmetry. . Other arguments . There are a couple of ontological arguments presented by Oppy that I had never heard of. One is the Mereological Ontological Argument, which argues that there is a unique thing of which everything is a part, and then calls this unique thing God. Although it seems to be a fine argument, the resulting being is just equivalent to the universe, and hence doesn’t fit very well with traditional monotheistic God. Another argument is from Kurt Gödel. This argument is logically valid, but I wasn’t sure of exactly what was meant by a few of the axioms; therefore, I don’t have many thoughts on this argument at the moment. . Conclusion . Skimming through the Proslogion, it’s clear that it’s not a philosophical journal article; rather, it’s a meditation at some points and a prayer at others. The meditations are mostly about how the various properties of God — such as omnipotence — could be realized. At the very least, Anselm’s ontological argument makes us think about the greatness of God and whether this greatness could even be conceived by us. And this seems to fit well with the rest of his work, even if it’s not a convincing argument for the existence of God. .",
            "url": "https://austin-hoover.github.io/blog/theism/ontological%20arguments/2021/07/05/arguing_about_gods_2.html",
            "relUrl": "/theism/ontological%20arguments/2021/07/05/arguing_about_gods_2.html",
            "date": " • Jul 5, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Space charge resonances and instabilities",
            "content": "I thought it would be interesting to create some animations based on a few of the discussions in the book Space Charge Physics for Particle Accelerators by Ingo Hofmann. The scripts I used can be found here. . Incoherent space charge effects . Hofmann discriminates between incoherent effects that involve a single particle and coherent effects that involve all particles in the beam. Nonlinear resonances driven by higher-order magnetic multipoles are examples of incoherent effects. Expanding the transverse magnetic field in a power series results in the addition of nonlinear terms to the equation of motion: . $$ x&#39;&#39; + k(s)x = sum_{i,j=1}^{ infty}{a_{i, j}x^i y^j}. $$ . The coefficients $a_{i,j}$ may be periodic — a single magnet error in a ring, for example. To avoid resonant behavior, the individual particle tunes $ nu_{0x}$ and $ nu_{0y}$ should be carefully chosen to avoid the lines defined by $M_x nu_{0x} + M_y nu_{0y} = N$. Space charge can have the negative effect of decreasing the tunes such that they approach these lines. This is the primary concern in circular accelerators. . It’s also possible for the beam itself to provide these higher-order terms through its electric field. Consider a matched beam — one whose density profile repeats itself after one lattice period —and suppose we track a particle through the external focusing fields and the beam’s electric field without affecting the beam (this is an approximation). The beam’s electric field may be able to be expanded in an infinite series, and the periodicity of the coefficients in this expansion will lead to additional resonance lines that the particle should avoid: $M_x nu_{x} + M_y nu_{y} = N$, where $ nu_x$ and $ nu_y$ are the depressed tunes. This is called an incoherent space charge resonance. . Hofmann simulates the case $4 nu_x = 4 nu_y = 360$ deg. using a PIC simulation. An evenly spaced FODO lattice is used with zero-current tunes $ nu_{0x} = nu_{0y} = 100$ deg. in both planes. The initial beam is a Gaussian distribution truncated at three standard deviations, the emittances are the same in both planes, and the rms beam dimensions are matched to the lattice using the KV envelope equations. Hofmann uses an elliptical longitudinal distribution, but I’ll use a uniform longitudinal distribution with no energy spread (coasting beam). The beam intensity is then chosen so that the depressed phase advances are $ nu_x = nu_y = 92$ deg., just a bit above the resonance condition.1 The simulation proceeds by slowly decreasing the zero-current phase advances: $100$ deg. $ rightarrow$ $90$ deg. over 500 cells. In this way, the core of the beam should remain approximately matched to the lattice. Below is the evolution of the horizontal phase space projection as calculated by PyORBIT. . &lt;/input&gt; Once Loop Reflect Also shown is the emittance — the rms area in phase space — which is calculated as . $$ varepsilon_x = sqrt{ langle{x^2} rangle langle{x’^2} rangle - langle{xx’} rangle^2},$$ . where $ langle dots rangle$ means the average over all the particles. There are four “islands” that have developed: this is characteristic of a fourth-order resonance. Based on the smooth change in the rms beam sizes and emittances, we can claim that the beam is well-matched and that this is an incoherent effect. It should be noted, though, that the distinction between incoherent and coherent is not always clear. . Coherent space charge effects . Coherent effects involve all particles in the beam and demand a self-consistent treatment. They’re generally difficult to analyze apart from simulation; however, the KV distribution does offer some insight. A well-known paper from 1983 — also by Hofmann — explores the stability of the KV distribution in a FODO lattice. The strategy was to start from the Vlasov equation, perturb the distribution function, and check for unbounded growth. The following plots were generated: . . Each curve is for a specific value of the zero-current phase advance; the plot then shows the depressed phase advance as a function of the beam intensity. The thicker parts of the curves represent regions of instability called stop bands. . Envelope instability . We can think of an instability of order $n$ being correlated with an $x^n$ term in the space charge potential. Let&#39;s first look at the second-order instability, commonly called the 1:2 envelope instability since it occurs when the beam envelope oscillates at twice the single-particle frequency. Since it depends on an $x^2$ term in the space charge potential, this instability should show up in the envelope equations. For example, I chose a zero-current phase advance of 100 degrees and a depressed phase advance of 85 degrees, which is within a stopband. The following plot shows the rms beam sizes at the end of each cell obtained by integrating the envelope equations. . The beam is perfectly matched for around 120 cells, after which the mismatch oscillations ensue. It’s important to note that the charge density remains uniform. The phase advances in real linacs are generally kept below 90 deg. to avoid this envelope resonance. . Higher-order terms . What about the third-order and fourth-order instabilities? Technically, these just don’t appear in the KV distribution. But simulations always involve a finite number of particles, so there will be small non-uniformities in the beam density. These can grow over time, so the simulated KV distribution is affected by these higher-order instabilities. Setting the zero-current phase advance to 90 deg., a third-order and fourth-order instability should appear at a depressed phase advance of 45 degrees and 30 degrees, respectively, according to the stopband diagram. Here is the evolution of the horizontal phase space in these two cases for an initial KV, Waterbag, and Gaussian beam. (if H is an invariant of the motion without space charge, the distribution functions $f$ take the form: $f_{KV}$ ~ $ delta(1 - H)$, $f_{WB}$ ~ $ Theta(1 - H)$, $f_{Gauss}$ ~ $ exp(-H)$, where $ Theta$ is the step function.) . &lt;/input&gt; Once Loop Reflect The order of the resonance is apparent in the symmetry of the phase space distribution (the number of “branches” extending from the core). Here is the emittance growth in each case: . The KV beam is the most violently affected by space charge. This is primarily because there is no tune spread in the KV distribution; thus, any resonance condition is going to affect the entire beam. A Gaussian, on the other hand, has a large tune spread; this tends to “wash out” instabilities because they only affect a fraction of the beam particles. The Waterbag distribution is somewhere in-between. These higher-order KV instabilities are not assumed to be dangerous since real beams are approximately Gaussian due to many other nonlinear processes. . . 1. I’m using the phase advance obtained from the KV envelope equations.↩ .",
            "url": "https://austin-hoover.github.io/blog/physics/space%20charge/simulation/resonances/2021/07/01/space_charge_instabilities.html",
            "relUrl": "/physics/space%20charge/simulation/resonances/2021/07/01/space_charge_instabilities.html",
            "date": " • Jul 1, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Arguing About Gods (part 1)",
            "content": "Introduction . I’m currently reading the book Arguing About Gods by Graham Oppy, one of the most prominent philosophers of religion. Oppy’s main thesis is that there are no successful arguments for the existence of orthodoxly conceived monotheistic gods, where “orthodoxly conceived monotheistic god” refers to an omnipotent, omniscient, omnibenevolent creator of the world — I’ll take “God” to mean the same thing — and “successful arguments” will be defined later. The book is organized as follows: . Preliminary Considerations | Ontological Arguments | Cosmological Arguments | Teleological Arguments | Pascal’s Wager | Arguments from Evil | Other Arguments | Concluding Remarks | I’m planning to write a post for each of these chapters. In each post, I’ll try to summarize Oppy’s ideas, discuss known challenges to these ideas if I’m aware of them, and record my thoughts and/or questions as they arise 1. This post will focus on the first chapter. . Before I begin, I should mention what I hope to achieve during this study. My general goal is to think more carefully about each of the following questions (G stands for the question of the existence of God): . How, and on what basis, do I answer G? | How confident am I in my answer to G? | Could my answer to G change in the future? | What is the relationship between my answer to G and my other beliefs? | Do I — or should I — treat G in the same way as I treat other questions? Why or why not? | How should I understand disagreement about G between equally intelligent people? | What role should internal evidence — intuitions, feelings, things I just take to be true — play when forming an answer to G? | . I’ll briefly give my current thinking on “How confident am I in my answer to G?”. A helpful concept discussed in the book and many other places is that of credence: my credence x in a proposition S is a value ranging from 0 to 1 which represents how confident I am that S. This contrasts with the three traditional responses: belief, disbelief, or withheld judgement. But perhaps it’s too difficult to choose a specific value for x; maybe it’s better to choose a credence interval r = (p, q) such that x is somewhere within this interval; perhaps r = (0.9, 1) if I’m very confident, r = (0, 0.1) if I’m not very confident, etc 2. It’s also still possible to choose a zero-width interval like r = (0.33, 0.33). If S = “God exists”, then the atheist’s credence interval may be bounded by (0, 0.5), the theist’s by (0.5, 1), and the agnostic’s by (0.5, 0.5). . My inclination is to make r quite narrow and centered on 0.5; I don’t think this is a good or bad thing — it’s just how my brain works. I’d be tempted to call this agnosticism. My actual credence interval, however, might be something like (0.45, 0.65). I’m attracted by several arguments for theism, but also by a couple of arguments for atheism. Overall, I’m more convinced by arguments for theism than for atheism, hence the higher upper bound. There are also other issues at play, but that’s the short explanation for my choice. . It’s hard to know if the numbers I chose are accurate 3. I mean “accurate” in the following sense: maybe I would choose an interval with a different width or mean if a gun was held to my head or if I thought about the question in more depth. I’ll hopefully have an improved credence interval by the end of this study. This may sound strange: why not change my interval if I don’t think it’s accurate? Part of the reason is that I’ve only recently begun to consider credence and belief as distinct, so I’d like to think more about how I’m choosing my interval. For example, what is my credence interval for the proposition “I won’t die at an early age”, and how does this relate to my belief that I won’t die at an early age? I’m also expecting my interval to shift as I think more about G. . I’ll now move on to summarizing and commenting on the four sections of this first chapter. . Arguments about the existence of monotheistic gods . This section explains how the various arguments for and against the existence of God have been organized in the book. First, ontological arguments appeal to reason alone. Second, cosmological arguments appeal to necessary features of the world. Third, teleological arguments appeal to contingent features of the world. Fourth, moral arguments appeal to the moral structure of the world. Fifth, arguments such as Pascal’s wager simply claim that belief in God is rational. Sixth, there are also “minor” arguments such as the argument from consciousness or the argument from beauty. Finally, there are arguments from personal religious experience, corporate religious experience, etc. . An interesting point is made about cumulative arguments. If there is a set of n arguments, all with the same conclusion, is the entire set stronger than any of the individual members? Not necessarily. For example, all the arguments could be strong except for one, which is circular; in this case, a mega-argument formed from all the arguments in the set would also be circular and hence weaker than any of the strong arguments in isolation. The mega-argument could be stronger than any individual argument if the arguments are viewed probabilistically, i.e., each argument uses some evidence to raise or lower the probability of its conclusion. Oppy doesn’t like this idea; he notes that each argument only considers a carefully selected portion of the full body of evidence and that the correct thing to do is to consider all the evidence at once. Discussion is left for later chapters. . Alexander Pruss raises the following problems with this treatment of cumulative arguments in his review. Suppose each argument has one premise that could be rejected by a reasonable person who holds a particular view —a moral realist will reject this premise, a materialist will reject that premise, etc. But unless each premise could be rejected on the same grounds, it doesn’t follow that a single person could hold all these views at the same time without contradictions. Another issue: maybe I assign a small probability of 0.1 to the truth of each of the n rejected premises. The probability that every premise is true is $0.1^n$, but the probability that every premise is false is $0.9^n$; the latter could become small. Pruss is also critical of Oppy’s suggestion that all the evidence must be considered at the same time if arguments are viewed probabilistically; instead, only the evidence relevant to a given statement is required to evaluate that statement. I’m keen to align with Pruss here. . Arguments . This section discusses what makes arguments successful. Oppy says this requires “(1) an account of rationality and rational belief revision, (2) an account of arguments, (3) an account of rational argumentation amongst rational agents, and (4) an account of the difficulties that arise as a result of the fact that we are not perfectly rational agents”. . It’s claimed that it’s possible for two perfectly rational people to disagree even if they have access to the same evidence. This seems wrong at first. The reasoning is: 1) the order in which the evidence is processed might be important, 2) there is a limit to how much information can be stored in a person’s mind at one time, and further 3) it could be argued that we generally make decisions in a Bayesian sense, evaluating the probability of the evidence given a set of prior probabilities or prior beliefs. In this way, every new belief is formed against the background of other beliefs. If this is true of perfectly rational people, then, of course, we can expect disagreement between less-than-perfectly rational people. . It&#39;s also claimed that the point of arguments is to cause reasonable belief revision 4. The following definition of a successful argument is given: . The most successful argument would be one that succeeds — or perhaps would or ought to succeed — in persuading any reasonable person to accept its conclusion; good, but less successful arguments would succeed — or perhaps would or ought to succeed — in persuading a non-zero percentage of reasonable people to accept their conclusions. . Thus, successful arguments are extremely difficult to form. Imagine I’m trying to change Alice’s mind. I should form an argument whose conclusion follows from premises Alice already believes; I’ll otherwise need to convince Alice of the premises using separate arguments. There’s nothing inherently wrong with this, but we may eventually reach beliefs for which there are no appeals to other beliefs for justification, and there is no hope of changing Alice’s mind if we disagree about those, at least not using arguments. If this view is correct, then arguments might not be the best way to persuade other reasonable people. . That it’s difficult to change the mind of a reasonable person is further supported by observing myself. I remember having long (friendly, non-serious) debates with my friends in college about various topics. I recall that, in some of these discussions, I didn’t even have a strong opinion on the topic beforehand. Yet, it was quite difficult to change the belief I formed at the beginning of the debate. I would think: “I formed this belief five minutes ago based on my intuition, and my intuition is infallible!” In hindsight, I was wrong some of the time and my arguments were likely weak. The fact that I had a strong resistance to changing my beliefs, even though I had very little to lose by giving them up, demonstrates that it would be difficult to change my mind about my core beliefs using arguments alone. . There are some questions about this view of successful arguments. Notice the phrase “would or ought to succeed”. Suppose there’s a premise h that any reasonable person would or ought to accept. I take “would” to mean that Austin would accept h if he had a certain set of priors and if his reasoning faculties were functioning correctly, and I take “ought” to mean that Austin ought to accept h since he has a certain set of priors and since his reasoning faculties are functioning correctly. One question is whether there are constraints on a reasonable person’s priors. On this, Oppy writes: . I am not committing myself to the claim that there are no substantive constraints on ‘priors’ — it may be that there are quite severe constraints on reasonable sets of ‘priors’; however, I am claiming that I can see no reason at all for thinking that there is a unique set of ‘priors’ that any reasonable person must have on pain of conviction of irrationality. . How can we know that Austin’s priors obey these constraints? Pruss has some things say about a related issue in the beginning of his review. . I also want to mention a potential problem for theists who accept that there can be disagreement about God betweeen reasonable truth-seeking people. This is the problem of divine hiddenness: it seems plausible that God would guide all reasonable truth-seekers to true beliefs about God if God existed, but there are reasonable truth seekers who don’t believe in God. Perhaps this will come up in future chapters. . Some considerations about agnosticism . In this section, Oppy distinguishes between weak agnosticism and strong agnosticism. He then argues that strong agnosticism fails, but that weak agnosticism is acceptable. The strong agnostic’s view can be summarized as: . In circumstances in which the available evidence no more — and no less — supports $p$ than it supports logically incompatible hypotheses $p_1$, $ dots$, $p_n$, $ dots$, one ought to suspend judgment between $p$, $p_1$, $ dots$, $p_n$, $ dots$ as well as the hypothesis that $p$ is false. | There are infinitely many God hypotheses, none of which are more likely than the others given the available evidence (e.g., a god who is not quite omniscient, a god who is not quite omnipotent, two gods rather than one, etc.). | It is neither rational to believe that God exists, nor to believe that God does not exist. | Let’s start with (1). It wasn’t obvious to me why we shouldn’t think that p is false in this case. There is, however, a paradox that might illustrate the reason. Consider a lottery with infinitely many tickets. I shouldn’t believe that my ticket is the winner since the probability is infinitesimal 5. But I should think this about each ticket, which amounts to believing that no ticket is the winner — a false belief. So, I should neither believe nor disbelieve that any particular ticket is the winner (that God exists). . So, (1) seems to make sense; however, there is a major problem: believing (1) could make it very difficult to avoid skepticism about basic things like the reality of my perceived world. Consider my belief that I am not a brain in a vat (BIV). On what basis do I believe this? My belief can’t be based on external evidence because the BIV hypothesis explains this evidence just as well as the alternative. Rather, it must be due to some internal evidence; in other words, the belief is basic. . If I accept that some beliefs are basic and accept the principle that it’s best to keep a belief unless there is some good reason to give it up (Oppy calls this “methodological conservatism”), then I have good reason to believe I’m not a BIV. The strong agnostic doesn’t agree with this and seems forced to suspend judgment on whether they are a BIV. The same would go for the belief that I didn’t pop into existence two seconds ago, and so on. . The strong agnostic needs to make some distinction between the God hypotheses and the cases involving extreme skepticism; however, there’s no clear way to do this. Oppy notes that the best approach is probably to say that my skepticism about the external world requires me to believe I’m special — somehow, I’m the only real thing in my perceived world — and that this gives me a reason to reject my skepticism. Call this the “I’m special” consideration ISC. However, Oppy proceeds, if the strong atheist concedes that the ISC is required to distinguish between these cases, then they have conceded that external evidence is not the only thing that can support reasonable beliefs, for the reason why the ISC should count against the truth of a belief is not clear from external evidence even though it may be clear to the strong agnostic. So, the problem is not that the strong agnostic is unreasonable for liking the ISC, it’s that they might be forced to choose between extreme skepticism and admitting that theists and atheists can be reasonable. In other words, the strong agnostic cannot believe (3) and the following statement at the same time: . It is possible to characterize a suitable notion of evidential support that does not rely upon a relativization to background assumptions. | This last view — suspending belief or non-belief in God while recognizing that this is not the only reasonable way to proceed — is weak agnosticism. . Alternative deities . The final section of this chapter considers the arguments for alternative deities like God*. God* is exactly like God but is completely evil. The thought is that the arguments for God work just as well for God*. For example, one way to respond to the problem of evil is to adopt skeptical theism 6, and it seems that this view could also be used as a defense of God*. Any arguments for which this applies would then seem to be ineffective as attacks against non-theists. A further step would be to claim that there is no way to choose God over God*, and that theists should therefore give up their belief in God. . If one thinks that moral facts exist and can be known, which I think I do, then the following critique will be convincing. An omniscient being would know all moral facts, but it’s impossible to believe that an action is wrong without being inclined to refrain from performing that action, i.e., I couldn’t know that murder is wrong but have no problem carrying out murder myself. Therefore, God* is logically impossible. . If God* is off the table, then God’ could be introduced. God’ is as close to God* as possible: completely evil, but not quite omniscient. Again, the arguments for God seem to work for God’. If the idea of God’ is shown to be incoherent, maybe we could think of God’’, God’’’, God#, etc. Oppy thinks these alternative deities could be useful to the non-theist as a defense, but not as an attack. This is because the theist likely has a set of prior beliefs that make an orthodoxly conceived monotheistic god seem the most likely to them even if they accept that some arguments don’t nail down God’s moral character. In general, it’s supposed that we shouldn’t be forced to give up a belief just because we recognize the existence of alternatives that seem just as good to an outside observer. . Conclusion . This chapter was primarily about epistemology. I haven’t thought much about this topic — almost not at all before recently reading Alvin Plantinga’s Knowledge and Christian Belief — and I think that articulating my epistemological views is necessary to answer the questions I listed at the beginning of this post. A good first step might be to read the relevant entry in the Stanford Encyclopedia of Philosophy. . I’m looking forward to the remaining chapters in the book in which specific arguments for the existence of God are examined. . . 1. I won’t use direct quotes very often; hopefully it’s clear when I’m paraphrasing the book vs. discussing the book.↩ . 2. I probably shouldn’t choose [0, q) or (p, 1]; in this case, I would possibly think there is zero chance that S is true (or false).↩ . 3. Some argue that credences don’t exist in reality; see this paper by Liz Jackson.↩ . 4. Oppy switches from “rational” to “reasonable”, and I’m sometimes confused by the difference.↩ . 5. An infinitesimal is smaller than every number but greater than zero↩ . 6. Skeptical theism is the view that we’re not in a position to know God’s thoughts, motives, etc. in any specific situation.↩ .",
            "url": "https://austin-hoover.github.io/blog/theism/arguments/2021/06/12/arguing_about_gods_1.html",
            "relUrl": "/theism/arguments/2021/06/12/arguing_about_gods_1.html",
            "date": " • Jun 12, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Painting a particle beam",
            "content": "This post explains an imporant step in the creation of intense proton beams in the Spallation Neutron Source (SNS). The method is often called &quot;phase space painting&quot; or simply &quot;painting&quot;, hence the title. . A quick tour of the SNS . There is a great scene in the otherwise disappointing movie Iron Man 2 in which Tony Stark builds a particle accelerator in his house. . The movie makes it look like pretty complicated process, but modern large-scale accelerators are immensely more complicated than the one Stark builds. In fact, I&#39;m actually quite amazed that such machines have been built and work as expected. One example is the SNS (overhead diagram below). . . The goal of the SNS is to produce extremely bright, pulsed neutron beams for neutron scattering experiments. These neutrons are produced through the process of spallation by colliding a proton beam with a Mercury target. The power of the proton beam must be as high as possible to maximize the brightness of the neutron beam, and creating such a high-power beam is a multi-step process; particles must travel all the way from the ion source on the far left to the target on the far right, passing though many different sections on their journey. We&#39;re first going to mention the basic function of each of these sections, and then we&#39;re going to look in more detail at the relatively small but extremely important injection region of the machine. This is the point where the HEBT meets the accumulator ring in the above diagram. . Ion source, front end, and linac . The beam originates in the ion source. As explained later, the beam is not actually made of protons at this point, but is instead made of H$^-$ ions (proton + two electrons). The ion source consists of a vacuum chamber filled with gas, and an oscillating electric field which ionizes the gas to form a glowing, pink plasma. The H$^-$ particles are then extracted from the plasma; I asked one of the researchers at SNS (who doesn&#39;t work on the ion source) how exactly these ions are extracted, and they said it&#39;s &quot;black magic&quot;, so I guess it&#39;s not straightforward. . . Diagram of the SNS ion source. From [1]. The H$^-$ beam is then accelerated to around 2.5 MeV, focused, and &quot;chopped&quot; into 1000 minipulses. Each minipulse is about 700 nanoseconds long, and they&#39;re separated by a gap of about 300 nanoseconds. The dynamics in this region are greatly affected by space charge, and it&#39;s hard to know exactly what the beam looks like when it exits the ion source. The SNS has built an exact replica of this front end called the Beam Test Facility (BTF) which is currently being used to better understand this section of the machine [2]. . Each minipulse is now ready to be accelerated. The next section of the machine is called the linac (linear accelerator), a long, straight section whose purpose is to accelerate the minipulses up to 1 GeV (around 90% of the speed of light) while maintaining an acceptable beam size. This is done using a series of normal-conducting and superconducting radio-frequency cavities. There is a lot to talk about in the linac, but I&#39;ll stop here since I don&#39;t have much knowledge of this area of the machine (yet). . HEBT, injection region, and accumulator ring . The high-energy beam transport (HEBT, pronounced &quot;hebbet&quot;) guides the fully accelerated minipulse from the linac to the left edge of the accumulator ring. At this point, all the ions in the minipulse are converted to protons and injected into the accumulator ring (more on this in the next section). The minpulse takes 1 microsecond to travel around the ring, at which point a second minipulse is injected and the circulating beam doubles in intensity. This repeats 1000 times over the course of one millisecond until the final beam, called a pulse, contains around $1.5 times 10^{14}$ protons. That seems like a lot until you consider that Avagadros number is one billion times larger! 60 of these pulses contains about the same energy as a stick of dynamite. . RTBT and target . Finally, the entire pulse is extracted from the ring and travels down the ring-target-beam-transport (RTBT) in which it is directed to the Mercury target, producing neutrons. These neutrons are then cooled and transported to various instrumental halls for use in neutron scattering experiments. . . Injection . Now we&#39;re going to discuss the injection region in more detail (see image below). Somehow, all the negatively charged ions need to be converted to positively charge protons, and the beam from the linac needs to merge with the circulating beam in the ring without derailing its trajectory; it&#39;s as if the ions were trying to merge onto a busy highway. The specific method used at SNS is charge exchange injection, which we discuss first. We&#39;ll then move on to discuss phase space painting, which is used to mitigate the effects of space charge in intense beams. . . H$^-$ charge exchange . Consider two oppositely charged beams which have the same kinetic energy but opposite charges. Also assume that we&#39;re dealing with point particles which do not interact with each other and have no transverse velocity. If these beams are sent thought a dipole magnet, their paths will be bent in opposite directions with the same radius of curvature. If they additionally have opposite angles of incidence, there will be a point in the dipole at which both beams are moving parallel to each other. Now imagine that, at this very instance, the charge of all the particles in one of the beams changes sign. The two beams would then be identical and would continue along the same trajectory, although there may be an offset. It&#39;s possible to also choose the horizontal and vertical positions of the two beams such that they converge and travel along identical paths. . . Oppositely charged particles are bent in different directions in a dipole magnetic field. The idea is to do this with the two beams in the SNS: the negatively charged H$^-$ beam the linac and the positively charged proton beam which is circulating in the ring. Russian scientists developed a method to do this in the 1960&#39;s using a thin foil which strips the two electrons from the Hydrogen ions but leaves the protons. The foil properties need to be chosen carefully. It needs to be the right material and thick enough to have a high stripping efficiency (number of ions successfully stripped divided by total number of ions), but not so thick that most of the protons are scattered. It also needs to be able to survive high numbers of foil hits without being destroyed. Thus, the choice of foil parameters requires a knowledge of materials science. The SNS uses diamond foils as in the following images. . . Dealing with extra particles . Some of the H$^-$ hold on to their electrons as they pass through the foil, and some only lose one electron, becoming H$^0$. To deal with these particles, the foil is placed in a dipole field. Because many of the H$^0$ particles are in excited states, it is likely that their electron will be stripped by the magnetic field soon after the foil; this is known as Lorentz stripping. So a lot of these will become protons and join the circulating beam, just a bit late to the party. The remaining non-protons continue away from the ring and encounter another foil which removes the electrons so that they can be guided to a beam dump. There is also the need to catch the stripped electrons, which can have significant kinetic energies, but I won&#39;t discuss that here. . Is Liouville&#39;s theorem violated? . Those familiar with Liouville&#39;s theorem may object to the charge-exchange method. Liouville&#39;s theorem applies to any system which obeys Hamilton&#39;s equations: . $$ dot{ mathbf{q}} = frac{ partial mathbf{H}}{ partial mathbf{p}} , quad dot{ mathbf{p}} = - frac{ partial mathbf{H}}{ partial mathbf{q}},$$ . where $ mathbf{q}$ are the coordinates and $ mathbf{p}$ are the momenta. Imagine we took a volume of phase space and started to fill it with particles; in fact, we fill all of the infinite number of points inside the volume. Then we evolve the system in time. The final distribution of particles may have changed shape, but Liouville&#39;s theorem states that its volume will not have changed. Mathematically, this is due to Hamilton&#39;s equations being equivalent to a coordinate transformation whose Jacobian has a determinant equal to one. So, the objection is that the phase space volume of the entire system (linac beam + circulating beam) seems to decrease when they are merged, i.e., the linac beam is stacked directly on top of the circulating beam, and that this should be disallowed by Liouville&#39;s theorem. Is this true? . I read a paper by A. Ruggiero which helped to clarify this issue [3]. The key point is that Liouville&#39;s theorem deals with distributions rather than finite numbers of particles. Any finite number of particles will not fill up every point in phase space, so there is nothing preventing another finite number of particles from being added to the empty regions. The limitation is that it&#39;s not clear how to guide two beams of the same charge to the same position using dipole magnets; hence, charge exchange. I should note, however, that this does not seem to be the explanation put forth in the talks I&#39;ve heard on this subject. I&#39;m planning to discuss this with some other people in the field. . The future: lasers . A major research project at the SNS is to demonstrate laser-assisted charge exchange (LACE), in which a laser is used to excite the ions and a dipole magnet is used to strip the magnets intead of a foil. This would overcome the scattering losses from foils as well as their finite lifetimes; it is a very promising approach as machines continue to increase in power [4]. . Phase space painting . In the last section, we assumed that the particles didn&#39;t interact with each other, but in reality, space charge is the fundamental limit on the intensity in high-power hadron accelerators. Injecting at the same position in space will cause the beam to become very dense, and the beam will then expand due to the increased space charge forces. It&#39;s likely that this will produce a very non-uniform distribution which, as mentioned in this post, is undesirable. This is the motivation for so-called phase space painting or simply painting. The idea is to change the transverse position and momentum of the circulating beam over time in order to slowly fill or &quot;paint&quot; the beam in phase space and hopefully produce a more uniform density beam. Another motivation for painting is to avoid excessive foil hits, since these lead to shorter foil lifetimes and also beam scattering. . Time-dependent kicker magnets . Here is a zoomed in view of the injection region. . . The blue elements which aren&#39;t labeled in the ring are just quadrupoles used to focus the beam. This leaves the &quot;bump&quot; or &quot;kicker&quot; magnets and the &quot;Chicane&quot; magnets. These are both dipoles, but they are a bit different. The chicane dipoles provide a fixed horizontal bump to the closed orbit so that it is aligned with the beam from the linac. The kickers, on the other hand, can move the closed orbit horizontally or vertically, and they&#39;re time-dependent. Regarding the latter point, the current from the magnet&#39;s power supply, and therefore the magnetic field, can be varied during injection. Let&#39;s take a look at the vertical closed orbit with the kickers turned on (black line). . . The dark blue boxes are quadrupoles, the red boxes are Chicane dipoles, and the remaining elements are the horizontal (green) and vertical (yellow) kickers. Without the kickers, the closed orbit will just go straight through the center of each magnet, and a similar thing holds in the horizontal plane. Thus, we have control over the horizontal and vertical position of the circulating beam relative to the injected beam. But there is also the possibility that the trajectory is converging or diverging at the foil, so we also have control over the horizontal and vertical circulating beam slope relative to the injected beam. These eight kickers therefore give full control over the transverse phase space coordinates of the circulating beam relative to the injected beam at every point during the injection. . Production painting . The time-dependence of each kicker magnet is determined by a waveform which determines the current given to its power supply as a function of time; for example, we could have a linear waveform, square root waveform, etc. Choosing these waveforms amounts to choosing the initial and final position/slope of the circulating beam, as well as the rate of change in the position/slope. The standard &quot;production&quot; scheme in the SNS (as in neutron production) is to use a square root waveform so that . $$ x_{inj} - x_{co} = (x_{max} - x_{min}) sqrt{t / t_{max}} + x_{min}, $$ $$ y_{inj} - y_{co} = (y_{max} - y_{min}) sqrt{t / t_{max}} + y_{min}, $$ . where the co subscript means &quot;closed orbit&quot; and inj means &quot;injected beam&quot;. The slope of the circulating beam is kept at zero in this scheme. . It&#39;s probably best to use some visualizations at this point. I simulated the injection painting using PyORBIT; included in this simulation are effects such as space charge, nonlinear magnetic fringe fields, scattering from the foil, etc., so the results should be somewhat similar to the real world. 260 simulation particles were injected on each turn to give a final number of 260,000, which should provide good statistics for the space charge solver which operates on a $128 times 128$ transverse grid. The whole simulation took a few hours to run on my laptop. (The space charge solver I used makes some approximations in the longitudinal dimension; a more realistic solver will push the execution time from a few hours to a few days.) . The following animation shows this simulated beam at the injection point during the first 35 turns in the ring. The off-diagonal subplots show the correlations between the four phase space variables (a sample of 10,000 particles is used) and the on-diagonal subplots show the histograms for each variable. The foil location is shown by the red dot in the $x$-$y$ plane. . . Keep in mind that each little cluster is actually a bunch of particles; it&#39;s hard to resolve because the width is small compared to the full beam. Notice that, since the circulating and injected beams are offset to begin with, the injected particles start to trace ellipses in the $x$-$x&#39;$ and $y$-$y&#39;$ projections. The frequencies at which the particles oscillate in each plane are not the same, so the path in $x$-$y$ space is constantly changing, eventually filling a rectangular region. The next animation shows the beam over all 1000 turns. . . Notice that the beam size is slowly increasing, and also that the density is steadily increasing; this is only apparent from the histograms since I&#39;m using a random sample of particles in the scatter plots. Space charge, as well as nonlinear effects, tend to round the hard edges of the the originally rectangular beam. The beam also exhibits some interesting dynamics after turn 100, and again after turn 800, when it begins to tilt back and forth, which is probably due to space charge coupling the two planes. In the end, the beam has a somewhat uniform density, is not tilted, is not rotating, and is quite intense ($1.5 times 10^{14}$ particles), so the basic goals of the painting scheme have been achieved. The beam can now be extracted and collided with the target to produce neutrons. . Conclusion . This post briefly outlined a method to &quot;paint&quot; a high-intensity proton beam in a ring. One exciting project which is going on right now is to modify the painting method to produce an elliptical, uniform density, rotating beam which approximates the Danilov distribution. This basically involves changing the slope along with the position of the circulating beam. . . [1] &quot;The Spallation Neutron Source accelerator system design,&quot; Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment 763:610–673 (2014). https://doi.org/https://doi.org/10.1016/j.nima.2014.03.067. . [2] B. Cathey, S. Cousineau, A. Aleksandrov, &amp; A. Zhukov, &quot;First Six Dimensional Phase Space Measurement of an Accelerator Beam,&quot; Phys. Rev. Lett. 121:064804 (2018). https://doi.org/10.1103/PhysRevLett.121.064804. . [3] A. G. Ruggiero, &quot;Are We Beating Liouville’s Theorem?,&quot; eConf C7803272:123 (1978). . [4] S. Cousineau, A. Rakhman, M. Kay, A. Aleksandrov, V. Danilov, T. Gorlov, Y. Liu, M. Plum, A. Shishlo, &amp; D. Johnson, &quot;First Demonstration of Laser-Assisted Charge Exchange for Microsecond Duration Beams,&quot; Phys. Rev. Lett. 118:074801 (2017). https://doi.org/10.1103/PhysRevLett.118.074801. .",
            "url": "https://austin-hoover.github.io/blog/accelerators/space%20charge/2021/05/27/painting_a_particle_beam.html",
            "relUrl": "/accelerators/space%20charge/2021/05/27/painting_a_particle_beam.html",
            "date": " • May 27, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Computing matched envelopes",
            "content": "My first peer-reviewed paper, titled Computation of the matched envelope of the Danilov distribution [1], was published in Physical Review Accelerators and Beams (PRAB) at the end of April. I thought I would summarize the results of the paper here. The paper was on the topic of self-consistent beams; I&#39;ll start by defining this term. . Self-consistent beams . Definition and attractive properties . We define a beam to be self-consistent if it satisfies the following conditions: . It gives rise to linear space charge forces. | This linearity is maintained as it is transported through arbitrary linear focusing fields. | Let&#39;s discuss the meaning and relevance of these conditions. Space charge forces are the forces that arise between charged particles in a beam. Linear refers to the dependence of the magnitude of this force on the position in the beam. For example, consider the following two distributions of charged particles and the radial electric fields they produce. . import numpy as np import matplotlib.pyplot as plt import proplot as pplt # Generate a 2D uniform density distribution. n = 10000 np.random.seed(6) rho = 2.0 * np.sqrt(np.random.uniform(size=n)) phi = np.random.uniform(0, 2*np.pi, size=n) X1 = np.vstack([rho * np.cos(phi), rho * np.sin(phi)]).T # Generate a Gaussian distribution. X2 = np.random.normal(scale=1.0, size=(n, 2)) # Plot the distributions. fig, axes = pplt.subplots( nrows=2, ncols=2, figwidth=5.5, spany=False, spanx=False, wspace=7.0, height_ratios=[1, 0.3] ) for ax, X in zip(axes[0, :], [X1, X2]): ax.scatter(X[:, 0], X[:, 1], s=0.05, c=&#39;k&#39;) axes[0, :].format(aspect=1.0, yticks=[], xspineloc=&#39;neither&#39;, yspineloc=&#39;neither&#39;) # Plot the radial electric field. def Efield(r, kind=&#39;gaussian&#39;): if r == 0: return 0.0 if kind == &#39;uniform&#39;: return 0.25 * r if abs(r) &lt;= 2.0 else 1.0 / r elif kind == &#39;gaussian&#39;: return (1.0 / r) * (1 - np.exp(-0.5 * r**2)) xvals = np.linspace(-4.0, 4.0, 100) axes[1, 0].plot(xvals, [Efield(x, &#39;uniform&#39;) for x in xvals], &#39;k-&#39;) axes[1, 1].plot(xvals, [Efield(x, &#39;gaussian&#39;) for x in xvals], &#39;k-&#39;) axes[1, :].format(ylabel=r&#39;$E_r$&#39;, yticks=[0], xlabel=r&#39;r / $ sigma$&#39;, xspineloc=&#39;bottom&#39;, yspineloc=&#39;left&#39;) . . The distribution on the left produces an electric field that is proportional to the radius, i.e., it gives rise to linear space charge forces (this isn&#39;t true outside $r = 2 sigma$, but no particles are in this region). On the other hand, the distribution on the right produces an electric field that depends nonlinearly on the radius, i.e., it gives rise to nonlinear space charge forces. . Nonlinear forces bring with them several negative effects. One of these is emittance growth. Emittance is a measure of the area occupied by the beam in phase space, so it measures both the spread in positions and the spread in velocities of the beam particles. Most of the time we are concerned with the horizontal emittance ($x$-$x&#39;$) and the vertical emittance ($y$-$y&#39;$). It&#39;s usually important to keep the emittance as small as possible; for instance, a smaller emittance leads to a larger luminosity in a collider. The key point here is that the emittance is only invariant if all forces are linear. Here is a simulation example showing emittance growth due to nonlinear space charge forces from [2]. . . Another effect of space charge in circular machines is called the tune shift. The single-particle tune is the number of oscillations a particle undergoes in phase space during one trip around the accelerator. Space charge decreases the tune of every particle, similar to how weakening a spring decreases the oscillation frequency of a mass attached to the spring. This is annoying because the tunes need to be precisely chosen to avoid resonances driven by nonlinear magnetic fields. A common strategy is to avoid any resonance lines up to a certain order defined by $M_x nu_x + M_y nu_y = N$, where $ nu_x$ and $ nu_y$ are the horizontal and vertical tunes, $M_x$, $M_y$, and $N$ are integers, and $|M_x| + |M_y|$ is the order of the resonance; if a particle lies on one of these lines in tune space, it has a greater chance of being driven to large amplitudes and eventually lost. So, there is some extra work to ensure that the tune doesn&#39;t cross any low-order resonance lines as it shifts. But nonlinear forces cause the magnitude of the tune shift to depend on the particle&#39;s position; the beam then spreads out in tune space, increasing the chance that some particles will cross resonance lines. This puts a fundamental limit on the beam intensity. Here is an old simulation example in the SNS from [3]. . . A third downside of nonlinear space charge forces is a loss of analytic tractability. Space charge introduces the self-consistency problem: the collective electric field causes each particle to move, which then changes the collective field, which then moves the particles, and so on. In general, there is no analytic solution to this problem and we must turn to expensive computer simulations. . At this point it&#39;s (hopefully) clear that a beam with linear space charge forces would alleviate several problems. In fact, we already showed an example of such a beam. But if we took this beam and transported it through an accelerator; would its uniform density be maintained? To answer this question, we would need to specify the initial distribution of particle velocities in addition to the initial positions. It turns out that there are only a few special cases for which the answer is yes — we call these self-consistent beams. . KV distribution . The first self-consistent beam was derived by a pair of Russian scientists in 1959 and is known as the KV distribution. Particles in the KV distribution uniformly populate the boundary of an ellipsoid (or sphere in certain coordinates) in 4D phase space ($x$, $x&#39;$, $y$, $y&#39;$). I&#39;m not going into the details here, but it&#39;s simlar to the microcanonical distribution from statistical mechancis. Interestingly, any 2D projection of the distribution is a uniform density ellipse; in particular, the $x$-$y$ distribution is an upright ellipse. There are a few ways to generate a uniform density on a 4D sphere, but the nicest way I&#39;ve found is to first create a 4D Gaussian, then normalize all vectors to unit length. . X = np.random.normal(size=(7000000, 4)) X = np.apply_along_axis(lambda x: x / np.linalg.norm(x), 1, X) axes = myplt.corner(X, figsize=(4.5, 4.5), rms_ellipse=True, autolim_kws=dict(pad=0.25), rms_ellipse_kws=dict(lw=0.3, ls=&#39;-&#39;, color=&#39;black&#39;), grid_kws=dict(labels=[&quot;x&quot;, &quot;x&#39;&quot;, &quot;y&quot;, &quot;y&#39;&quot;])) . . The electric field within a uniform density, upright ellipse can (not easily) be shown to be . $$ mathbf{E}(x, y) propto frac{x}{c_x(c_x + c_y)} hat{x} + frac{y}{c_y(c_x + c_y)} hat{y}, tag{1}$$ . where $c_x$ and $c_y$ are the semi-axes of the ellipse (I&#39;ve left out a factor out front containing the beam intensity). Notice that it&#39;s both linear and uncoupled, meaning that the $x$ component of the field is proportional to $x$. Using this field, a system of differential equations can be produced which evolve the beam envelope, the elliptical boundary containing the particles in the $x$-$y$ plane. The neat thing is that these equations close the feedback loop presented by the self-consistency problem; every change to the envelope is consistent with its own fields, even when the external fields are time-dependent. These envelope equations, as they&#39;re called, have been incredibly important for understanding space charge affects. In addition to providing a theoretical benchmark for computer simulations, the equations can capture the approximate behavior of more realistic beams in some cases (see the first figure in this post — the Gaussian has has an approximately uniform core). More details can be found in [4]. Let&#39;s try integrating these equations in a FODO lattice. . from scipy.integrate import odeint def fodo(s, quad_strength=0.556, cell_length=5.0): s = (s % cell_length) / cell_length delta = 0.125 if s &lt; delta or s &gt; 1 - delta: return +quad_strength elif 0.5 - delta &lt;= s &lt; 0.5 + delta: return -quad_strength return 0.0 def kv_derivs(params, s, Q, foc): cx, cxp, cy, cyp = params k0x = foc(s) k0y = -k0x w = np.zeros(4) w[0], w[2] = cxp, cyp w[1] = -k0x*cx + 2*Q/(cx+cy) + 16*epsx**2/(cx**3) w[3] = -k0y*cy + 2*Q/(cx+cy) + 16*epsy**2/(cy**3) return w def track_kv(params, positions, Q, foc): tracked = odeint(kv_derivs, params, positions, args=(Q, foc)) sizes = tracked[:, [0, 2]] return sizes * 1000 # convert to mm # Create KV envelope alphax, alphay, betax, betay = 0.0, 0.0, 8.017, 1.544 epsx = epsy = 10e-6 cx = 2 * np.sqrt(epsx * betax) cy = 2 * np.sqrt(epsy * betay) cxp = cyp = 0.0 params = [cx, cxp, cy, cyp] # Integrate envelope equations cell_length, periods, npts = 5.0, 4, 1000 positions = np.linspace(0, cell_length * periods, npts) Q = 1e-5 sizes = track_kv(params, positions, Q, fodo) sizes0 = track_kv(params, positions, 0.0, fodo) . . from matplotlib.lines import Line2D from matplotlib.patches import Ellipse colors = pplt.Cycle(&#39;colorblind&#39;).by_key()[&#39;color&#39;] fps = 12 stride = 10 umax, umin = np.max(sizes), np.min(sizes) umax_pad = 1.25 * umax fig, axes = pplt.subplots(nrows=2, ncols=2, figsize=(7, 2.5), spany=False, aligny=True, sharey=False, sharex=False, hspace=0.2, height_ratios=[5, 1], width_ratios=[2.75, 1]) axes[0, 0].format(xlabel=&#39;&#39;, ylabel=&#39;Beam size [mm]&#39;, ylim=(umin - 5, umax + 5)) axes[1, 0].format(xlabel=&#39;s [m]&#39;, ylabel=r&#39;$k_x$&#39;, yticks=[0], ylim=(-0.6116, 0.6116)) axes[:, 0].format(xlim=positions[[0, -1]]) axes[1, 0].spines[&#39;top&#39;].set_visible(False) axes[0, 1].format(xticklabels=[], yticklabels=[], xlabel=&#39;x&#39;, ylabel=&#39;y&#39;, xlim=(-umax_pad, umax_pad), ylim=(-umax_pad, umax_pad)) for side in [&#39;top&#39;, &#39;right&#39;]: axes[0, 1].spines[side].set_visible(False) axes[1, 1].axis(&#39;off&#39;) axes[0, 0].format(xticklabels=[]) axes[0, 0].legend(handles=[Line2D([0], [0], color=colors[0]), Line2D([0], [0], color=colors[1])], labels=[&#39;x&#39;, &#39;y&#39;], ncols=1, loc=&#39;upper left&#39;, fontsize=&#39;small&#39;, handlelength=1.5) axes[1, 0].plot(positions, [fodo(s) for s in positions], color=&#39;k&#39;, lw=1) plt.close() line1, = axes[0, 0].plot([], []) line2, = axes[0, 0].plot([], []) axes[0, 0].format(cycle=&#39;colorblind&#39;) line3, = axes[0, 0].plot([], [], ls=&#39;--&#39;, lw=0.5) line4, = axes[0, 0].plot([], [], ls=&#39;--&#39;, lw=0.5) def update(i): i *= stride line1.set_data(positions[:i], sizes[:i, 0]) line2.set_data(positions[:i], sizes[:i, 1]) line3.set_data(positions[:i], sizes0[:i, 0]) line4.set_data(positions[:i], sizes0[:i, 1]) for patch in axes[0, 1].patches: patch.set_visible(False) axes[0, 1].add_patch(Ellipse((0, 0), 2.0 * sizes[i, 0], 2.0 * sizes[i, 1], fc=&#39;lightgrey&#39;, lw=0.75, ec=&#39;None&#39;)) axes[0, 1].add_patch(Ellipse((0, 0), 2*sizes0[i, 0], 2*sizes0[i, 1], fill=False, ls=&#39;--&#39;, color=&#39;k&#39;, lw=0.5, alpha=0.5)) animation.FuncAnimation(fig, update, frames=len(positions[::stride]), interval=1000.0/fps) . . &lt;/input&gt; Once Loop Reflect On the top left, I&#39;ve plotted the beam sizes with space charge (solid lines) and without space charge (dashed lines) as a function of the position $s$ in the lattice. The bottom plot shows the horizontal focusing strength; $k_x &gt; 0$ is a focusing quadrupole, $k_x &lt; 0$ is a defocusing quadrupole, and $k_x = 0$ is a drift. On the right, I&#39;ve plotted the beam ellipse in the $x$-$y$ plane (the dashed ellipse is the zero-space-charge solution). Notice, that the beam remains upright at all times. I should also mention that the horizontal and vertical emittances are invariants of the motion (more on that in a moment). . Danilov distribution . For a while, the KV distribution was assumed to be the only self-consistent beam in the case of time-dependent external focusing; recently, however, a larger class of self-consistent beams were derived. One of these is the so-called Danilov distribution, which was the focus of this work. This beam is similar to the KV distribution in that it has a uniform particle density, but the main difference is that it has a nonzero angular momentum. Each particle in the beam satisfies . $$ x&#39; = e_{11}x + e_{12}y, tag{2}$$ $$ y&#39; = e_{21}x + e_{22}y, $$ . where the $e_{ij}$ terms are constants. Suppose $e_{11} = e_{22} = 0$ and $e_{21} = -e_{12} = 1$; this gives $y&#39; = x$ and $x&#39; = -y$, which describes a rotating rigid disk. Here are the phase space projections of the beam. . X = np.random.normal(size=(7000000, 4)) X = np.apply_along_axis(lambda x: x / np.linalg.norm(x), 1, X) X[:, 3] = +X[:, 0] X[:, 1] = -X[:, 2] axes = myplt.corner(X, figsize=(4.5, 4.5), rms_ellipse=True, autolim_kws=dict(pad=0.25), rms_ellipse_kws=dict(lw=0.3, ls=&#39;-&#39;, color=&#39;black&#39;), grid_kws=dict(labels=[&quot;x&quot;, &quot;x&#39;&quot;, &quot;y&quot;, &quot;y&#39;&quot;])) . . We can again derive equations for the elliptical beam envelope, but now they&#39;re going to include coupling between $x$ and $y$. This is because tilting the ellipse from Eq. (1) produces a term proportional to $xy$ for the electric field. More details can be found in [5]. . def get_tilt_angle(a, b, e, f): return -0.5 * np.arctan2(2*(a*e + b*f), a**2 + b**2 - e**2 - f**2) def get_radii(a, b, e, f): phi = get_tilt_angle(a, b, e, f) sin, cos = np.sin(phi), np.cos(phi) sin2, cos2 = sin**2, cos**2 xx = a**2 + b**2 yy = e**2 + f**2 xy = a*e + b*f cx = np.sqrt(abs(xx*cos2 + yy*sin2 - 2*xy*sin*cos)) cy = np.sqrt(abs(xx*sin2 + yy*cos2 + 2*xy*sin*cos)) return cx, cy def danilov_derivs(params, s, Q, foc): k0x = foc(s) k0y = -k0x a, b, ap, bp, e, f, ep, fp = params phi, (cx, cy) = get_tilt_angle(a, b, e, f), get_radii(a, b, e, f) cos, sin = np.cos(phi), np.sin(phi) cos2, sin2, sincos = cos**2, sin**2, sin * cos T = 2 * Q / (cx + cy) w = np.zeros(8) w[0], w[1], w[4], w[5] = ap, bp, ep, fp w[2] = -k0x*a + T*((a*cos2 - e*sincos)/cx + (a*sin2 + e*sincos)/cy) w[3] = -k0x*b + T*((e*sin2 - a*sincos)/cx + (e*cos2 + a*sincos)/cy) w[6] = -k0y*e + T*((b*cos2 - f*sincos)/cx + (b*sin2 + f*sincos)/cy) w[7] = -k0y*f + T*((f*sin2 - b*sincos)/cx + (f*cos2 + b*sincos)/cy) return w def track_danilov(params, positions, Q, foc): tracked = odeint(danilov_derivs, params, positions, args=(Q, foc)) a, b, ap, bp, e, f, ep, fp = tracked.T xsizes = np.sqrt(a**2 + b**2) ysizes = np.sqrt(e**2 + f**2) sizes = np.vstack([xsizes, ysizes]).T * 1000 cx, cy = get_radii(a, b, e, f) radii = np.vstack([cx, cy]).T * 1000 angles = np.degrees(get_tilt_angle(a, b, e, f)) return sizes, radii, angles # (Calculated initial parameters offline to save space) params = np.array([0.0179, 0.0, 0, 0.0022, 0, -0.0079, 0.0051, 0]) positions = np.linspace(0.0, 20.0, 1000) sizes, radii, angles = track_danilov(params, positions, Q, fodo) sizes0, radii0, angles0 = track_danilov(params, positions, 0.0, fodo) . . umax, umin = np.max(sizes), np.min(sizes) umax_pad = 1.25 * umax fig, axes = pplt.subplots(nrows=2, ncols=2, figsize=(7, 2.5), spany=False, aligny=True, sharey=False, sharex=False, hspace=0.2, height_ratios=[5, 1], width_ratios=[2.75, 1]) axes[0, 0].format(xlabel=&#39;&#39;, ylabel=&#39;[mm]&#39;, ylim=(umin - 5, umax + 5)) axes[1, 0].format(xlabel=&#39;s [m]&#39;, ylabel=r&#39;$k_x$&#39;, yticks=[0], ylim=(-0.6116, 0.6116)) axes[:, 0].format(xlim=positions[[0, -1]]) axes[1, 0].spines[&#39;top&#39;].set_visible(False) axes[0, 1].format(xticklabels=[], yticklabels=[], xlabel=&#39;x&#39;, ylabel=&#39;y&#39;, xlim=(-umax_pad, umax_pad), ylim=(-umax_pad, umax_pad)) for side in [&#39;top&#39;, &#39;right&#39;]: axes[0, 1].spines[side].set_visible(False) axes[1, 1].axis(&#39;off&#39;) axes[0, 0].format(xticklabels=[]) axes[0, 0].legend(handles=[Line2D([0], [0], color=colors[0]), Line2D([0], [0], color=colors[1])], labels=[r&#39;$ sqrt{ langle{x^2} rangle}$&#39;, r&#39;$ sqrt{ langle{y^2} rangle}$&#39;], ncols=1, loc=&#39;upper left&#39;, fontsize=&#39;small&#39;, handlelength=1.5) axes[1, 0].plot(positions, [fodo(s) for s in positions], color=&#39;k&#39;, lw=1) plt.close() line1, = axes[0, 0].plot([], []) line2, = axes[0, 0].plot([], []) axes[0, 0].format(cycle=&#39;colorblind&#39;) line3, = axes[0, 0].plot([], [], ls=&#39;--&#39;, lw=0.5) line4, = axes[0, 0].plot([], [], ls=&#39;--&#39;, lw=0.5) def update(i): i *= stride line1.set_data(positions[:i], sizes[:i, 0]) line2.set_data(positions[:i], sizes[:i, 1]) line3.set_data(positions[:i], sizes0[:i, 0]) line4.set_data(positions[:i], sizes0[:i, 1]) for patch in axes[0, 1].patches: patch.set_visible(False) axes[0, 1].add_patch(Ellipse((0, 0), 2*radii[i, 0], 2*radii[i, 1], angles[i], fc=&#39;lightgrey&#39;, lw=0.75, ec=&#39;None&#39;)) axes[0, 1].add_patch(Ellipse((0, 0), 2*radii0[i, 0], 2*radii0[i, 1], angles0[i], fill=False, ls=&#39;--&#39;, color=&#39;k&#39;, lw=0.5, alpha=0.5)) animation.FuncAnimation(fig, update, frames=len(positions[::stride]), interval=1000.0/fps) . . &lt;/input&gt; Once Loop Reflect Notice that the beam tilts even without space charge. This has to do with the phase relationship between $x$ and $y$. When the beam is upright in the $x$-$y$ plane, $x$ and $y$ are 90 degrees out of phase (think of circular motion); on the other hand, a 0 or 180 degree phase difference would lead to a diagonal line in the $x$-$y$ plane and no correlations in $x$-$y&#39;$ or $y$-$x&#39;$. So, the beam is going to tilt any time the $x$ and $y$ phase are changed by unequal amounts. . What is a bit less obvious is how space charge space charge affects the beam when it tilts. Although the forces are still linear, the $x$ and $y$ emittances are no longer conserved. Below is an example of a turn-by-turn plot (1 turn = 1 period = 5 meters in the above example) in which there are two frequencies involved: a faster oscillation of the beam envelope, and a slower coupling oscilllation corresponding the emittance exchange. . . Finding the matched beam . I&#39;ll now move on to describing the problem we addressed in the paper. Notice that the focusing in the above animations repeats itself after five meters; we call this the period length. A beam is matched to the lattice if its envelope repeats itself after one period length, or more precisely, its covariance matrix repeats itself: . $$ mathbf{ Sigma}(s + L) = Sigma(s) tag{3}$$ . for all $s$, where $s$ is the position in the lattice, $L$ is the period length, and $ Sigma$ is the covariance matrix given by . $$ mathbf{ Sigma} = begin{bmatrix} langle{x^2} rangle &amp; langle{xx&#39;} rangle &amp; langle{xy} rangle &amp; langle{xy&#39;} rangle langle{xx&#39;} rangle &amp; langle{x&#39;^2} rangle &amp; langle{yx&#39;} rangle &amp; langle{x&#39;y&#39;} rangle langle{xy} rangle &amp; langle{yx&#39;} rangle &amp; langle{y^2} rangle &amp; langle{yy&#39;} rangle langle{xy&#39;} rangle &amp; langle{x&#39;y&#39;} rangle &amp; langle{yy&#39;} rangle &amp; langle{y&#39;^2} rangle end{bmatrix} tag{4}$$ (assuming all means are zero). So the matched beam has not only the same shape and orientation in $x$-$y$ space, but also the same spread in velocities and correlations between the positions and velocites. Finding the matched beam amounts to choosing the correct initial $ mathbf{ Sigma}$ such that Eq. (3) is satisfied. As I&#39;ll explain in a moment, the task is trivial without space charge but difficult with space charge. . Motivation . There are a few reasons for computing this matched envelope. First, it&#39;s a sort of minimum energy solution, and this free energy can drive emittance growth in real beams. Second, it&#39;s the most radially compact solution for a given beam intensity, i.e., the maximum beam size is minimized. Previous studies have computed the matched envelope of the KV distribution as a first step in a stability analysis — checking that perturbations around the matched solution do not grow exponentially — and it would be nice to extend this analysis to the Danilov distribution. But the main motivation is related to an ongoing project to create an approximate Danilov distribution in the SNS, for which it is critical to know the matched solution in the 250 meter long SNS accumulator ring. For all these reasons, we developed a method to compute the matched envelope of Danilov distribution. Our strategy was to do this in as simple a case as possible (FODO), but also to include some interesting effects such as splitting the lattice tunes or adding external coupling, planning to apply the method to the more complicated SNS at a later time. . Challenges . There are two challenges to overcome. First, space charge causes the final beam to depend on the initial beam in a potentially complicated way which is unknown before tracking the beam; this is especially true for long lattices and large beam intensities. This just means we&#39;ll need to iterate to get the correct answer. The second challenge is knowing which beam parameters to vary. This is straightforward in the KV distribution since space charge doesn&#39;t couple $x$ and $y$, but it wasn&#39;t immediately clear how to do this when coupling is present. . Solution . Consider the equation of motion for a particle in a coupled lattice: . $$ x&#39;&#39; + k_{11}(s)x + k_{13}(s)y + k_{14}(s)y&#39; = 0, $$ $$ y&#39;&#39; + k_{31}(s)x + k_{33}(s)y + k_{23}(s)x&#39; = 0. tag{5}$$ . Now, what does space charge do to these equations? For a tilted, uniform density ellipse, it simply modifies $k_{11}$, $k_{13}$, $k_{31}$, and $k_{33}$. Thus, we could replicate the effect of space charge by inserting a large number of linear defocusing elements into the lattice. We decided to call this new lattice the effective lattice. This is illustrated below. . . The figure on the left is just showing how space charge is like a (possibly tilted) defocusing quadrupole in both planes. Notice that space charge causes $k_{13}$ to be nonzero, which shows that the system is coupled. We can think of the matched beam as generating a periodic effective lattice. The question then becomes which parameters to use to describe a coupled lattice... if we can do that, then we just vary those parameters to find the solution. There is actually no universally agreed upon method to describe coupled motion in beam physics; we used the one I wrote about in a previous post. I&#39;ll briefly comment on the application to this problem. . The following figure shows the turn-by-turn trajectory of a single particle in a coupled lattice. . . The particle traces out a surface in 4D phase space consisting of points $ mathbf{x}_1$, $ mathbf{x}_2$, etc. We can create a matched beam by putting a particle at each of these points since the particles will just trade positions after a turn ($ mathbf{x}_i rightarrow mathbf{x}_{i+1}$). This can also be understood in terms of the eigenvectors of the one-turn transfer matrix; each particle can be written as a linear combination of these eigenvectors (shown in blue and red): . $$ mathbf{x} = sqrt{ varepsilon_1} mathbf{v}_1e^{-i psi_1} + sqrt{ varepsilon_2} mathbf{v}_2e^{-i psi_2}. $$ . The eigenvectors have amplitudes ($ varepsilon$) and phases ($ psi$), and a matched beam is formed by distributing particles along either or both of the eigenvectors with phases ranging uniformly between zero and $2 pi$. The ellipses traced out by the eigenvectors are described by the 4D Twiss parameters $ alpha_{1x}$, $ beta_{1x}$, etc. . A key insight was that particles in the Danilov distribution only depend on one eigenvector; in other words, all the particles oscillate at the same frequency. There are more details on this in the paper, but the short story is that this cuts the number of parameters to six. There is the beam size in each dimension relative to the emittance, the beam divergence in each dimension relative to the emittance, the phase difference between $x$ and $y$ (this could go from 0 for a flat line or 90 degrees for an upright ellipse), and the ratio between the $x$ and $y$ emittances. We can also choose which eigenvector to use, so there are actually two different solutions. We wrap up all these parameters into a vector $ mathbf{p}$. . We can frame this as an optimization problem in which we search for the $ mathbf{p}$ which minimizes the sum of the squared differences between the initial and final moments when tracking through one period. We utilized two different optimization methods. The first was SciPy&#39;s nonlinear least squares, which worked really well in most cases. The second strategy was to track the beam over a number of terms and compute the average of $ mathbf{p}$, then use that as the seed for the next round. This figure shows that method converging toward the solution after a few iterations. The relevant code is found here. . . Example application . We demonstrated the matching routine in a FODO lattice, just like the one in the earlier animations. We also tried some variations like making the horizontal focusing stronger than the vertical, tilting the quadrupoles, and adding a longitudinal magnetic field. In the paper I write an analysis of each case, but I guess that would be a bit boring to include here. I can just show what the matched beam looks like instead. I&#39;ll also only look at one case: the simple FODO lattice. Here are the 2D phase space projections as a function of position. . . The colors are for different beam intensities — the darkest curve is zero intensity and the brightest curve is the highest intensity. The main takeaway is that, although space charge seems to scale the size of the matched beam, the evolution looks quite similar. There are differences, however. Here are the x and y beam sizes, emittances, and the difference between the x and y phases ($ nu$). . . In the top two subplots, the dashed lines represent $y$ and the solid lines represent $x$. The difference that wasn&#39;t super evident in the animation is that the $x$ and $y$ emittances are oscillatory in the matched beam when space charge is turned on. The bottom plot sort of represents the roundness of the beam: $ nu$ = 0 or 180 degrees is a complete correlation between $x$ and $y$, while $ nu$ = 90 is no correlation. Space charge tends to pull these curves backtoward 90 degrees, which is apparent in the animation. Finally, I should note that there is another solution for the same lattice if we choose to use the other eigenvector; this solution rotates clockwise, but is otherwise the same. The scripts and analysis notebooks used to generate these plots, as well as the other plots in the paper, can be found on GitHub. . Conclusion . Hopefully this gives a flavor of what went into this paper. Of course I&#39;ve left out quite a bit, including a discussion of the relevance of the cases we tried. These findings have practical implications for future experiments in the SNS, but I won&#39;t discuss those here. I&#39;m now contributing to a different project: to create an approximate Danilov distribution in the SNS. I will be working on implementing a method to measure the beam&#39;s covariance matrix (Eq. (4)). . . [1] A. Hoover, N. J. Evans, &amp; J. A. Holmes, &quot;Computation of the matched envelope of the Danilov distribution,&quot; Phys. Rev. Accel. Beams 24:044201 (2021). https://doi.org/10.1103/PhysRevAccelBeams.24.044201. . [2] I. Hofmann &amp; O. Boine-Frankenheim, &quot;Parametric instabilities in 3D periodically focused beams with space charge,&quot; Phys. Rev. Accel. Beams 20:014202 (2017). https://doi.org/10.1103/PhysRevAccelBeams.20.014202. . [3] J. Galambos, S. Danilov, D. Jeon, J. Holmes, D. Olsen, J. Beebe-Wang, &amp; A. Luccio, &quot;ORBIT-a ring injection code with space charge,&quot; Proceedings of the 1999 Particle Accelerator Conference (Cat. No.99CH36366) (1999), pp. 3143–3145 vol.5. https://doi.org/10.1109/PAC.1999.792230. . [4] S. M. Lund &amp; B. Bukh, &quot;Stability properties of the transverse envelope equations describing intense ion beam transport,&quot; Phys. Rev. ST Accel. Beams 7:024801 (2004). https://doi.org/10.1103/PhysRevSTAB.7.024801. . [5] V. Danilov, S. Cousineau, S. Henderson, &amp; J. Holmes, &quot;Self-consistent time dependent two dimensional and three dimensional space charge distributions with linear force,&quot; Phys. Rev. ST Accel. Beams 6:094202 (2003). https://doi.org/10.1103/PhysRevSTAB.6.094202. .",
            "url": "https://austin-hoover.github.io/blog/accelerators/space%20charge/differential%20equations/publications/2021/05/13/matched_Danilov_dist.html",
            "relUrl": "/accelerators/space%20charge/differential%20equations/publications/2021/05/13/matched_Danilov_dist.html",
            "date": " • May 13, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Authorship identification",
            "content": "In this post, I&#39;ll summarize a paper by John Houvardas and Efstathios Stamatatos titled N-Gram Feature Selection for Authorship Identification [1]. The topic of the paper is authorship identification, that is, to identify the author of an unlabeled document given a list of possible authors and some sample of each author&#39;s writing. I&#39;ll first motivate the problem of authorship identification, then briefly introduce the relevant statistical methods, and finally summarize and implement the methods in the paper. My goal is to reproduce the authors&#39; results. . . Stylometry . Motivation: the Federalist Papers . The Federalist Papers are an important collection of 85 essays written by Hamilton, Madison, and Jay during 1787 and 1788. The essays were published under the alias &quot;Plubious&quot;, and although it became well-known that the three men were involved, the authorship of each individual paper was kept hidden for over a decade. This was actually in the interest of both Hamilton and Madison; both were politicians who had changed positions on a number of issues and didn&#39;t want their political opponents to use their own words against them. Days before his death, Hamilton allegedly wrote down who he believed to be the correct author of each essay, claiming over 60 for himself. Madison waited a number of years before publishing his own list, and in the end there were 12 essays claimed by both Madison and Hamilton. Many interesting details on the controversy can be found in [2]. . . Alexander Hamilton (left) and James Madison (right). Credit: Wikipedia. There are a few ways one might go about resolving this dispute. One approach is to analyze the actual content of the text. For example, perhaps an essay draws from a reference with which only Madison was intimately familiar, or maybe an essay is similar to Hamilton&#39;s previous work. This approach was used many times over the next 150 years, but perhaps the final word on the subject was by Adair, who in 1944 concluded that Madison likely wrote all 12 essays. An alternative approach is to analyze the style of the text. For example, maybe Madison used many more commas than Hamilton. The field of stylometry attempts to statistically quantify these stylistic differences. David Holmes writes the following about stylometry [3]: . At its heart lies an assumption that authors have an unconscious aspect to their style, an aspect which cannot consciously be manipulated but which possesses features which are quantifiable and which may be distinctive. . I think this a valid assumption. The question is which features best characterize the author&#39;s style and which methods are best to use in the analysis of these features. Let&#39;s go back in time a bit to see how stylometry has developed over the past 150 years. . History . The physicist Thomas Mendenhall is considered the first to statistically analyze large literary texts. He presented the following interesting idea in an 1887 paper titled The Characteristic Curves of Composition [4]: it is known that each chemical element emits light with a unique distribution of wavelengths when it is heated; perhaps each author has a unique distribution of word lengths in the texts they have written. It&#39;s a really cool idea, and I highly recommend reading his original paper. Mendenhall tallied word lengths by hand for various books, usually in batches of 1000 words or so. Here is Fig. 2 from his paper which shows the characteristic curves for a few excerpts of Oliver Twist. . . Distribution of word lengths in &quot;Oliver Twist&quot;. Each curve is for a different sample of 1000 words. From [4]. He showed that these curves are very interesting and that they do reveal similarities between different works by the same author. The use of these statistics for authorship identification was left for future work. . The next significant advance in the statistical analysis of text was made by Zipf in 1932. Zipf found an interesting relationship between an integer $k$ and the frequency $f(k)$ of the $k$th most frequent word. This is often called a rank-frequency relationship, where $k$ is the rank. The scaling law can be written as . $$ f(k) propto k^{-1}. tag{1} $$ . The idea expressed by this law is that short words are much more frequent than large words. Surprisingly, the law holds up very well, albeit not perfectly, for most texts. Why this is the case is still unknown; a comprehensive review of the current state of the law can be found in [5]. The law also shows up in other situations such as national GDP: . . National GDPs appear to be moving toward the prediction by Zipf&#39;s Law (red line). From [6]. The success of Zipf&#39;s Law was very encouraging and led to a flurry of new mathematical models. Stylometry reached a landmark case in the 1960&#39;s when researchers used the frequency distributions of short function words — words we don&#39;t think about too much like &quot;upon&quot; or &quot;therefore&quot; — to support Adair&#39;s conclusion that Madison wrote the 12 disputed Federalist Papers. At the end of the day, however, models created in the spirit of Zipf&#39;s Law are probably doomed to fail. The &quot;true&quot; underlying model must be very complex due to its dependence on human psychology. There are now many algorithms available which instead build predictive models directly from data, and these can be readily applied to the problem of authorship identification. Here we focus on the use of the Support Vector Machine (SVM). . Support Vector Machine (SVM) . I include here the basic idea behind the SVM approach. There are a huge number of resources which go into the details (such as [7]). I&#39;ll follow the Wikipedia page since it has a nice short summary. . Maximum margin hyperplane . Consider a linear binary classifier, i.e., a plane which splits the data into two classes. The equation for a plane in any number of dimensions is . $$ y( mathbf{x}) = mathbf{w}^T mathbf{x} + w_0 = 0 tag{2}. $$ . This plane is called the decision surface; points are assigned to class 1 if $y( mathbf{x}) &gt; 0$ or class 2 when $y( mathbf{x}) &lt; 0$. Suppose the data is linearly separable (able to be completely split in two) and that we&#39;ve found a plane which correctly splits the data. We could then scale the coordinates such that all points with $y( mathbf{x}) ge 1$ belong to class 1 and all points with $y( mathbf{x}) le -1$ belong to class 2. The separating plane then sits in the middle as in the following figure. . . Maximum margin separating plane. Credit: Wikipedia. Notice that the plane could be rotated while still correctly splitting the existing data; the SVM attempts to find the optimal plane by maximizing the orthogonal distance from the decision plane to the closest point. This is known as the margin, and it can be shown that it is inversely proportional to the magnitude of $ mathbf{w}$. Thus, the SVM tries to minimize $| mathbf{w}|^2$ subject to the constraint that all points are correctly categorized. New data is then assigned based on this optimal boundary. . Some datasets won&#39;t be linear separable, in which case we can add a penalty function in order to minimize the number of miscategorized points. So, for N samples we minimize . $$ frac{1}{2}| mathbf{w}|^2 + C sum_{i=1}^{N}{ max left[0, 1 - {t_i y( mathbf{x}_i)} right]} . tag{3}$$ . where $t_i$ is the true class of point $i$ ($ pm 1$) and $C$ is a positive constant. Correctly classified points don&#39;t contribute anything to the sum since $t_i y( mathbf{x}_i)$ will be greater than or equal to one. Let&#39;s try this on non-linearly separable data sampled from two Gaussian distributions in 2D space. The Python package scikit-learn has a user-friendly interface for the SVM implementation in LIBLINEAR which we use here. . import numpy as np from sklearn import svm import matplotlib.pyplot as plt import proplot as plot # Create two Gaussian distributions np.random.seed(0) n = 200 X = np.vstack([np.random.normal(size=(n, 2), loc=[0, 0], scale=2.0), np.random.normal(size=(n, 2), loc=[5, 5], scale=2.5)]) y = n * [1] + n * [-1] # Find SVM decision boundary clf = svm.LinearSVC(C=1) clf.fit(X, y) # Plot the data def despine(ax): ax.format(xticks=[], yticks=[]) for side in [&#39;left&#39;, &#39;right&#39;, &#39;top&#39;, &#39;bottom&#39;]: ax.spines[side].set_visible(False) def padded_ranges(X, pad=0.5): xmin, ymin = np.min(X, axis=0) - pad xmax, ymax = np.max(X, axis=0) + pad return (xmin, xmax), (ymin, ymax) def plot_dec_boundary(ax, clf, xlim=(-100, 100), i=0, **kws): w0 = clf.intercept_ if type(clf.intercept_) is float else clf.intercept_[i] (w1, w2) = clf.coef_[i] line_x = np.array(xlim) line_y = -(w1 / w2) * line_x - (w0 / w2) kws.setdefault(&#39;c&#39;, &#39;black&#39;) ax.plot(line_x, line_y, **kws) def plot_dec_regions(ax, clf, xlim, ylim, nsteps=500, **kws): (xmin, xmax), (ymin, ymax) = xlim, ylim xx, yy = np.meshgrid(np.linspace(xmin, xmax, nsteps), np.linspace(ymin, ymax, nsteps)) Z = np.c_[xx.ravel(), yy.ravel()] y_pred = clf.predict(Z) zz = y_pred.reshape(xx.shape) kws.setdefault(&#39;alpha&#39;, 0.05) kws.setdefault(&#39;zorder&#39;, 0) ax.contourf(xx, yy, zz, **kws) fig, ax = plot.subplots() despine(ax) ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y) xlim, ylim = padded_ranges(X, pad=0.5) plot_dec_boundary(ax, clf) plot_dec_regions(ax, clf, xlim, ylim) ax.format(xlim=xlim, ylim=ylim) ax.annotate(&#39;Decision nboundary&#39;, xy=(0.55, 0.02), xycoords=&#39;axes fraction&#39;); . . The points are colored by their true classes, and the background is shaded according to the SVM prediction at each point. It can be important to try at least a few different values of $C$, which determines the trade-off between correctly classifying all samples and maximizing the margin, and to observe the effect on the accuracy as well as the algorithm convergence. Parameters such as this one which change the algorithm behavior but aren&#39;t optimized by the algorithm itself are commonly known as hyperparameters. . Kernel trick . In some cases the linear model is going to be bad; a frequently used example is &quot;target&quot; dataset. . n = 400 r1 = np.sqrt(np.random.uniform(0.0, 0.2, size=(n,))) r2 = np.sqrt(np.random.uniform(0.5, 1.0, size=(n,))) t1 = np.random.uniform(0, 2*np.pi, size=(n,)) t2 = np.random.uniform(0, 2*np.pi, size=(n,)) X = np.vstack([np.vstack([r1*np.cos(t1), r1*np.sin(t1)]).T, np.vstack([r2*np.cos(t2), r2*np.sin(t2)]).T]) y = n * [1] + n * [-1] xlim, ylim = padded_ranges(X, pad=0.1) fig, ax = plot.subplots() despine(ax) ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y) ax.format(xlim=xlim, ylim=ylim) . . A line obviously won&#39;t work; ideally we would draw a circle around the inner cluster to split the data. The kernel trick can be used to alleviate this problem by performing a transformation to a higher dimensional space in which the data is linearly separable. For example, consider the transformation . $$ (x_1, x_2) rightarrow (x_1^2, x_2^2, sqrt{2} x_1 x_2) . tag{4}$$ . from plotly import graph_objects as go x1, x2 = X.T u = x1**2 v = np.sqrt(2) * x1 * x2 w = x2**2 fig = go.Figure(data=go.Scatter3d(x=u, y=v, z=w, mode=&#39;markers&#39;, marker=dict(color=y, size=3, opacity=0.5))) fig.update_scenes(xaxis_visible=False, yaxis_visible=False, zaxis_visible=False) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . It&#39;s clear from rotating this plot (it&#39;s interactive) that the transformed data can be split with a 2D plane. This need not be the transformation used by the SVM — in fact, many transformations can be used — but it clearly demonstrates the idea. The linear boundary in the transformed space can then be transformed to a nonlinear boundary in the original space. One way to plot this boundary is to make a prediction on a grid of points, then make a contour plot (the boundary is shown in grey). . clf = svm.SVC(kernel=&#39;rbf&#39;) clf.fit(X, y) fig, ax = plot.subplots() despine(ax) ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y) ax.format(xlim=xlim, ylim=ylim) plot_dec_regions(ax, clf, xlim, ylim) . . There are still several advantages to the linear SVM. First, it is much faster to train, and second, the kernel trick may be unnecessary for high-dimensional data. As we&#39;ll see, text data can involve a large number of very high-dimensional samples, so we&#39;ll be sticking with linear kernels. . Multi-class . A binary classifier can also be used for multi-class problems. Here we use the one-versus-rest(OVR) approach. Suppose we had $N$ classes denoted by $c_1$, $c_2$ ... $c_N$. In the OVR approach we train $N$ different classifiers; the ith classifier $L_i$ tries to split the data into two parts: $c_i$ and not $c_i$. Then we observe a new point and ask each classifier $L_i$ how confident it is that the point belongs to $c_i$. The point is assigned to the class with the highest score. We can extend our previous example to three Gaussian distributions to get a sense of how the decision boundaries are formed. . # Create three Gaussian distributions np.random.seed(0) n = 200 X = np.vstack([np.random.normal(size=(n, 2), loc=[0, 0], scale=2.0), np.random.normal(size=(n, 2), loc=[5, 5], scale=2.5), np.random.normal(size=(n, 2), loc=[-6, 6], scale=2.5)]) y = n * [1] + n * [0] + n * [-1] # Find SVM decision boundary clf = svm.LinearSVC(C=1, multi_class=&#39;ovr&#39;, max_iter=10000) clf.fit(X, y) # Plot the data fig, ax = plot.subplots() despine(ax) ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y, cmap=plot.Colormap((&#39;pink9&#39;, &#39;grey&#39;, &#39;darkgreen&#39;))) # Plot decision boundary xlim, ylim = padded_ranges(X) for i in range(3): ls = [&#39;-&#39;, &#39;--&#39;, &#39;dotted&#39;][i] plot_dec_boundary(ax, clf, i=i, ls=ls) ax.format(xlim=xlim, ylim=ylim) ax.legend(labels=[&#39;class {} boundary&#39;.format(i) for i in range(1, 4)], ncols=1, loc=(1.1, 0.6)); plot_dec_regions(ax, clf, xlim, ylim, alpha=0.1) . . The same idea holds with more classes and dimensions. Notice that there are some regions which are claimed by multiple classifiers, so it&#39;s not a perfect method. . N-grams and feature selection methods . As I mentioned in the introduction, the paper I&#39;m following is called N-Gram Feature Selection for Authorship Identification. In short, the paper used n-gram frequencies (defined in a moment) as features in the classification task and developed a new method to select the most significant or &quot;dominant&quot; n-grams. This was tested on a collection of short news articles. Let&#39;s step through their method. . Data set description . The Reuters Corpus Volume 1 (RCV1) data set is a big collection of news articles labeled by topic. Around 100,000 of these have known authors, and there are around 2000 different authors. A specific topic was chosen, and only authors who wrote at least one article which fell under this topic were considered. From this subset of authors, the top 50 in terms of number of articles written were chosen. 100 articles from each author were selected — 5000 in total — and these were evenly split into a training and testing set. The resulting corpus is a good challenge for authorship identification because the genre is invariant across documents and because the authors write about similar topics. Hopefully this leaves the author&#39;s style as the primary distinguishing factor. The data set can be downloaded here. The files are organized like this: . . There are plenty of functions available to load the data and to extract features from it, but I&#39;ll do everything manually just for fun. To load the data, let&#39;s first create two lists of strings, texts_train and texts_test, corresponding to the 2500 training and testing documents. The class id and author name for each document are also stored. . from os import listdir from os.path import join def load_files(outer_path): texts, class_ids, class_names = [], [], [] for class_id, folder in enumerate(sorted(listdir(outer_path))): folder_path = join(outer_path, folder) for filename in listdir(folder_path): class_ids.append(class_id) class_names.append(folder) file = open(join(folder_path, filename), &#39;r&#39;) text = file.read().replace(&#39; &#39;, &#39;_&#39;) texts.append(text) file.close() return texts, class_ids, class_names texts_train, y_train, authors_train = load_files(&#39;reuters_data/train&#39;) texts_test, y_test, authors_test = load_files(&#39;reuters_data/test&#39;) . Author Name Author ID Training Text . 0 AaronPressman | 0 | A_group_of_leading_trademark_specialists_plans... | . 1 AaronPressman | 0 | Prospects_for_comprehensive_reform_of_U.S._ban... | . 2 AaronPressman | 0 | An_influential_economic_research_group_is_prep... | . 3 AaronPressman | 0 | The_Federal_Communications_Commission_proposed... | . 4 AaronPressman | 0 | An_international_task_force_charged_with_resol... | . ... ... | ... | ... | . 2495 WilliamKazer | 49 | China_could_list_more_railway_companies_and_is... | . 2496 WilliamKazer | 49 | The_choice_of_Singapore_for_the_listing_of_Chi... | . 2497 WilliamKazer | 49 | China_ushered_in_1997,_a_year_it_has_hailed_as... | . 2498 WilliamKazer | 49 | China_on_Tuesday_announced_a_ban_on_poultry_an... | . 2499 WilliamKazer | 49 | China&#39;s_leaders_have_agreed_on_a_need_to_stimu... | . 2500 rows × 3 columns . The following histogram shows the distribution of document lengths in the training set; it&#39;s expected that the short average document length will greatly increases the difficulty of the classification task relative to longer works such as books. . word_counts = [len(text) for text in texts_train] fig, ax = plot.subplots(figsize=(5, 1.5)) ax.hist(word_counts, bins=&#39;auto&#39;, color=&#39;k&#39;, density=True) ax.format(xlabel=&#39;Document length (characters)&#39;, ylabel=&#39;Num. docs&#39;, yticks=[], title=&#39;Distribution of document lengths in training set&#39;) ax.annotate(r&#39;mean = {:.0f}&#39;.format(np.mean(word_counts)), xy=(0.8, 0.5), xycoords=&#39;axes fraction&#39;) ax.annotate(&#39;std = {:.0f}&#39;.format(np.std(word_counts)), xy=(0.8, 0.3), xycoords=&#39;axes fraction&#39;); from scipy.stats import norm mean, std = norm.fit(word_counts) x = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 1000) y = norm.pdf(x, mean, std) ax.plot(x, y, c=&#39;red&#39;, alpha=0.4); ax.legend(labels=[&#39;Gaussian fit&#39;], frameon=False); . . N-grams . An obvious feature candidate is word frequency; a less obvious one is n-gram frequency. A character n-gram is a string of length n. For example, the 3-grams contained in red_bike! are red, ed_, d_b, _bi, bik, ike, ke!. These shorter strings may be useful because they capture different aspects of style such as the use of punctuation or certain prefixes/suffixes. They also remove any ambiguities in word extraction and work for all languages. In order to use these features in the SVM classifier, we need to create a feature matrix $X$ where $X_{ij}$ is the frequency of the jth n-gram in the ith document. Thus, each document is represented as a vector in $k$ dimensional space, where $k$ is the number of unique n-grams selected from the training documents. We&#39;ll also normalize each vector so that all points are mapped onto the surface of the $k$-dimensional unit sphere while preserving the angles between the vectors; this should help the SVM performance a bit. . from collections import Counter def get_ngrams(text, n): return [text[i - n : i] for i in range(n, len(text) + 1)] def get_ngrams_in_range(text, min_n, max_n): ngrams = [] for n in range(min_n, max_n + 1): ngrams.extend(get_ngrams(text, n)) return ngrams def sort_by_val(dictionary, max_items=None, reverse=True): n_items = len(dictionary) if max_items is None or max_items &gt; n_items: max_items = n_items sorted_key_val_list = sorted(dictionary.items(), key=lambda item: item[1], reverse=reverse) return {k: v for k, v in sorted_key_val_list[:max_items]} class NgramExtractor: def __init__(self, ngram_range=(3, 5)): self.vocab = {} self.set_ngram_range(ngram_range) def set_ngram_range(self, ngram_range): self.min_n, self.max_n = ngram_range def build_vocab(self, texts, max_features=None): self.vocab, index = {}, 0 for n in range(self.min_n, self.max_n + 1): ngrams = [] for text in texts: ngrams.extend(get_ngrams(text, n)) counts = sort_by_val(Counter(ngrams), max_features) for ngram, count in counts.items(): self.vocab[ngram] = (index, count) index += 1 def create_feature_matrix(self, texts, norm_rows=True): X = np.zeros((len(texts), len(self.vocab))) for text_index, text in enumerate(texts): ngrams = get_ngrams_in_range(text, self.min_n, self.max_n) for ngram, count in Counter(ngrams).items(): if ngram in self.vocab: term_index = self.vocab[ngram][0] X[text_index, term_index] = count if norm_rows: X = np.apply_along_axis(lambda row: row / np.linalg.norm(row), 1, X) return X . Now we need to decide which value(s) of n to use as features. Let&#39;s look at the distribution of n-grams in the training documents. . extractor = NgramExtractor(ngram_range=(1, 15)) extractor.build_vocab(texts_train) len_counts = Counter([len(ngram) for ngram in extractor.vocab.keys()]) fig, axes = plot.subplots(ncols=2, sharex=False) x, y = zip(*len_counts.items()) axes[0].barh(x, y, color=&#39;k&#39;) axes[1].barh(x, np.log10(y), color=&#39;k&#39;) axes.format(yticks=x, ytickminor=False, ylabel=&#39;n&#39;, suptitle=&#39;Distribution of character n-grams in training text&#39;) axes[0].format(xlabel=&#39;Counts&#39;, xformatter=&#39;sci&#39;) axes[1].format(xlabel=&#39;log$_{10}$(Counts)&#39;, xformatter=&#39;sci&#39;) for ax in axes: ax.grid(axis=&#39;x&#39;) . . The total number of n-grams with 1 $ le$n $ le$ 15 is about 31 million; training a classifier on data with this number of dimensions is probably infeasible, and even more so on a larger data set. Previous studies have apparently had success with fixing the value of n to be either 3, 4, or 5, so the authors chose to restrict their attention to these values. Their new idea was to use all n-grams in the range 3 $ le$n $ le$ 5. This leaves a few hundred thousand features. . The next section will discuss statistical methods to prune the features; for now, though, we&#39;ll implement the simple method of keeping the $k$ most frequent across all the training documents. As long as this doesn&#39;t affect the accuracy too much, we reap the benefits of a reduction in computational time and the ability to fix the feature space dimensionality for comparison of different feature types. To see why many low frequency terms may be unimportant, suppose one of the authors wrote a single article about sharks in the training set. The the term &quot;shark&quot; would have a small global frequency and be very useful in the training set since no other writers write about sharks, but it&#39;s probabably a good idea to discard it since its unlikely to appear in the testing set. We must be careful, however, because some low-frequency terms could be important. These are probably terms that an author uses rarely but consistently over time. Maybe they like to use &quot;incredible&quot; as an adjective; the global frequency of &quot;incred&quot; would be much less than, say, &quot;that_&quot;, but it&#39;s valuable because its frequency distribution will likely be the same in future writing. A quick test on our data set shows that $k$ = 15,000 is a good number. Let&#39;s try this out on the 15,000 most frequent 3-grams. . ngram_range = (3, 3) max_features = 15000 norm_rows = True extractor = NgramExtractor(ngram_range) extractor.build_vocab(texts_train, max_features) X_train = extractor.create_feature_matrix(texts_train, norm_rows) X_test = extractor.create_feature_matrix(texts_test, norm_rows) . Here are some of the values in X_train. The columns have been sorted by descending frequency from left to right. . _th he_ the _in ed_ _to ng_ ing to_ _of ... Agi gip L_I +12 n t t _VW VW_ ig. kw_ d-n . 0 0.129339 | 0.129339 | 0.086226 | 0.129339 | 0.086226 | 0.043113 | 0.129339 | 0.129339 | 0.043113 | 0.086226 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 0.278956 | 0.266276 | 0.272616 | 0.088759 | 0.088759 | 0.044379 | 0.145818 | 0.133138 | 0.044379 | 0.069739 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 0.308339 | 0.247880 | 0.266018 | 0.108825 | 0.102780 | 0.102780 | 0.120917 | 0.114871 | 0.102780 | 0.096734 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 0.244807 | 0.244807 | 0.231207 | 0.102003 | 0.108803 | 0.102003 | 0.061202 | 0.068002 | 0.108803 | 0.108803 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 0.231920 | 0.248486 | 0.207072 | 0.066263 | 0.149092 | 0.115960 | 0.066263 | 0.066263 | 0.074546 | 0.082829 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2495 0.148444 | 0.197926 | 0.141375 | 0.091894 | 0.077756 | 0.127238 | 0.247407 | 0.162582 | 0.106032 | 0.162582 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2496 0.264067 | 0.193649 | 0.211254 | 0.146704 | 0.158440 | 0.082154 | 0.146704 | 0.228858 | 0.082154 | 0.193649 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2497 0.346437 | 0.276067 | 0.276067 | 0.092022 | 0.151566 | 0.113675 | 0.276067 | 0.151566 | 0.102849 | 0.151566 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2498 0.286280 | 0.293262 | 0.251367 | 0.125684 | 0.181543 | 0.111719 | 0.104736 | 0.076807 | 0.104736 | 0.160596 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2499 0.334275 | 0.227915 | 0.205123 | 0.159540 | 0.151943 | 0.129152 | 0.091166 | 0.113957 | 0.121554 | 0.189929 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2500 rows × 15000 columns . We can now feed this array to the SVM and make predictions on the testing data. I&#39;ll keep the $C$ parameter fixed at $C = 1$ in all cases since this is what is done in the paper (I tried a few different values of $C$ and there wasn&#39;t a large effect on the accuracy). Here is the confusion matrix obtained after training and testing: . from sklearn.metrics import accuracy_score, confusion_matrix import seaborn as sns clf = svm.LinearSVC(C=1) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) acc = accuracy_score(y_test, y_pred) cmat = confusion_matrix(y_test, y_pred) fig, ax = plt.subplots(figsize=(4.5, 3.5)) sns.heatmap(cmat, ax=ax, cmap=&#39;binary&#39;, cbar_kws=dict(label=&#39;Number of documents&#39;)) ax.set_title(&#39;Confusion Matrix (accuracy = {:.3f})&#39;.format(acc)) ax.set_ylabel(&#39;True class&#39;); ax.set_xlabel(&#39;Predicted class&#39;); . . Feature selection . In the rest of this post, we&#39;ll study how to use statistical methods to further eliminate features from this initial set of 15,000. This process of selecting features which are &quot;best&quot; in a statistical sense is known as feature selection. . Information gain . A classical statistical measure of feature &quot;goodness&quot; is called information gain (IG). The idea is that knowing whether or not a term t is found in a document of a known class $c$ gives information about $c$, and that some terms will contribute more information than others. The information gain can be written as [8] . $$ IG(t) = p(t) sum_{i=1}^{m}p(c_i | t) log p(c_i | t) + p( bar{t}) sum_{i=1}^{m}p(c_i | bar{t}) log p(c_i | bar{t}) - sum_{i=1}^{m}p(c_i) log p(c_i). tag{5}$$ . The probability of choosing term $t$ out of all terms in the corpus is given by $p(t)$, and $p(t) + p( bar{t}) = 1$. Similarly, $p(c_i)$ is the probability that a randomly chosen document belongs to class $c_i$, and $p(c_i) + p( bar{c_i}) = 1$. The probability that a document belongs to $c_i$ given that it contains $t$ is $p(c_i | t)$, or $p(c_i | bar{t})$ if it doesn&#39;t contain $t$. The strategy is then to keep the terms with the highest information gain scores. . class InfoGainSelector: def __init__(self): self.idx = None def fit(self, X, y): # Compute probability distributions n_docs, n_terms = X.shape n_classes = len(np.unique(y)) P_c_and_t = np.zeros((n_classes, n_terms)) for doc_index, class_index in enumerate(y): P_c_and_t[class_index, :] += (X[doc_index, :] &gt; 0).astype(int) P_c_and_t /= np.sum(P_c_and_t) P_t = np.sum(P_c_and_t, axis=0) P_c = np.sum(P_c_and_t, axis=1) P_c_given_t = P_c_and_t / P_t P_c_given_tbar = 1 - (1 - P_c_and_t) / (1 - P_t) # Compute information gain for each feature def XlogX(X): return X * np.log2(X, out=np.zeros_like(X), where=(X &gt; 0)) scores = np.zeros(n_terms) scores += np.sum(P_t * XlogX(P_c_given_t), axis=0) scores += np.sum((1 - P_t) * XlogX(P_c_given_tbar), axis=0) scores -= np.sum(XlogX(P_c)) self.idx = np.argsort(scores) def select(self, X, k=-1): return X[:, self.idx[:k]] . We&#39;ll now compare 4 sets of 15,000 features: 3-grams, 4-grams, 5-grams, and equal parts 3/4/5-grams, each time using IG to select the best $k$ features and plotting the accuracy vs. $k$. I&#39;ll start from $k$ = 1 to 200. . extractor = NgramExtractor() selector = InfoGainSelector() clf = svm.LinearSVC(C=1) def compare_acc_ig(ngram_ranges, kmin, kmax, kstep): n_keep = np.arange(kmin, kmax + kstep, kstep).astype(int) accuracies = np.zeros((len(ngram_ranges), len(n_keep))) for i, ngram_range in enumerate(ngram_ranges): extractor.set_ngram_range(ngram_range) max_features = 5000 if ngram_range == (3, 5) else 15000 extractor.build_vocab(texts_train, max_features) X_train = extractor.create_feature_matrix(texts_train) X_test = extractor.create_feature_matrix(texts_test) selector.fit(X_train, y_train) for j, k in enumerate(n_keep): X_train_red = selector.select(X_train, k) X_test_red = selector.select(X_test, k) clf.fit(X_train_red, y_train) y_pred = clf.predict(X_test_red) accuracies[i, j] = accuracy_score(y_test, y_pred) return accuracies def plot_accs(accuracies, kmin, kmax, kstep): fig, ax = plot.subplots(figsize=(4, 3)) for i in range(4): m = [&#39;D&#39;, &#39;s&#39;, &#39;^&#39;, &#39;s&#39;][i] mfc = [None, None, &#39;w&#39;, &#39;w&#39;][i] ax.plot(np.arange(kmin, kmax + kstep, kstep), accuracies[i, :], marker=m, mew=1, mfc=mfc) ax.format(title=&#39;Information Gain feature selection&#39;) ax.format(xlabel=&#39;Number of features selected (k)&#39;) ax.format(ylabel=&#39;Accuracy&#39;) ax.legend(labels=[&#39;n = 3&#39;, &#39;n = 4&#39;, &#39;n = 5&#39;, &#39;n = (3, 4, 5)&#39;], ncols=1); return ax . . The accuracy at $k$ = 1 is 0.04, so using the feature with the highest IG score is actually twice as effective as random guessing! By the end of the plot 3-grams and variable length n-grams have taken a clear leaad, with 5-grams in last place. The performance gap between the different n-grams also appears to be growing with $k$. The next region we&#39;ll look at is $200 le k le 2000$. . Now the gap is decreasing as we approach an upper performance limit at higher $k$, especially for 3-grams. We&#39;ll now look at the region which is plotted in the paper: $2,000 le k le 10,000$. . One interesting thing is that 5-grams make a big jump from last place to first place. I&#39;m not sure if I have any deep insights into this behavior, but it&#39;s interesting that the best n-gram to choose depends on the number of features selected. Now, I should compare with Fig. 1 from the paper: . . The first difference is the maximum achieved accuracy which is a few percentage points higher. The second difference is that the authors found 3-grams to be worst at low $k$ and best at high $k$.and the opposite for 5-grams. I&#39;ll leave this as an open problem for now. . LocalMaxs algorithm . Let&#39;s look at the top IG scoring n-grams from from the variable-length feature set. . extractor.set_ngram_range((3, 5)) extractor.build_vocab(texts_train, max_features=5000) X_train = extractor.create_feature_matrix(texts_train) X_test = extractor.create_feature_matrix(texts_test) selector.fit(X_train, y_train) def get_term(i): for key, (idx, count) in extractor.vocab.items(): if idx == i: return key for rank, i in enumerate(selector.idx[:10], start=1): print(&#39;{:02}. {}&#39;.format(rank, get_term(i))) . . 01. _th 02. _the_ 03. the_ 04. _to 05. _the 06. _in 07. he_ 08. ed_ 09. the 10. on_ . Notice all the variants of the which were included. IG has no way of knowing that these are basically the same. This motivates the definition of something called &quot;glue&quot;. Consider the word bigram Amelia Earhart. These two words are very likely to be found next to each other and could probably be treated as a single multi-word unit; it is as if there is a glue holding the two words together. The amount of glue is probably higher than that between, say, window and Earhart. A technique has been developed to quantify this glue and extend its calculation to word n-grams instead of just word bi-grams [9]. The same idea can then be applied to character n-grams. . Let $g(C)$ be the glue of character n-gram $C = c_1 dots c_n$. Assuming we had a way to calculate the glue, how could this concept be used for feature selection? One solution is called the LocalMaxs algorithm. First define an antecedent $ant(C)$ as an (n-1)-gram which is contained in $C$, e.g., &quot;string&quot; $ rightarrow$ &quot;strin&quot; or &quot;tring&quot;. Then define a successor $succ(C)$ as an (n+1)-gram which contains $C$, e.g., &quot;string&quot; $ rightarrow$ &quot;strings&quot; or &quot;astring&quot;. C is selected as a feature if . $$ g(C) ge g(ant(C)) , , and , , g(C) &gt; g(succ(C)) tag{6}$$ . for all ant(C) and succ(C). Since we&#39;re dealing with 3 $ le$ n $ le$ 5, only the latter condition is checked if n = 3, and only the former condition is checked for n = 5. Eq. (6) says that the glue of a selected feature shouldn&#39;t increase by adding a character to or removing a character from the start or end of the n-gram, i.e., the glue is at a local maximum with respect to similar n-grams. Now that the selection criteria are established, we can move on to calculating the glue. Here there are several options, but the one used in the paper is called symmetrical conditional probability (SCP). If we have a bigram $C = c_1c_2$, then . $$ SCP(c_1c_2) = p(c_1|c_2) cdot p(c_2|c_1) = frac{p(c_1,c_2)^2}{p(c_1)p(c_2)}, tag{7}$$ . so SCP is a measure of how likely one character is given the other and vice versa. This formula can be applied to an n-gram $C = c_1 dots c_n$ by performing a pseudo bigram transformation, which means splitting the n-gram into two parts at a chosen dispersion point; for example, &quot;help&quot; could be split as &quot;h*elp&quot;, &quot;he*lp&quot;, or &quot;hel*p&quot;, where * is the dispersion point. Splitting $C$ as $c_1 dots c_{n-1}$*$c_n$ would give . $$ SCP((c_1 dots c_{n-1})c_n) = frac{p(c_1 dots c_n)^2}{p(c_1 dots c_{n-1})p(c_n)}. tag{8}$$ . Of course, the answer will depend on the dispersion point. We therefore introduce the FairSCP which averages over the possible dispersion points: . $$ FairSCP(c_1 dots c_n) = frac{p(c_1 dots c_n)^2}{ frac{1}{n-1} sum_{i=1}^{n-1} p(c_1 dots c_i)p(c_{i+1} dots c_n)}. tag{9}$$ . In summary, LocalMaxs loops through every n-gram in the vocabulary, computes the glue as $g(C) = FairSCP(C)$, and keeps the n-gram if Eq. (6) is satisfied. It differs from IG selection in that the features are not ranked, so the number of selected features is completely determined by the text. The method is implemented below. . import string def antecedents(ngram): return [ngram[:-1], ngram[1:]] def successors(ngram, characters=None): if characters is None: characters = string.printable successors = [] for character in characters: successors.append(character + ngram) successors.append(ngram + character) return successors . class LocalMaxsExtractor(NgramExtractor): def __init__(self, ngram_range=(3, 5)): super().__init__(ngram_range) self.counts_list = [] # ith element is dictionary of unique (i+1)-gram counts self.sum_counts_list = [] # ith element is the sum of `counts_list[i].values()` def build_vocab(self, texts, max_features=None): # Count all n-grams with n &lt;= self.max_n self.counts_list, self.sum_counts_list = [], [] candidate_ngrams = {} for n in range(1, self.max_n + 1): ngrams = [] for text in texts: ngrams.extend(get_ngrams(text, n)) counts = Counter(ngrams) self.counts_list.append(counts) self.sum_counts_list.append(sum(counts.values())) if self.min_n &lt;= n &lt;= self.max_n: candidate_ngrams.update(sort_by_val(counts, max_features)) self.available_characters = self.counts_list[0].keys() # Select candidate n-grams whose glue is at local maximum self.vocab, index = {}, 0 for ngram, count in candidate_ngrams.items(): if self.is_local_max(ngram): self.vocab[ngram] = (index, count) index += 1 def is_local_max(self, ngram): glue, n = self.glue(ngram), len(ngram) if n &lt; self.max_n: for succ in successors(ngram, self.available_characters): if self.glue(succ) &gt;= glue: return False if n &gt; self.min_n: for ant in antecedents(ngram): if self.glue(ant) &gt; glue: return False return True def glue(self, ngram): n = len(ngram) P = self.counts_list[n-1].get(ngram, 0) / self.sum_counts_list[n-1] if P == 0: return 0.0 Avp = 0.0 for disp_point in range(1, n): ngram_l, ngram_r = ngram[:disp_point], ngram[disp_point:] n_l, n_r = disp_point, n - disp_point P_l = self.counts_list[n_l-1].get(ngram_l, 0) / self.sum_counts_list[n_l-1] P_r = self.counts_list[n_r-1].get(ngram_r, 0) / self.sum_counts_list[n_r-1] Avp += P_l * P_r Avp /= (n - 1) return P**2 / Avp . The first thing we should do is check the the glue of the derivative n-grams the, _the, etc. . extractor = LocalMaxsExtractor(ngram_range=(3, 5)) extractor.build_vocab(texts_train) for ngram in [&#39;the&#39;, &#39;_the&#39;, &#39;the_&#39;, &#39;_the_&#39;]: glue = extractor.glue(ngram) selected = extractor.is_local_max(ngram) freq = extractor.counts_list[len(ngram) - 1][ngram] print(&#39;{:&lt;5}: glue = {:.4f}, selected = {}&#39;.format(ngram, glue, selected)) . the : glue = 0.0856, selected = True _the : glue = 0.0846, selected = False the_ : glue = 0.0748, selected = False _the_: glue = 0.0808, selected = False . It seems to be working correctly. Now we&#39;d like to compare the performance to IG. There&#39;s no way to directly compare since LocalMaxs doesn&#39;t rank features; however, it&#39;s possible to vary the size of the initial set of features from which LocalMaxs makes its selections. Below, this initial size is varied from 3,000 to 24,000 using equal parts 3/4/5 grams as features. . lm_extractor = LocalMaxsExtractor(ngram_range=(3, 5)) clf = svm.LinearSVC(C=1) max_features_list = np.arange(2000, 8000, 1000).astype(int) lm_accuracies, lm_vocabs = [], [] for max_features in max_features_list: lm_extractor.build_vocab(texts_train, max_features) X_train_red = lm_extractor.create_feature_matrix(texts_train) X_test_red = lm_extractor.create_feature_matrix(texts_test) clf.fit(X_train_red, y_train) y_pred = clf.predict(X_test_red) lm_accuracies.append(accuracy_score(y_test, y_pred)) lm_vocabs.append(lm_extractor.vocab) n_keep = [len(vocab) for vocab in lm_vocabs] ig_extractor = NgramExtractor(ngram_range=(3, 5)) ig_extractor.build_vocab(texts_train, max_features=5000) X_train = ig_extractor.create_feature_matrix(texts_train) X_test = ig_extractor.create_feature_matrix(texts_test) ig_selector = InfoGainSelector() ig_selector.fit(X_train, y_train) ig_accuracies, ig_vocabs = [], [] for lm_vocab in lm_vocabs: k = len(lm_vocab) X_train_red = ig_selector.select(X_train, k) X_test_red = ig_selector.select(X_test, k) clf.fit(X_train_red, y_train) y_pred = clf.predict(X_test_red) ig_accuracies.append(accuracy_score(y_test, y_pred)) ig_vocabs.append(extractor.vocab) fig, ax = plot.subplots(figsize=(4, 3)) ax.plot(n_keep, lm_accuracies, c=&#39;k&#39;, marker=&#39;D&#39;) ax.plot(n_keep, ig_accuracies, marker=&#39;D&#39;, c=&#39;red8&#39;) ax.format(xlabel=&#39;Number of features selected&#39;, ylabel=&#39;Accuracy&#39;, title=&#39;IG vs. LocalMaxs feature selection&#39;) ax.legend(labels=[&#39;n = (3, 4, 5) — LocalMaxs&#39;, &#39;n = (3, 4, 5) — IG&#39;], ncols=1); . . As you can see, LocalMaxs achieves a higher accuracy with the same number of features. The neat thing is that the vocabularies are totally different; for example, at the last data point, only about 15% of the n-grams are found in both sets! Let&#39;s count the number of related n-grams in the two sets, where x is related to y if x is an antecedent or successor of y. . def count_related(ngrams): count = 0 for n1 in ngrams: for n2 in ngrams: if n1 != n2 and n1 in n2: count += 1 break return count lm_ngrams = list(lm_vocabs[-1]) vocab_size = len(lm_ngrams) ig_vocab = list(ig_extractor.vocab) ig_ngrams = [ig_vocab[i] for i in ig_selector.idx[:vocab_size]] shared = len([ig_ngram for ig_ngram in ig_ngrams if ig_ngram in lm_ngrams]) print(&#39;Vocab size: {}&#39;.format(vocab_size)) print(&#39;n-grams selected by both IG and LM: {}&#39;.format(shared)) print(&#39;IG related n-grams: {}&#39;.format(count_related(ig_ngrams))) print(&#39;LM related n-grams: {}&#39;.format(count_related(lm_ngrams))) . . Vocab size: 2706 n-grams selected by both IG and LM: 426 IG related n-grams: 1413 LM related n-grams: 178 . As mentioned earlier, IG selects many related terms such as the and the_. The LocalMaxs vocabulary is much &quot;richer&quot;, as the authors put it. Here is the corresponding figure from the paper (ignore the white squares): . . For some reason, their implementation extracted way more features than mine did. I don&#39;t have access to the author&#39;s code, and I couldn&#39;t find any implementation of LocalMaxs online, so it&#39;s hard for me to say what&#39;s happening. I&#39;m happy with my implementation since it exhibits the expected behavior (less related terms, better performance at lower feature numbers). . Conclusion . This post summarized a research paper in the field of Natural Language Processing (NLP) which focused on feature selection techniques. I didn&#39;t exactly reproduce the authors&#39; results, so if anyone reads this (unlikely) and finds a mistake, I would love to know about it. . In a future post I may apply these methods to my own data set; I&#39;m particularly interested in what would happen with Chinese characters. There are, of course, a ton of different techniques and experiments to explore which involve NLP. A different problem I&#39;d like to examine is that of artist identification; the problem would be to match a collection of paintings with their painters. The Web Gallery of Art is a potential database that I found after a quick search, and I&#39;m sure there are others. This would give me the chance to learn about image classification techniques. . . [1] J. Houvardas &amp; E. Stamatatos, &quot;N-Gram Feature Selection for Authorship Identification,&quot; In J. Euzenat, &amp; J. Domingue,eds., Artificial Intelligence: Methodology, Systems, and Applications (Berlin, Heidelberg: Springer Berlin Heidelberg, 2006), pp. 77–86. . [2] A. Douglass, &quot;The Authorship of the Disputed Federalist Papers,&quot; The William and Mary Quarterly 1:97–122 (1944). . [3] D. Holmes, &quot;The Evolution of Stylometry in Humanities Scholarship,&quot; Literary and Linguistic Computing 13:111–117 (1998). . [4] T. C. Mendenhall, &quot;THE CHARACTERISTIC CURVES OF COMPOSITION,&quot; Science ns-9:237–246 (1887). https://doi.org/10.1126/science.ns-9.214S.237. . [5] S. T. Piantadosi, &quot;Zipf’s word frequency law in natural language: A critical review and future directions,&quot; Psychon Bull Rev 21:1112–1130 (2014). https://doi.org/https://doi.org/10.3758/s13423-014-0585-6. . [6] M. Cristelli, M. Batty, &amp; L. Pietronero, &quot;There is More than a Power Law in Zipf,&quot; Sci Rep 2 (2012). https://doi.org/https://doi.org/10.1038/srep00812. . [7] C. M. Bishop, Pattern Recognition and Machine Learning (Springer, 2006). . [8] Y. Yang &amp; J. O. Pedersen, &quot;A Comparative Study on Feature Selection in Text Categorization,&quot; ICML (1997). . [9] J. Silva, &quot;A Local Maxima method and a Fair Dispersion Normalization for extracting multi-word units from corpora,&quot; (2009). .",
            "url": "https://austin-hoover.github.io/blog/stylometry/machine%20learning/natural%20language%20processing/2021/04/29/authorship_identification.html",
            "relUrl": "/stylometry/machine%20learning/natural%20language%20processing/2021/04/29/authorship_identification.html",
            "date": " • Apr 29, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Nonlinear resonances",
            "content": "Most of us are familiar with the experience of pushing someone else on a playground swing. We intuitively know that we should sync our pushes with the swing oscillation frequency, which appears to be independent of the swing amplitude. This strategy employs the idea of a resonance, which is an increase of the oscillation amplitude of a system for certain driving frequencies. In this post we first review the mathematics of this simple example, then extend the machinery to the nonlinear dynamics in a particle accelerator. My goal here is to write down the main results which are relevant to accelerators in order to improve my own understanding of the topic. . . Linear resonances . Consider a mass on a spring which, if left alone, oscillates at freqency $ omega_0^2$. . . The equation of motion for $x$ is . $$ frac{d^2}{dt^2}{x} + omega_0^2x = 0. tag{1}$$ . Now consider a sinusoidal driving force $f(t) = f_0 cos( omega t)$ as well as a damping term: . $$ frac{d^2}{dt^2}{x} + b dot{x} + omega_0^2x = f_0 cos( omega t).$$ . After doing some work it can be shown that the gravitational, damping, and driving forces initially fight against each other, but in the end the driving force dominates and the position oscillates as . $$ x(t) = A cos( omega t - delta) tag{2}$$ . where . $$A^2 = frac{f_0^2}{( omega - omega_0)^2 + b omega^2}. tag{3}$$ . The figure below shows the squared amplitude as the driving frequency is varied. The maximum amplitude approaches infinity as the damping term goes to zero. . The next step is to consider what happens when the driving force is not a pure sine wave. We&#39;ll only consider periodic driving forces, and any periodic function can be written as a sum of sines and cosines of different frequencies. Assuming $f(t)$ is an even function so that we can drop the sine terms in the Fourier expansion, the equation of motion becomes . $$ ddot{x} + b dot{x} + omega_0^2 x = sum_{n=0}^{ infty} {f_n cos(n omega t)}. tag{4}$$ . The long-term solution is found by just adding up the solutions to each term in the sum: . $$x(t) = sum_{n = 0}^{ infty}{A_n cos{(n omega t - delta_n)}}, tag{5}$$ . where $A_n$ is given by Eq. (3) for the frequency $n omega$. The resonance condition will apply to each of these amplitudes individually, which means that a resonance could be excited if any component of the driving force is near the natural frequency. . Sources of nonlinearity . We&#39;re now going to apply these ideas to a particle accelerator. We&#39;ll assume small transverse oscillations, no acceleration, no deviation from the design momentum, and no particle-particle interactions. Under these assumptions, the transverse equation of motion of a particle with charge $q$ and momentum $p$ in a magnetic field $ mathbf{B} = (B_x, B_y)^T$ is . $$ x&#39;&#39; approx - frac{q}{mc beta_s gamma_s} B_y(x, y, s), tag{6} $$$$ y&#39;&#39; approx + frac{q}{mc beta_s gamma_s} B_x(x, y, s), $$Remember that $x&#39; = dx/ds$, and $s$ is the position in accelerator (from now on we&#39;ll assume a circular accelerator or &quot;ring&quot; of circumference $L$). Any 2D magnetic field can be expanded as the following infinite sum: . $$B_y - iB_x = sum_{n=1}^{ infty} left({b_n - ia_n} right) left( frac{x + iy}{r_0} right)^{n-1}, tag{7}$$ . where $r_0$ is a constant. The $b_n$ and $a_n$ terms are called the multipole coefficients and skew multipole coefficients, respectively. The $n^{th}$ term in the expansion is the field produced by $2n$ symmetrically arranged magnetic poles. . . We can see that terms with $n &gt; 2$ introduce nonlinear powers of $x$ and $y$ on the right side of Eq. (6), while terms with $n le 2$ introduce linear or constant terms. One may ask why we are considering a general magnetic field when in reality we use only dipoles and quadrupoles. The answer is two-fold. First, the best we can do in a real magnet is to make the $n &gt; 2$ terms as small as possible; they aren&#39;t zero and we need to know how they affect the motion. Second, sextupoles (and sometimes even octopoles) can be introduced intentionally. Their primary use is to correct for the fact that not all beam particles have the same momentum. . . An example of a sextupole electromagnet. Source: CERN. Perturbation analysis . The nonlinear terms in Eq. (6) eliminate any hope of an analytic solution. There are two options in situations such as these: 1) use a computer, or 2) use perturbation theory. The strategy of option 2 is to make approximations until an exact solution can be found, then to add in small nonlinear terms and see how the solution changes. The process can be repeated to solve the problem up to a certain order of accuracy. Usually this is infeasible beyond a few iterations, but it is a helpful tool for gaining intuition and interpreting numerical results. In particular, we&#39;ll be looking for regions where the particle may encounter a resonance. Without many details, let&#39;s try out the perturbation approach. Later on we&#39;ll use a computer and see if our analysis was accurate. . Floquet coordinates . The first step is to find an exact solution under some approximation. We&#39;ll neglect coupling by setting $y = 0$ and focus on one dimension to make things easier. Let&#39;s denote the linear focusing from the lattice by $k$, with all other terms in the field expansion folded into $ Delta B$ (there are still $n = 1$ and $n = 2$ terms in $ Delta B$, but they represent deviations from the design values). We&#39;re also assuming that these variables are normalized by the ratio $q / p$. This results in the equation of motion . $$ x&#39;&#39; + k(s)x = Delta B. tag{8}$$ . This is Hill&#39;s equation with a nonlinear driving term. The stable solution when $ Delta B = 0$ is . $$x(s) = sqrt{2J beta(s)} cos left({ mu(s) + delta} right), tag{9}$$ . with the phase advance is given by . $$ mu(s) = int_{0}^{s}{ frac{ds}{ beta(s)}}. tag{10}$$ . These pseudo-harmonic oscillations are still a bit difficult to visualize, so it&#39;s helpful to perform the Floquet transformation which scales the $x$ coordinate as . $$x(s) rightarrow u(s) = frac{x(s)}{ sqrt{ beta_x(s)}}. tag{11}$$ . Furthermore, it is convenient to replace the $s$ coordinate with . $$ phi(s) = frac{1}{ nu_0} int_{0}^{C}{ frac{ds}{ beta_x(s)}}. tag{12}$$ . Here $ nu_0$ is the number of phase space oscillations per trip around the ring. As a result, the unperturbed equation of motion becomes (with $ dot{x} = dx / d phi$) . $$ ddot{u} + nu_0^2 u = 0. tag{13}$$ . But this is just a harmonic oscillator — the trajectory in phase space is a circle, and the particle revolves once around this circle for every turn around the ring. Finally, we can write $ Delta B$ as a power series in $u$ and derive the equation of motion in Floquet coordinates: . $$ ddot{u} + nu_0^2 u = - nu_0^2 beta^{3/2} Delta B = - nu_0^2 sum_{n=0}^{ infty} left({ beta^{ frac{n + 3}{2}} b_{n+1}} right) u^n. tag{14}$$ . Fourier expansion . The tools to analyze driven harmonic oscillators are now available to us. Similar to Eq. (4), each term on the right hand side can be Fourier expanded, the reason being that $ beta$ (the oscillation amplitude of the unperturbed motion) and $b_n$ (a multipole coefficient) depend only on the position in the ring, so of course they are periodic in $ phi$. Grouping these terms together and performing the expansion gives . $$ ddot{u} + nu_0^2 u = - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} C_{n,k} , u^n , e^{ik phi}. tag{16}$$ . We&#39;re now going to linearize this equation. This means plugging in $u = u_0 + delta u$, where $u_0$ is the unperturbed solution and $ delta_u$ is small, and discarding all higher powers of $ delta_u$. This gives . $$ ddot{ delta u} + nu_0^2 delta u approx - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} C_{n,k} , u_0^n , e^{ik phi} . tag{17}$$ . This equation tells us how the perturbation evolves with time — ideally it remains finite, but at a resonant condition it will grow without bound. The final step is to write $u_0^n$ in a managable form. There is this trick involving the binomial expansion: . $$ u_0^n propto cos^n( nu phi) = frac{1}{2^n} sum_{m=0}^{n} binom{n}{m} e^{i(n-2m) nu_0 phi}. tag{17}$$ . So, we finally arrive at . $$ ddot{ delta u} + nu_0^2 delta u approx - nu_0^2 sum_{n=0}^{ infty} sum_{k=- infty}^{ infty} sum_{m=0}^{n} {n choose m} frac{C_{n,k}}{2^n} e^{i left[(n - 2m) nu_0 + k right] phi}. tag{18} $$ . There are a lot of indices floating around; $n$ is one less than the multipole coefficient of the magnetic field, $k$ is for the Fourier expansion, and $m$ is just a dummy index we used to binomially expand $u_0^2$. . Resonance diagram . Eq. (18) describes a driven harmonic oscillator like Eq. (5), so we can expect a resonance condition to occur when any of the frequency components of the driving force are close to the natural frequency $ nu_0$. In other words, a resonance could occur when . $$ (n - 2m) nu_0 + k = pm nu_0. tag{19}$$ . If you write out the different cases ($n$ = 0, 1, 2, ...), you&#39;ll find that dipole terms ($n = 0$) forbid integer tunes, quadrupole terms forbid 1/2 integer tunes, sextupole terms forbid 1/3 integer tunes, and so on. The same thing can be done for the vertical dimension. Once coupling is included between $x$ and $y$, we&#39;re lead to the definition of resonance lines: . $$ M_x nu_x + M_y nu_y = N, tag{20}$$ . where $M_x$, $M_y$, and $N$ are integers and $|M_x| + |M_y|$ is the order of the resonance. The reason for calling these resonance lines is because they define lines in $ nu_x$-$ nu_y$ space (tune space). . def plot_resonance_lines(ax, max_order, c=&#39;k&#39;): for N in range(-max_order, max_order + 1): for Mx in range(-max_order, max_order + 1): for My in range(-max_order, max_order + 1): order = abs(Mx) + abs(My) if order &gt; 1: factor = (1 - (order - 2)/5) lw = 1.0 * factor lw = 0.4 if lw &lt; 0 else lw alpha = 1.0 * factor alpha = 0.25 if alpha &lt; 0 else alpha if order &lt;= max_order: if My == 0: if Mx != 0: ax.axvline(N / Mx, c=c, alpha=alpha, lw=lw) else: ax.plot([0, 1], [N / My, (N - Mx) / My], c=c, alpha=alpha, lw=lw) fig, axes = plot.subplots(ncols=2, figwidth=6.5) axes.format(xlim=(0, 1), ylim=(0, 1), xlabel=r&#39;$ nu_x$&#39;, ylabel=r&#39;$ nu_y$&#39;) axes[0].set_title(&#39;5th order&#39;) axes[1].set_title(&#39;10th order&#39;) plot_resonance_lines(axes[0], 5) plot_resonance_lines(axes[1], 10); . . Resonance strengths tend to decrease with order number, so people generally don&#39;t consider anything beyond order 3 or 4. That being said, the machine tunes $ nu_x$ and $ nu_y$ need to be carefully chosen to avoid all low order resonance lines. Ideally, all beam particles would occupy this single point in tune space, but space charge complicates things by decreasing the tune by different amounts for each particle, possible placing them on one of the above resonance lines. This effect, called tune spread, places a fundamental limit on the number of particles in the beam. . Numerical exploration of the sextupole resonance . Let&#39;s explore the behavior of a beam under the influence of a sextupole magnet. This section recreates some figures from the book Accelerator Physics by S. Y. Lee. The easiest way to do this is to approximate the multipole as an instantaneous change to the slope of the particle&#39;s trajectory. This is valid if the magnet isn&#39;t too long. . import numpy as np class Multipole: &quot;&quot;&quot;Class to apply multipole kick to particle. Adapted from PyORBIT tracking routine in `py-orbit/src/teapotbase.cc`. Attributes - order : int The order of the multipole term (dipole: 1, quadrupole: 2, ...). strength : float Integrated multipole strength [m^-(order - 1)]. skew : bool If True, rotate the magnet 45 degrees. &quot;&quot;&quot; def __init__(self, order, strength, skew=False): self.order, self.strength, self.skew = order, strength, skew def track_part(self, vec): &quot;&quot;&quot;Apply transverse kick to particle slopes. vec : ndarray, shape (4,) Transverse phase space coordinate vector [x, x&#39;, y, y&#39;]. &quot;&quot;&quot; x, xp, y, yp = vec k = self.strength / np.math.factorial(self.order - 1) zn = (x + 1j*y)**(self.order- 1) if self.skew: vec[1] += k * zn.imag vec[3] += k * zn.real else: vec[1] -= k * zn.real vec[3] += k * zn.imag return vec . The situation we&#39;ll consider is a circular lattice which is made of linear uncoupled elements + one thin sextupole. We&#39;ll observe the beam at the location of the sextupole after each turn. A key result of the linear theory is that the details of the rest of the lattice are unimportant for this task. All we need to do is choose the Twiss parameters and tune in each dimension to form the transfer matrix, then we can just track using matrix multiplication. Recall that the transfer matrix is written as $ mathbf{M} = mathbf{V P V^{-1}}$, where $ mathbf{V} = mathbf{V}( alpha_x, alpha_y, beta_x, beta_y)$ performs the Floquet normalization and $ mathbf{P} = mathbf{P}( nu_x, nu_y)$ is a rotation in the $x$-$x&#39;$ and $y$-$y&#39;$ phase spaces by the angle $2 pi nu_x$ and $2 pi nu_y$, respectively. The following class implements this representation of the lattice. . def V_2D(alpha, beta): &quot;&quot;&quot;Floquet normalization matrix in 2D phase space.&quot;&quot;&quot; return np.array([[beta, 0.0], [-alpha, 1.0]]) / np.sqrt(beta) def P_2D(tune): &quot;&quot;&quot;Phase advance matrixmin 2D phase space.&quot;&quot;&quot; phase_advance = 2 * np.pi * tune cos, sin = np.cos(phase_advance), np.sin(phase_advance) return np.array([[cos, sin], [-sin, cos]]) class Lattice: &quot;&quot;&quot;Represents lattice as linear one-turn transfer matrix + multipole kick. Attributes - M : ndarray, shape (4, 4) Linear one-turn transfer matrix. aperture : float Radius of cylindical boundary containing the particles [m]. multipole : Multipole object Must implement `track_part(vec)`, where vec = [x, xp, y, yp]. &quot;&quot;&quot; def __init__(self, alpha_x, alpha_y, beta_x, beta_y, tune_x, tune_y, aperture=0.2): &quot;&quot;&quot;Constructor. Parameters - alpha_x, alpha_y, beta_x, beta_y : float Twiss parameters at the lattice entrance. tune_x, tune_y : float Number of phase space oscillations per turn. &quot;&quot;&quot; self.P = np.zeros((4, 4)) self.V = np.zeros((4, 4)) self.M = np.zeros((4, 4)) self.P[:2, :2] = P_2D(tune_x) self.P[2:, 2:] = P_2D(tune_y) self.V[:2, :2] = V_2D(alpha_x, beta_x) self.V[2:, 2:] = V_2D(alpha_y, beta_y) self.M = np.linalg.multi_dot([self.V, self.P, la.inv(self.V)]) self.aperture = aperture self.multipole = None def add_multipole(self, multipole): self.multipole = multipole def track_part(self, vec): &quot;&quot;&quot;Track a single particle through the lattice. vec : ndarray, shape (4,) Transverse phase space coordinate vector [x, x&#39;, y, y&#39;]. &quot;&quot;&quot; vec = np.matmul(self.M, vec) if self.multipole is not None: vec = self.multipole.track_part(vec) return vec def track_bunch(self, X): &quot;&quot;&quot;Track a particle bunch through the lattice. X : ndarray, shape (nparts, 4) Transverse phase space coordinate array. &quot;&quot;&quot; X = np.apply_along_axis(self.track_part, 1, X) return self.collimate(X) def collimate(self, X): &quot;&quot;&quot;Delete particles outside aperture.&quot;&quot;&quot; radii = np.sqrt(X[:, 0]**2 + X[:, 2]**2) return np.delete(X, np.where(radii &gt; self.aperture), axis=0) def get_matched_bunch(self, nparts=2000, emittance=10e-6, cut=3.0): &quot;&quot;&quot;Generate truncated Gaussian distribution matched to the lattice.&quot;&quot;&quot; from scipy.stats import truncnorm X = truncnorm.rvs(a=4*[-cut], b=4*[cut], size=(nparts, 4)) A = np.sqrt(emittance) * np.identity(4) V = self.V X = np.apply_along_axis(lambda vec: np.matmul(A, vec), 1, X) X = np.apply_along_axis(lambda vec: np.matmul(V, vec), 1, X) return X . 1/3 integer resonance . We focus first on the 1/3 integer resonance. Below, a particle is tracked over 100 turns starting from few different initial amplitudes. We set $y = y&#39; = 0$ in all cases. The $x$-$x&#39;$ trajectories should be upright ellipses in the absence of nonlinear elements. Some helper functions are defined in the following collapsed cell. . # Define the Twiss parameters at the observation point. alpha_x = alpha_y = 0.0 beta_x = beta_y = 20.0 def create_lattice(tune_x, tune_y, multipole=None): lattice = Lattice(alpha_x, alpha_y, beta_x, beta_y, tune_x, tune_y) lattice.add_multipole(multipole) return lattice def get_traj(lattice, emittance, nturns=1): &quot;&quot;&quot;Return array of shape (nturns, 4) of tracked single particle coordinates. The vertical coordinate and slope are set to zero. &quot;&quot;&quot; X = np.array([[np.sqrt(emittance * beta_x), 0, 0, 0]]) tracked_vec = [X[0]] for _ in range(nturns): X = lattice.track_bunch(X) if X.shape[0] == 0: # particle was deleted break tracked_vec.append(X[0]) return 1000 * np.array(tracked_vec) # convert from m to mm def compare_traj(tunes_x, tune_y, emittances, nturns=1, multipole=None, limits=(45, 2.5), **kws): &quot;&quot;&quot;Compare trajectories w/ different emittances as horizontal tune is scaled.&quot;&quot;&quot; kws.setdefault(&#39;s&#39;, 1) kws.setdefault(&#39;c&#39;, &#39;pink8&#39;) fig, axes = plot.subplots(nrows=2, ncols=3, figsize=(6.5, 4)) xlim, ylim = (-limits[0], limits[0]), (-limits[1], limits[1]) axes.format(xlabel=&quot;x [mm]&quot;, ylabel=&quot;x&#39; [mrad]&quot;, xlim=xlim, ylim=ylim) for ax, tune_x in zip(axes, tunes_x): lattice = create_lattice(tune_x, tune_y, multipole) for emittance in emittances: tracked_vec = get_traj(lattice, emittance, nturns) ax.scatter(tracked_vec[:, 0], tracked_vec[:, 1], **kws) ax.annotate(r&#39;$ nu_x = {:.3f}$&#39;.format(tune_x), xy=(0.97, 0.965), xycoords=&#39;axes fraction&#39;, horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;, bbox=dict(fc=&#39;white&#39;, ec=&#39;black&#39;) ) return axes def track_bunch(X, lattice, nturns=1): &quot;&quot;&quot;Track and return list of coordinate array after each turn. Also return the fraction of particles which were lost (exceeded aperture) at each frame.&quot;&quot;&quot; coords, nparts, frac_lost = [X], X.shape[0], [0.0] for _ in range(nturns): X = lattice.track_bunch(X) coords.append(X) frac_lost.append(1 - X.shape[0] / nparts) return [1000*X for X in coords], frac_lost def animate_phase_space(coords, frac_lost=None, limits=(55.0, 5.0)): &quot;&quot;&quot;Create animation of turn-by-turn x-x&#39; and y-y&#39; distributions.&quot;&quot;&quot; fig, axes = plot.subplots(ncols=2, figwidth=6.5, wspace=12.0, sharey=False, sharex=False) xlim, ylim = (-limits[0], limits[0]), (-limits[1], limits[1]) axes.format(xlim=xlim, ylim=ylim) axes[0].format(xlabel=&quot;x [mm]&quot;, ylabel=&quot;x&#39; [mrad]&quot;) axes[1].format(xlabel=&quot;y [mm]&quot;, ylabel=&quot;y&#39; [mrad]&quot;) plt.close() kws = dict(marker=&#39;.&#39;, color=&#39;black&#39;, ms=2.0, lw=0, mec=&#39;None&#39;) line0, = axes[0].plot([], [], **kws) line1, = axes[1].plot([], [], **kws) def update(t): x, xp, y, yp = coords[t].T line0.set_data(x, xp) line1.set_data(y, yp) axes[0].set_title(&#39;Turn {}&#39;.format(t), fontsize=&#39;medium&#39;) if frac_lost: axes[1].set_title(&#39;Frac. lost = {:.3f}&#39;.format(frac_lost[t]), fontsize=&#39;medium&#39;) return animation.FuncAnimation(fig, update, frames=len(coords)) . . tunes_x = np.linspace(0.61, 0.66, 6) tune_y = 0.518 emittances = 1e-6 * np.linspace(2.0, 10.0, 10)**2 nturns = 100 kws = dict(s=0.6, c=&#39;black&#39;, ec=&#39;none&#39;) axes = compare_traj(tunes_x, tune_y, emittances, nturns, limits=(55.0, 3.0), **kws) axes.format(suptitle=&#39;Linear lattice&#39;) . Now turn on the sextupole magnet. . order, strength = 3, 0.5 multipole = Multipole(order, strength, skew=False) axes = compare_traj(tunes_x, tune_y, emittances, nturns, multipole, limits=(55.0, 3.0), **kws) axes.format(suptitle=&#39;Linear lattice + sextupole&#39;); . The initially elliptical orbits are morphed into a triangular shape as the tune approaches the resonance condition, and some of the larger orbits become unstable. It turns out that by looking at the Hamiltonian you can find a triangular region defining a separatrix between stable and unstable motion. Particles inside the triangle will oscillate forever, particles at the corner of the triangle are at unstable equilibrium points, and particles outside the triangle will eventually stream outward from the corners. This is easier to see by tracking a bunch of particles. The interesting stuff will be in the horizontal plane, but I&#39;ll plot the vertical plane as well for comparison. . lattice = create_lattice(0.66, tune_y, multipole) X = lattice.get_matched_bunch() coords, frac_lost = track_bunch(X, lattice, nturns=50) animate_phase_space(coords, frac_lost) . &lt;/input&gt; Once Loop Reflect The triangular region of stability is clearly visible at the end of 50 turns. Interestingly, the third order resonance can be used to extract a beam from an accelerator at a much slower rate than normal. To do this, the strength and spacing of sextupole magnets must be carefully chosen to control the shape and orientation of the stability triangle, then tune is slowly moved closer to the 1/3 integer resonance value. The result is that the triangle shrinks as the stable phase space area decreases, and that more and more particles will find themselves in the unstable area and eventually stream out along the vertices. . Integer resonance . The sextupole should also excite the integer resonance. . compare_traj(np.linspace(0.96, 0.976, 6), tune_y, emittances, nturns, multipole, limits=(60, 2.5), **kws); . lattice = create_lattice(0.99, 0.18, multipole) animate_phase_space(*track_bunch(X, lattice, nturns=50)) . &lt;/input&gt; Once Loop Reflect Cool pattern! The separatrix is now shaped like a tear drop. It looks like it&#39;s evolving more slowly because the tune is close to an integer, so the particles almost return to the same location in phase space after a turn. . Higher order resonances . There are also higher order resonances which a sextupole can drive. You can actually find fourth and fifth order resonances if you perform perturbation theory up to second order (at least that&#39;s what I&#39;m told in a textbook... I&#39;d like to avoid carrying out such a procedure). Do these show up using our mapping equations? They are expected to be weaker, so we&#39;ll double the sextupole strength. . emittances = 1e-6 * np.array([1, 7, 15, 25, 50, 100, 150, 200, 250, 350]) compare_traj(np.linspace(0.7496, 0.798, 6), 0.23, emittances, 1000, Multipole(3, 1.0), limits=(150, 6), **kws); . These are interesting plots. The tune near 0.75 (it&#39;s actually 0.7496) is exciting a fourth order resonance, while the tune near 0.8 is exciting a fifth order resonance. In all the plots, the low amplitude orbits are stable ellipses. We then see the behavior change as the amplitude is increased, with the particle jumping between distinct &quot;islands&quot;. Eventually the trajectories once again form closed loops, but in deformed shapes. The motion is unstable at even larger amplitudes. Understanding exactly why the the plots look like they do would take more work. . Conclusion . This post outlined the theory of nonlinear resonances driven by magnetic multipoles. The effect of a sextupole-driven resonance on the phase space trajectory was then examined using mapping equations. Taking the time to write down the steps which lead to Eq. (20), an equation which is often referenced in accelerator physics, was a rewarding experience and helped make the topic less mysterious to me (although I&#39;m no expert). Here are a number of helpful references: . Lectures S. Lund, Transverse Particle Resonances with Application to Circular Accelerators | E. Prebys, Resonances and Coupling | . | Textbooks D. Edwards and M. Syphers, An introduction to the Physics of High Energy Accelerators | H. Wiedemann, Particle Accelerator Physics | S. Y. Lee, Accelerator Physics | L. Reichl, The Transition to Chaos — Conservative Classical Systems and Quantum Manifestations | J. Taylor, Classical Mechanics | H. Goldstein, Classical Mechanics | . | .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/nonlinear%20systems/resonances/2021/03/28/nonlinear_resonances.html",
            "relUrl": "/physics/accelerators/nonlinear%20systems/resonances/2021/03/28/nonlinear_resonances.html",
            "date": " • Mar 28, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Particle-in-cell simulation",
            "content": "Many simulation codes exist for beam physics (one example is PyORBIT). A key component of the these simulations is the inclusion of the electromagnetic interactions between particles in the beam, also known as space charge forces. One way to compute space charge forces is the particle-in-cell (PIC) method. This post implements the PIC method in Python. . Theoretical model . We&#39;ll use bunch to refer to a group of particles in three-dimensional (3D) space, and we&#39;ll use a local cartesian coordinate system whose origin moves with the center of the bunch as shown below: . . The $s$ coordinate specifies the position of the bunch in the accelerator, and the path can be curved. Now for a few assumptions and approximations. First, assume all particles in the bunch move at a constant velocity $ beta c$, where $c$ is the speed of light. We then make the paraxial approximation. It&#39;s conventional to use the slope $x&#39; = dx/ds$ instead of the velocity, and the paraxial approximation assumes this slope is very small. Usually we report this slope in milliradians since $tan theta approx theta$ for small angles. Next we assume that the transverse ($x$-$y$) size of the bunch varies slowly along the $s$ axis. If this is true and we look at the electric field in a transverse slice of the bunch, there won&#39;t be much difference between the true field and the field of an infinitely long, uniform density cylinder. Our focus will be on the transverse dynamics of such a slice, so we&#39;ll treat each &quot;particle&quot; as an infinite line of charge. The figure below illustrates this approximation. . . Credit: G. Franchetti Another approximation is to neglect any magnetic fields generated by the beam, which is again valid if the transverse velocities are very small relative to $ beta c$. All this being said, the equations of motion without any external forces, i.e., in free space, can be written as . $$ mathbf{x}&#39;&#39; = frac{q}{mc^2 beta^2 gamma^3} mathbf{E}, tag{1}$$ . where $ mathbf{x} = [x, y]^T$ is the coordinate vector, $ mathbf{E} = [E_x, E_y]^T$ is the self-generated electric field, $m$ is the particle mass, and $ gamma = left({1 - beta^2} right)^{-1/2}$. Let&#39;s first address the factor $ gamma^{-3}$ in the equation of motion, which means that the space charge force goes to zero as the velocity approaches the speed of light. This is because parallel moving charges generate an attractive magnetic force which grows with velocity, completely cancelling the electric force in the limit $v rightarrow c$. . . Credit: OpenStax University PhysicsOne may ask: what about the rest frame in which there is no magnetic field? But special relativity says that electrogmagnetic fields change with reference frame. Using the transformations defined here, you can quickly prove that . $$ mathbf{E}_{lab} = frac{ mathbf{E}_{rest}}{ gamma}. tag{2}$$ . This inverse relationship between velocity and the space charge force has real-life consequences. It tells us that space charge is important if 1) the beam is very intense, meaning there are many particles in a small area, or 2) the beam is very energetic, meaning it is moving extremely fast. For example, space charge can usually be ignored in electron beams, which move near the speed of light for very modest energies due to their tiny mass, but is significant in high-intensity, low-energy hadron accelerators such as FRIB, SNS, and ESS. . We should now address the difficulty in determining the evolution of this system: the force on a particle in an $n$-particle bunch depends on the positions of the other $n - 1$ particles. The approach of statistical mechanics to this problem is to introduce a distribution function $f( mathbf{x}, mathbf{x}&#39;, s)$ which gives the particle density at axial position $s$ and phase space coordinates $ mathbf{x}$, $ mathbf{x}&#39;$. The Vlasov-Poisson system of equations determines the evolution of $f$ as long as we ignore collisions between particles: . $$ frac{ partial{f}}{ partial{s}} + mathbf{x}&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}}} + mathbf{x}&#39;&#39; cdot frac{ partial{f}}{ partial{ mathbf{x}&#39;}} = 0. tag{3}$$ . We know $ mathbf{x&#39;&#39;}$ from Eq. (1). The electric field is obtained from Poisson&#39;s equation: . $$ nabla cdot mathbf{E} = - nabla^2 phi = frac{ rho}{ varepsilon_0}. tag{4}$$ . Finally, the transverse charge density $ rho$ is determined by . $$ rho = q int{f dx&#39;dy&#39;}. tag{5}$$ . Although these equations are easy to write down, they are generally impossible to solve analytically. We need to turn to a computer for help. . Computational method . The Vlasov equation could be solved directly, but this is difficult, especially in 2D or 3D. On the other end of the spectrum, the notion of a fluid in phase space could be abandoned and each particle could be tracked individually, computing the forces using direct sums. But this is infeasible with current hardware; the time complexity would by $O(n^2)$, where $n$ is the number of particles, and $n$ may be on the order of $10^{14}$. The particle-in-cell (PIC) method is a sort of combination of these two approaches. The idea is to track a group of macroparticles according to Eq. (1), each of which represents a large number of real particles. The fields, however, are solved from Eq. (4). The key step is transforming back and forth between a discrete and continuous representation of the bunch. The simulation loop for the PIC method is shown below. . . In the next sections I will discuss each of these steps and implement them in Python. The hidden cell below shows all the imports needed to run the code. . import numpy as np from scipy.interpolate import RegularGridInterpolator from scipy.fft import fft2, ifft2 from scipy.integrate import odeint from scipy.stats import truncnorm from tqdm import trange import Cython %load_ext cython # Plotting from matplotlib import pyplot as plt, animation from matplotlib.lines import Line2D from matplotlib.patches import Ellipse import seaborn as sns import proplot as plot . . The cython extension is already loaded. To reload it, use: %reload_ext cython . Let&#39;s first create a Bunch class, which is a simple container for the bunch coordinates. . class Bunch: &quot;&quot;&quot;Container for 2D distribution of positive elementary charges. Attributes - intensity : float Number of physical particles in the bunch. length : float Length of the bunch [m]. mass, kin_energy : float Mass [GeV/c^2], charge [C], and kinetic energy [GeV] per particle. nparts : float Number of macroparticles in the bunch. X : ndarray, shape (nparts, 4) Array of particle coordinates. Columns are [x, x&#39;, y, y&#39;]. Units are meters and radians. positions : ndarray, shape (nparts, 2): Just the x and y positions (for convenience). &quot;&quot;&quot; def __init__(self, intensity=1e14, length=250., mass=0.938, kin_energy=1.0): self.intensity, self.length = intensity, length self.mass, self.kin_energy = mass, kin_energy self.gamma = 1 + (kin_energy / mass) # Lorentz factor self.beta = np.sqrt(1 - (1 / self.gamma)**2) # v/c r0 = 1.53469e-18 # classical proton radius [m] self.perveance = 2 * r0 * intensity / (length * self.beta**2 * self.gamma**3) self.nparts = 0 self.compute_macrosize() self.X, self.positions = None, None def compute_macrosize(self): &quot;&quot;&quot;Update the macrosize and macrocharge.&quot;&quot;&quot; self.macrosize = self.intensity // self.nparts if self.nparts &gt; 0 else 0 def fill(self, X): &quot;&quot;&quot;Fill with particles.&quot;&quot;&quot; self.X = X if self.X is None else np.vstack([self.X, X]) self.positions = self.X[:, [0, 2]] self.nparts = self.X.shape[0] self.compute_macrosize() def compute_extremum(self): &quot;&quot;&quot;Get extreme x and y coorinates.&quot;&quot;&quot; self.xmin, self.ymin = np.min(self.positions, axis=0) self.xmax, self.ymax = np.max(self.positions, axis=0) self.xlim, self.ylim = (self.xmin, self.xmax), (self.ymin, self.ymax) . Weighting . Starting from a group of macroparticles, we need to produce a charge density $ rho_{i,j}$ on a grid. The most simple approach is the nearest grid point (NGP) method, which, as the name suggests, assigns the full particle charge to the closest grid point. This is commonly called zero-order weighting; although it is very fast and easy to implement, it is not commonly used because it can lead to significant noise. A better method called cloud-in-cell (CIC) treats each particle as a rectangular, uniform density cloud of charge with dimensions equal to the grid spacing. A fractional part of the charge is assigned based on the fraction of the cloud overlapping with a given cell. This can be thought of as first-order weighting. To get a sense of what these methods are doing (in 1D), we can slide a particle across a cell and plot the resulting density of the cell at each position, thus giving an effective particle shape. . def shape_func(u, v, cell_width, method=&#39;ngp&#39;): S, diff = 0, np.abs(u - v) if method == &#39;ngp&#39;: S = 1 if diff &lt; (0.5 * cell_width) else 0 elif method == &#39;cic&#39;: S = 1 - diff/cell_width if diff &lt; cell_width else 0 return S / cell_width fig, axes = plot.subplots(ncols=2, figsize=(5, 1.5)) xvals = np.linspace(-1, 1, 1000) for ax, method in zip(axes, [&#39;ngp&#39;, &#39;cic&#39;]): densities = [shape_func(x, 0, 1, method) for x in xvals] ax.plot(xvals, densities, &#39;k&#39;) axes.format(toplabels=[&#39;NGP&#39;, &#39;CIC&#39;]) axes[0].set_xlabel(&#39;($x - x_k) ,/ , Delta x$&#39;, fontsize=&#39;large&#39;) axes[1].set_ylabel(&#39;Density&#39;, fontsize=&#39;large&#39;); . . The NGP method leads to a discontinuous boundary while the CIC method leads to a continous boundary (but discontinous derivative). There are also higher order methods which lead to a smooth boundary, but I don&#39;t cover those here. . We also need to perform the inverse operation: given the electric field at each grid point, interpolate the value at each particle position. The same method applies here. NGP just uses the electric field at the nearest grid point, while CIC weights the four nearest grid points. The following Grid class implements the CIC method. Notice that Cython is used in the for-loop in the distribute method. I couldn&#39;t figure out a way to perform this operation with the loop, and in pure Python it took about 90% of the runtime for a single simulation step. Using Cython gave a significant performance boost. . %%cython import numpy as np from scipy.interpolate import RegularGridInterpolator class Grid: &quot;&quot;&quot;Class for 2D grid. Attributes - xmin, ymin, xmax, ymax : float Minimum and maximum coordinates. Nx, Ny : int Number of grid points. dx, dy : int Spacing between grid points. x, y : ndarray, shape (Nx,) or (Ny,) Positions of each grid point. cell_area : float Area of each cell. &quot;&quot;&quot; def __init__(self, xlim=(-1, 1), ylim=(-1, 1), size=(64, 64)): self.xlim, self.ylim = xlim, ylim (self.xmin, self.xmax), (self.ymin, self.ymax) = xlim, ylim self.size = size self.Nx, self.Ny = size self.dx = (self.xmax - self.xmin) / (self.Nx - 1) self.dy = (self.ymax - self.ymin) / (self.Ny - 1) self.cell_area = self.dx * self.dy self.x = np.linspace(self.xmin, self.xmax, self.Nx) self.y = np.linspace(self.ymin, self.ymax, self.Ny) def set_lims(self, xlim, ylim): &quot;&quot;&quot;Set the min and max grid coordinates.&quot;&quot;&quot; self.__init__(xlim, ylim, self.size) def zeros(self): &quot;&quot;&quot;Create array of zeros with same size as the grid.&quot;&quot;&quot; return np.zeros((self.size)) def distribute(self, positions): &quot;&quot;&quot;Distribute points on the grid using the cloud-in-cell (CIC) method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. Returns - rho : ndarray, shape (Nx, Ny) The value rho[i, j] gives the number of macroparticles in the i,j cell. &quot;&quot;&quot; # Compute area overlapping with 4 nearest neighbors (A1, A2, A3, A4) ivals = np.floor((positions[:, 0] - self.xmin) / self.dx).astype(int) jvals = np.floor((positions[:, 1] - self.ymin) / self.dy).astype(int) ivals[ivals &gt; self.Nx - 2] = self.Nx - 2 jvals[jvals &gt; self.Ny - 2] = self.Ny - 2 x_i, x_ip1 = self.x[ivals], self.x[ivals + 1] y_j, y_jp1 = self.y[jvals], self.y[jvals + 1] _A1 = (positions[:, 0] - x_i) * (positions[:, 1] - y_j) _A2 = (x_ip1 - positions[:, 0]) * (positions[:, 1] - y_j) _A3 = (positions[:, 0] - x_i) * (y_jp1 - positions[:, 1]) _A4 = (x_ip1 - positions[:, 0]) * (y_jp1 - positions[:, 1]) # Distribute fractional areas rho = self.zeros() cdef double[:, :] rho_view = rho cdef int i, j for i, j, A1, A2, A3, A4 in zip(ivals, jvals, _A1, _A2, _A3, _A4): rho_view[i, j] += A4 rho_view[i + 1, j] += A3 rho_view[i, j + 1] += A2 rho_view[i + 1, j + 1] += A1 return rho / self.cell_area def interpolate(self, grid_vals, positions): &quot;&quot;&quot;Interpolate values from the grid using the CIC method. Parameters - positions : ndarray, shape (n, 2) List of (x, y) positions. grid_vals : ndarray, shape (n, 2) Scalar value at each coordinate point. Returns - int_vals : ndarray, shape (nparts,) Interpolated value at each position. &quot;&quot;&quot; int_func = RegularGridInterpolator((self.x, self.y), grid_vals) return int_func(positions) def gradient(self, grid_vals): &quot;&quot;&quot;Compute gradient using 2nd order centered differencing. Parameters - grid_vals : ndarray, shape (Nx, Ny) Scalar values at each grid point. Returns - gradx, grady : ndarray, shape (Nx, Ny) The x and y gradient at each grid point. &quot;&quot;&quot; return np.gradient(grid_vals, self.dx, self.dy) . It should also be mentioned that the field interpolation method should be the same as the charge deposition method; if this is not true, it is possible for a particle to exert a force on itself! Let&#39;s test the method on a Gaussian distribution of 100,000 macroparticles in the $x$-$y$ plane, truncated at three standard devations. We&#39;ll choose the number of grid points to be $N_x = N_y = 64$. . # Create coordinate array nparts = 5000000 nbins = 64 cut = 3.0 X = truncnorm.rvs(scale=15.0, a=4*[-cut], b=4*[cut], size=(nparts, 4)) # Create bunch bunch = Bunch() bunch.fill(X) bunch.compute_extremum() # Distribute bunch particles on grid grid = Grid(bunch.xlim, bunch.ylim, size=(nbins, nbins)) rho = grid.distribute(bunch.positions) # Plot fig, axes = plt.subplots(1, 2, figsize=(5.5, 2.25), sharey=True, sharex=True) X_samp = X[np.random.choice(X.shape[0], 5000, replace=False), :] x = (X_samp[:, 0] - grid.xmin) / grid.dx y = (X_samp[:, 2] - grid.ymin) / grid.dy axes[0].set_facecolor(&#39;k&#39;) axes[0].scatter(x, y, s=1, c=&#39;w&#39;, ec=&#39;none&#39;) sns.heatmap(rho.T, ax=axes[1], cmap=&#39;mono_r&#39;, cbar=False) axes[0].set_title(&#39;5000 random samples&#39;) axes[1].set_title(&#39;CIC weighting&#39;); . . Field solver . The workhorse in the simulation loop is the field solver. We need to solve Poisson&#39;s equation: . $$ left({ frac{ partial^2}{ partial x^2} + frac{ partial^2}{ partial y^2}} right) = - frac{ rho left(x, y right)}{ varepsilon_0}. tag{6}$$ . The discretized version of the equation reads . $$ frac{ phi_{i+1,j} -2 phi_{i,j} + phi_{i-1,j}}{{ Delta_x}^2} + frac{ phi_{i,j+1} -2 phi_{i,j} + phi_{i,j-1}}{{ Delta_y}^2} = - frac{ rho_{i,j}}{ varepsilon_0} tag{7}$$ . for a grid with spacing $ Delta_x$ and $ Delta_y$. There are multiple paths to a solution; we will focus on the method implemented in PyORBIT which utilizes the Fourier convolution theorem. Let&#39;s briefly go over this method. The potential from an infinite line of elementary charges at the origin with number density $ lambda$ is . $$ phi( mathbf{x}) = - frac{ lambda e}{2 pi varepsilon_0} ln{| mathbf{x}|} = - frac{ lambda e}{2 pi varepsilon_0} int{ ln{| mathbf{x} - mathbf{y}|} delta( mathbf{y})d mathbf{y}}. tag{8}$$ . Note that $ mathbf{y}$ is just a dummy variable. By letting $G( mathbf{x} - mathbf{y}) = - ln{| mathbf{x} - mathbf{y}|}$ and $ rho( mathbf{x}) = delta( mathbf{x})$, then up to a scaling factor we have . $$ phi( mathbf{x}) = int{G( mathbf{x} - mathbf{y}) rho( mathbf{y})d mathbf{y}} = G( mathbf{x}) * rho( mathbf{x}). tag{9}$$ . In this form the potential is a convolution (represented by $*$) of the charge density $ rho$ with $G$, which is called the Green&#39;s function. On the grid this will look like . $$ phi_{i, j} = sum_{k,l ne i,j}{G_{i-k, j-l} rho_{k, l}}. tag{11}$$ . This solves the problem in $O(N^2)$ time complexity for $N$ grid points. This is already much faster than a direct force calculation but could still get expensive for fine grids. We can speed things up by exploiting the convolution theorem, which says that the Fourier transform of a convolution of two functions is equal to the product of their Fourier transforms. The Fourier transform is defined by . $$ hat{ phi}( mathbf{k})= mathcal{F} left[ phi( mathbf{x}) right] = int_{- infty}^{ infty}{e^{-i mathbf{k} cdot mathbf{x}} phi( mathbf{x}) d mathbf{x}}. tag{12}$$ . The convolution theorem then says $$ mathcal{F} left[ rho * G right] = mathcal{F} left[ rho right] cdot mathcal{F} left[G right]. tag{13}$$ . For the discrete equation this gives . $$ hat{ phi}_{n, m} = hat{ rho}_{n, m} hat{G}_{n, m}, tag{14}$$ . where the hat represents the discrete Fourier transform. The time complexity can be reduced to $O left(N log N right)$ with the FFT algorithm at our disposal. . There is a caveat to this method: Eq. (11) must be a circular convolution in order to use the FFT algorithm, which means $G$ must be periodic. But the beam is in free space (we&#39;ve neglected any conducting boundary), so this is not true. We can make it true by doubling the grid size in each dimension. We then make $G$ a mirror reflection in the new quadrants so that it is periodic, and also set the charge density equal to zero in these regions. After running the method on this larger grid, the potential in the new quadrants will be unphysical; however, the potential in the original quadrant will be correct. There are also some tricks we can play to reduce the space complexity, and in the end doubling the grid size is not much of a price to pay for the gain in speed. The method is implemented in the PoissonSolver class. . class PoissonSolver: &quot;&quot;&quot;Class to solve Poisson&#39;s equation on a 2D grid. Attributes - rho, phi, G : ndarray, shape (2*Nx, 2*Ny) The density (rho), potential (phi), and Green&#39;s function (G) at each grid point on a doubled grid. Only one quadrant (i &lt; Nx, j &lt; Ny) corresponds to to the real potential. &quot;&quot;&quot; def __init__(self, grid, sign=-1.): self.grid = grid new_shape = (2 * self.grid.Nx, 2 * self.grid.Ny) self.rho, self.G = np.zeros(new_shape), np.zeros(new_shape) self.phi = np.zeros(new_shape) def set_grid(self, grid): self.__init__(grid) def compute_greens_function(self): &quot;&quot;&quot;Compute Green&#39;s function on doubled grid.&quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny Y, X = np.meshgrid(self.grid.x - self.grid.xmin, self.grid.y - self.grid.ymin) self.G[:Nx, :Ny] = -0.5 * np.log(X**2 + Y**2, out=np.zeros_like(X), where=(X + Y &gt; 0)) self.G[Nx:, :] = np.flip(self.G[:Nx, :], axis=0) self.G[:, Ny:] = np.flip(self.G[:, :Ny], axis=1) def get_potential(self, rho): &quot;&quot;&quot;Compute the scaled electric potential on the grid. Parameters - rho : ndarray, shape (Nx, Ny) Number of macroparticles at each grid point. Returns - phi : ndarray, shape (Nx, Ny) Scaled electric potential at each grid point. &quot;&quot;&quot; Nx, Ny = self.grid.Nx, self.grid.Ny self.rho[:Nx, :Ny] = rho self.compute_greens_function() self.phi = ifft2(fft2(self.G) * fft2(self.rho)).real return self.phi[:Nx, :Ny] . Running the algorithm gives the following potential on the doubled grid: . solver = PoissonSolver(grid) phi = solver.get_potential(rho) fig, axes = plt.subplots(ncols=2, figsize=(6.5, 3), sharey=True) sns.heatmap(solver.rho.T, ax=axes[0], cmap=&#39;mono_r&#39;, cbar=False) sns.heatmap(solver.phi.T, ax=axes[1], cmap=&#39;viridis&#39;, cbar=False) for ax in axes: ax.axvline(grid.Nx - 0.5, c=&#39;w&#39;) ax.axhline(grid.Ny - 0.5, c=&#39;w&#39;) for xy in [(0.65, 0.75), (0.15, 0.25), (0.65, 0.25)]: ax.annotate(&#39;unphysical&#39;, xy=xy, xycoords=&#39;axes fraction&#39;, c=&#39;w&#39;) axes[0].set_title(r&#39;Density $ rho$&#39;) axes[1].set_title(r&#39;Potential $ phi$&#39;); . . We can then approximate the gradient of the potential using second-order centered differencing. This gives . $$( nabla phi)_{i,j} = frac{ phi_{i+1,j} - phi_{i-1,j}}{2 Delta_x} hat{x} + frac{ phi_{i,j+1} - phi_{i,j-1}}{2 Delta_y} hat{y}. tag{15}$$ . The following plot shows the electric field at each position (red is positive, blue is negative). . Ex, Ey = grid.gradient(-phi) fig, axes = plt.subplots(ncols=2, figsize=(6.5, 3), sharey=True) sns.heatmap(Ex.T, ax=axes[0], cmap=&#39;Vlag&#39;, cbar=False) sns.heatmap(Ey.T, ax=axes[1], cmap=&#39;Vlag&#39;, cbar=False) axes[0].set_title(&#39;$E_x$&#39;) axes[1].set_title(&#39;$E_y$&#39;); . . Finally, the value of the electric field at each particle position can be interpolated from the grid. . Ex_int = grid.interpolate(Ex, bunch.positions) Ey_int = grid.interpolate(Ey, bunch.positions) . Particle mover . All we need to do in this step is integrate the equations of motion. A common method is leapfrog integration in which the position and velocity are integrated out of phase as follows: . $$ m left( frac{ mathbf{v}_{i+1/2} - mathbf{v}_{i-1/2}}{ Delta_t} right) = mathbf{F}( mathbf{x}_i), tag{16}$$ . $$ frac{ mathbf{x}_{i+1} - mathbf{x}_i}{ Delta_t} = mathbf{v}_{i+1/2} tag{17}$$ . . Credit: S. LundA different scheme must be used when velocity-dependent forces are present. This is a symplectic integrator, which means it conserves energy. It is also second-order accurate, meaning that its error is proportional to the square of the $ Delta_t$. Finally, it is time-reversible. The only complication is that, because the velocity and position are out of phase, we need to push the velocity back one half-step before starting the simulation, and push it one half-step forward when taking a measurement. . Putting it all together . Simulation loop . We have all the tools to implement the simulation loop. While $s &lt; s_{max}$ we: . Compute the charge density on the grid. | Find the electric potential on the grid. | Interpolate the electric field at the particle positions. | Update the particle positions. | We&#39;ll first create a History class which stores the beam moments or phase space coordinates. . class History: &quot;&quot;&quot;Class to store bunch data over time. Atributes moments : list Second-order bunch moments. Each element is ndarray of shape (10,). coords : list Bunch coordinate arrays. Each element is ndarray of shape (nparts, 4) moment_positions, coord_positions : list Positions corresponding to each element of `moments` or `coords`. &quot;&quot;&quot; def __init__(self, bunch, samples=&#39;all&#39;): self.X = bunch.X self.moments, self.coords = [], [] self.moment_positions, self.coord_positions = [], [] if samples == &#39;all&#39; or samples &gt;= bunch.nparts: self.idx = np.arange(bunch.nparts) else: self.idx = np.random.choice(bunch.nparts, samples, replace=False) def store_moments(self, s): Sigma = np.cov(self.X.T) self.moments.append(Sigma[np.triu_indices(4)]) self.moment_positions.append(s) def store_coords(self, s): self.coords.append(np.copy(self.X[self.idx, :])) self.coord_positions.append(s) def package(self): self.moments = np.array(self.moments) self.coords = np.array(self.coords) . Now we&#39;ll create a Simulation class. . class Simulation: &quot;&quot;&quot;Class to simulate the evolution of a charged particle bunch in free space. Attributes - bunch : Bunch: The bunch to track. distance : float Total tracking distance [m]. step_size : float Distance between force calculations [m]. nsteps : float Total number of steps = int(length / ds). steps_performed : int Number of steps performed so far. s : float Current bunch position. history : History object Object storing historic bunch data. meas_every : dict Dictionary with keys: &#39;moments&#39; and &#39;coords&#39;. Values correspond to the number of simulations steps between storing these quantities. For example, `meas_every = {&#39;coords&#39;:4, &#39;moments&#39;:2}` will store the moments every 4 steps and the moments every other step. Defaults to storing only the initial and final positions. samples : int Number of bunch particles to store when measuring phase space coordinates. Defaults to the entire coordinate array. &quot;&quot;&quot; def __init__(self, bunch, distance, step_size, grid_size, meas_every={}, samples=&#39;all&#39;): self.bunch = bunch self.distance, self.step_size = distance, step_size self.nsteps = int(distance / step_size) self.grid = Grid(size=grid_size) self.solver = PoissonSolver(self.grid) self.fields = np.zeros((bunch.nparts, 2)) self.history = History(bunch, samples) self.s, self.steps_performed = 0.0, 0 self.meas_every = meas_every self.meas_every.setdefault(&#39;moments&#39;, self.nsteps) self.meas_every.setdefault(&#39;coords&#39;, self.nsteps) self.sc_factor = bunch.perveance / bunch.nparts def set_grid(self): &quot;&quot;&quot;Set grid limits from bunch size.&quot;&quot;&quot; self.bunch.compute_extremum() self.grid.set_lims(self.bunch.xlim, self.bunch.ylim) self.solver.set_grid(self.grid) def compute_electric_field(self): &quot;&quot;&quot;Compute self-generated electric field.&quot;&quot;&quot; self.set_grid() rho = self.grid.distribute(self.bunch.positions) phi = self.solver.get_potential(rho) Ex, Ey = self.grid.gradient(-phi) self.fields[:, 0] = self.grid.interpolate(Ex, self.bunch.positions) self.fields[:, 1] = self.grid.interpolate(Ey, self.bunch.positions) def kick(self, step_size): &quot;&quot;&quot;Update particle slopes.&quot;&quot;&quot; self.bunch.X[:, 1] += self.sc_factor * self.fields[:, 0] * step_size self.bunch.X[:, 3] += self.sc_factor * self.fields[:, 1] * step_size def push(self, step_size): &quot;&quot;&quot;Update particle positions.&quot;&quot;&quot; self.bunch.X[:, 0] += self.bunch.X[:, 1] * step_size self.bunch.X[:, 2] += self.bunch.X[:, 3] * step_size def store(self): &quot;&quot;&quot;Store bunch data.&quot;&quot;&quot; store_moments = self.steps_performed % self.meas_every[&#39;moments&#39;] == 0 store_coords = self.steps_performed % self.meas_every[&#39;coords&#39;] == 0 if not (store_moments or store_coords): return Xp = np.copy(self.bunch.X[:, [1, 3]]) self.kick(+0.5 * self.step_size) # sync positions/slopes if store_moments: self.history.store_moments(self.s) if store_coords: self.history.store_coords(self.s) self.bunch.X[:, [1, 3]] = Xp def run(self, meas_every={}): &quot;&quot;&quot;Run the simulation.&quot;&quot;&quot; self.store() self.compute_electric_field() self.kick(-0.5 * self.step_size) # desync positions/slopes for i in trange(self.nsteps): self.compute_electric_field() self.kick(self.step_size) self.push(self.step_size) self.s += self.step_size self.steps_performed += 1 self.store() self.history.package() . Demonstration . We need some way of checking our method&#39;s accuracy. Luckily there is an analytic benchmark available: the Kapchinskij-Vladimirskij (KV) distribution. Without going into any detail, the beam projects to a uniform density ellipse in the $x$-$y$ plane, and the space charge forces produced within this ellipse are linear (in general space charge forces are nonlinear). If we plug the KV distribution into the Vlasov equation, it can be seen that these forces will remain linear for all time if the external focusing forces are also linear. As a consequence, a set of self-consistent differential equations describing the evolution of the ellipse boundary can be written down. If we consider the beam to be an upright ellipse with semi-axis $a$ along the $x$ axis and $b$ along the $y$ axis, then without external fields the equations read: . $$ a&#39;&#39; = frac{2Q}{a + b} + frac{ varepsilon_x}{a^3}, $$ $$ b&#39;&#39; = frac{2Q}{a + b} + frac{ varepsilon_y}{b^3}. tag{18}$$ . These are known as the KV envelope equations or simply envelope equations. $Q$, called the perveance, is a dimensionless number which is proportional to the beam intensity but reduced by the beam energy. We can think of this constant as a measure of the space charge strength. The $ varepsilon_x$ and $ varepsilon_y$ terms are called the emittances and determine the area occupied by the beam in $x$-$x&#39;$ and $y$-$y&#39;$ phase space. For example, a beam with all particles sitting perfectly still in the $x$-$y$ plane has no emittance, but a beam which is instead spreading out has a nonzero emittance. These emittances will also be conserved for the KV distribution. The following function integrates the envelope equations. . def track_env(X, positions, perveance=0.0): &quot;&quot;&quot;Track beam moments (assuming KV distribution) through free space. Parameters - X : ndarray, shape (nparts, 4) Transverse bunch coordinate array. positions : list List of positions at which to evaluate the equations. perveance : float The dimensionless space charge perveance. Returns - ndarray, shape (len(positions), 4) Each row gives [a, a&#39;, b, b&#39;], where a and b are the beam radii in the x and y dimension, respectively. &quot;&quot;&quot; Sigma = np.cov(X.T) a, b = np.sqrt(Sigma[0, 0]), np.sqrt(Sigma[2, 2]) ap, bp = Sigma[0, 1] / a, Sigma[2, 3] / b epsx = np.sqrt(np.linalg.det(Sigma[:2, :2])) epsy = np.sqrt(np.linalg.det(Sigma[2:, 2:])) def derivs(env, s): a, ap, b, bp = env envp = np.zeros(4) envp[0], envp[2] = ap, bp envp[1] = 0.5 * perveance/(a + b) + epsx**2 / a**3 envp[3] = 0.5 * perveance/(a + b) + epsy**2 / b**3 return envp return odeint(derivs, [a, ap, b, bp], positions, atol=1e-14) . Some care must be taken in the choice of simulation parameters; we need a fine enough grid to resolve the hard edge of the beam and enough macroparticles per grid cell to collect good statistics. I chose what I thought was reasonable: 128,000 macroparticles, a step size of 2.5 cm, and a $128 times 128$ grid. Let&#39;s create and track four identical KV distributions, each with a different intensity. . nparts = 128000 bunch_length = 250.0 # [m] intensities = [0.0, 10e14, 20e14, 40e14] # Simulation parameters distance = 10.0 # [m] step_size = 0.025 # [m] grid_size = (128, 128) samples = 10000 meas_every = { &#39;moments&#39;: int(0.1 * distance / step_size), &#39;coords&#39;: 4 } # Create KV bunch in normalized coordinates (surface of 4D unit sphere) X = np.random.normal(size=(nparts, 4)) # 4D Gaussian X = np.apply_along_axis(lambda row: row/np.linalg.norm(row), 1, X) # normalize rows # Scale by emittance eps_x, eps_y = 10e-6, 10e-6 A = 2 * np.sqrt(np.diag([eps_x, eps_x, eps_y, eps_y])) X = np.apply_along_axis(lambda row: np.matmul(A, row), 1, X) # Scale beam size and divergence relative to emittance alpha_x, alpha_y = 0.0, 0.0 beta_x, beta_y = 20.0, 20.0 V = np.zeros((4, 4)) V[:2, :2] = np.sqrt(1/beta_x)* np.array([[beta_x, 0], [alpha_x, 1]]) V[2:, 2:] = np.sqrt(1/beta_y)* np.array([[beta_y, 0], [alpha_y, 1]]) X = np.apply_along_axis(lambda row: np.matmul(V, row), 1, X) # Create and track bunches sims = [] for intensity in intensities: bunch = Bunch(intensity, bunch_length) bunch.fill(np.copy(X)) sim = Simulation(bunch, distance, step_size, grid_size, meas_every=meas_every, samples=samples) sim.run() sims.append(sim) . 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:46&lt;00:00, 8.58it/s] 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:59&lt;00:00, 6.73it/s] 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:54&lt;00:00, 7.38it/s] 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:43&lt;00:00, 9.27it/s] . bunch_positions = sims[0].history.moment_positions env_positions = np.linspace(0, distance, 400) rms_sizes_lists = {&#39;bunch&#39;:[], &#39;env&#39;:[]} for sim in sims: rms_sizes_lists[&#39;bunch&#39;].append(np.sqrt(sim.history.moments[:, [0, 7]])) rms_sizes_lists[&#39;env&#39;].append( track_env(X, env_positions, sim.bunch.perveance)[:, [0, 2]]) fig, axes = plot.subplots(ncols=2, figsize=(5.5, 2.5), spany=False) alphas = np.linspace(0.4, 1.0, 4) cycle = plot.Cycle(plot.Colormap(&#39;mono&#39;, left=0.25, right=1.0)) colors = cycle.by_key()[&#39;color&#39;][::3] for key, rms_sizes_list in rms_sizes_lists.items(): for rms_sizes, alpha, color in zip(rms_sizes_list, alphas, colors): x, y = 1000.0 * rms_sizes.T if key == &#39;env&#39;: axes[0].plot(env_positions, x, c=color, alpha=alpha) axes[1].plot(env_positions, y, c=color, alpha=alpha) elif key == &#39;bunch&#39;: axes[0].scatter(bunch_positions, x, s=8, c=color, zorder=99) axes[1].scatter(bunch_positions, y, s=8, c=color, zorder=99) lines = [Line2D([0], [0], color=color) for color in colors] axes[1].legend(lines, [&#39;I = {:.0e}&#39;.format(I) for I in intensities] + [&#39;PIC&#39;], ncols=1, fontsize=7) axes[0].set_title(&#39;Horizontal&#39;) axes[1].set_title(&#39;Vertical&#39;) axes.format(ylabel=&#39;rms beam size [$mm$]&#39;, xlabel=&#39;Distance [m]&#39;, suptitle=&#39;KV benchmark: drift&#39;, grid=False); . . This plot shows the horizontal and vertical beam size over time for each of the four chosen beam intensities. The solid lines are the result of integrating the envelope equations, while the dots are the result of the PIC calculation. Notice that the beam expands on its own due to the nonzero emittance and that the effect of space charge is to increase the expansion rate. It seems to be quite accurate over this distance, and the runtime is acceptable for my purposes. Here is the evolution of a sample of 10,000 of the macroparicles as well as an ellipse showing the KV envelope. . # Get coordinates coords_list = [sim.history.coords for sim in sims] positions = sims[0].history.coord_positions umax = 1.25 * 1000 * max([np.max(np.max(coords, axis=1)[:, [0, 2]]) for coords in coords_list]) # Create figure fig, axes = plot.subplots(nrows=2, ncols=2, figsize=(5, 4.5)) for ax in axes: for side in [&#39;top&#39;, &#39;right&#39;]: ax.spines[side].set_visible(False) axes.format(xlim=(-umax, umax), ylim=(-umax, umax), xlabel=&#39;x [mm]&#39;, ylabel=&#39;y [mm]&#39;, grid=False) axes[1].legend([Line2D([0], [0], color=&#39;k&#39;)], [&#39;KV envelope&#39;], frameon=False, loc=(0.5, 0.95)) for ax, intensity in zip(axes, intensities): ax.annotate(&#39;I = {:.2e}&#39;.format(intensity), xy=(0.05, 0.9), xycoords=&#39;axes fraction&#39;) plt.close() # Create lines plt_kws = dict(ms=2, c=&#39;black&#39;, marker=&#39;.&#39;, lw=0, mew=0, fillstyle=&#39;full&#39;) lines = [] for ax in axes: line, = ax.plot([], [], **plt_kws) lines.append(line) def update(t): for coords, line, ax, rms_sizes_list in zip(coords_list, lines, axes, rms_sizes_lists[&#39;env&#39;]): line.set_data(1000 * coords[t, :, 0], 1000 * coords[t, :, 2]) rms_sizes_list = rms_sizes_list[::4] a, b = 1000 * rms_sizes_list[t] ax.patches = [] ell = Ellipse((0, 0), 4*a, 4*b, color=&#39;pink7&#39;, fill=False, zorder=100, lw=1) ax.add_patch(ell) axes[0].set_title(&#39;s = {:.2f} m&#39;.format(positions[t])) fps = 20 frames = len(coords_list[0]) - 1 animation.FuncAnimation(fig, update, frames=frames, interval=1000/fps) . . &lt;/input&gt; Once Loop Reflect Conclusion . This post implemented an electrostatic PIC solver in Python. I learned quite a bit from doing this and was happy to see my calculations agree with the theoretical benchmark. One extension of this code would be to consider the velocity-dependent force from magnetic fields. It would also be straightforward to extend the code to 3D. Finally, all the methods used here are applicable to gravitational simulations. Here are some helpful references: . USPAS course | Hockney &amp; Eastwood | Birdsall &amp; Langdon | .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/simulation/space%20charge/2021/02/22/PIC.html",
            "relUrl": "/physics/accelerators/simulation/space%20charge/2021/02/22/PIC.html",
            "date": " • Feb 22, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Coupled parametric oscillators",
            "content": "Equations of motion . A previous post examined the analytic solutions to the equation of motion describing a parametric oscillator — an oscillator whose physical properties are time-dependent. This problem was motivated by describing the transverse oscillations of a charged particle in an accelerator. In this post, the treatment will be extended to a coupled parametric oscillator. Basically, we are trying to solve the following equation of motion: . $$x&#39;&#39; + k_{11}(s)x + k_{13}(s)y + k_{14}(s)y&#39; = 0,$$ $$y&#39;&#39; + k_{33}(s)y + k_{31}(s)x + k_{32}(s)x&#39; = 0,$$ . where the prime denotes differentiation with respect to $s$. We will also assume that $k_{ij}(s + L) = k_{ij}(s)$ for some $L$. . The first possible source of coupling is the longitudinal magnetic field produced within a solenoid magnet. . . Source: brilliant.org If we assume that the solenoid is very long, the field withing the coils points in the longitudinal direction and is approximately constant ($ mathbf{B}_{sol} = B_0 hat{s}$). Plugging this into the Lorentz force equation, we find: . $$ dot{ mathbf{v}} = frac{q}{m} mathbf{v} times mathbf{B} = frac{qB_0}{m} left({v_y hat{x} - v_x hat{y}} right).$$ . The motion in $x$ depends on the velocity in $y$, and vice versa. Coupling can also be produced by transverse magnetic fields. Recall the multipole expansion of a transverse magnetic field $ mathbf{B} = (B_x, B_y)$: . . There will be nonlinear coupling (terms proportional to $x^j y^k$, where $j,k &gt; 1$) when $n &gt; 2$, but we are interested only in linear coupling. This occurs when the skew quadrupole term $a_2$ is nonzero, which is true when a quadrupole is tilted in the transverse plane. The field couples the motion in one plane to the displacement in the other. . Solution . Let&#39;s review the approach we took in analyzing the 1D parametric oscillator. We wrote the solution in pseudo-harmonic form with an amplitude $ sqrt{2 J beta(s)}$ and phase $ mu(s)$. We then focused on the motion of a particle in $x$-$x&#39;$ phase space after each focusing period. We observed that the particle jumps around an ellipse: the area of the ellipse is constant and proportional to $2 J$; the dimensions of the ellipse are determined by $ beta$ and $ alpha = - beta&#39; / 2$; the size of the jumps around the ellipse are determined by $ mu$. We then wrote the symplectic $2 times 2$ transfer matrix $ mathbf{M}$, which connects the initial and final phase space coordinates through one period, as . $$ mathbf{M} = mathbf{V} mathbf{P} mathbf{V}^{-1}.$$ . $ mathbf{V}^{-1}$, which is a function of $ alpha$ and $ beta$, is a symplectic transformation that deforms the ellipse into a circle while preserving its area, and $ mathbf{P}$ is a rotation in phase space by the phase advance $ mu$. . This is a very elegant way to describe the motion with a minimal set of parameters. The question is: can we do something similar for coupled motion, in which the phase space is 4D, not 2D? To start, let&#39;s track a particle in a lattice with a nonzero skew quadrupole coefficient and plot its phase space coordinates after each period. . . The particle traces donut-like shapes in $x$-$x&#39;$ and $y$-$y&#39;$ instead of ellipses. Here are the shapes after 1000 periods. . There is clearly more than one frequency present in the oscillations. . This is typical of a coupled oscillator. The motion in such systems is typically understood as the superposition of normal modes, each of which corresponds to a single frequency. For example, consider two masses connected with a spring. There are two possible ways for the masses to oscillate at the same frequency. The first is a breathing mode in which they move in opposite directions, and the second is a sloshing mode in which they move in the same direction. The motion is the sum of these two modes. We will try to do something similar for a coupled parameteric oscillator. . Transfer matrix eigenvectors . If the phase space coordinate vector $ mathbf{x} = (x, x&#39;, y, y&#39;)^T$ evolves according to . $$ mathbf{x} rightarrow mathbf{Mx},$$ . where $ rightarrow$ represents tracking through one period, it can be shown that $ mathbf{M}$ is symplectic due to the Hamiltonian mechanics of the system. Consider the eigenvectors of $ mathbf{M}$: . $$ mathbf{Mv} = e^{-i mu} mathbf{v}.$$ . The symplecticity condition causes the eigenvalues and eigenvectors come in two complex conjugate pairs; this gives $ mathbf{v}_1$, $ mathbf{v}_2$, $ mu_1$, $ mu_2$ and their complex conjugates. The seemingly complex motion is simplified when written in terms of the eigenvectors. We can write any coordinate vector as a linear combination of the real and imaginary components of $ mathbf{v}_1$ and $ mathbf{v}_2$: . $$ mathbf{x} = Re left( sqrt{2 J_1} mathbf{v}_1e^{-i psi_1} + sqrt{2 J_2} mathbf{v}_2e^{-i psi_2} right).$$ . We have introduced two invariant amplitudes ($J_1$ and $J_2$) as well as two initial phases ($ psi_1$ and $ psi_2$). Applying the transfer matrix tacks on a phase to each eigenvector. Thus, what we are observing are the 2D projections of the real components of these eigenvectors as they rotate in the complex plane. . $$ mathbf{Mx} = Re left( sqrt{2 J_1} mathbf{v}_1e^{-i left( psi_1 + mu_1 right)} + sqrt{2 J_2} mathbf{v}_2e^{-i( psi_2 + mu_2)} right).$$ . Let&#39;s replay the animation, but this time draw a red arrow for $ mathbf{v}_1$ and a blue arrow for $ mathbf{v}_2$. We&#39;ve chosen $J_1 = 4 J_2$ and $ psi_2 - psi_1 = pi/2$. . . Much simpler. Each eigenvector simply rotates at its frequency $ mu_l$. It also explains why the amplitude in the $x$-$x&#39;$ and $y$-$y&#39;$ planes trade back and forth: it is because the projections of the eigenvectors rotate at different frequencies, sometimes aligning and sometimes anti-aligning. Because of this, the previous invariants $J_{x,y}$ are replaced by $J_{1,2}$. It is helpful to think of a torus (shown below). The two amplitudes would determine the inner and outer radii of the torus, and the two phases determine the location of a particle on the surface. . . Source: Wikipedia parameterization . We are now going to introduce a set of parameters for these eigenvectors, and in turn the transfer matrix. We already have two phases, so that leaves 8 parameters. Our strategy is to observe that each eigenvector traces an ellipse in both horizontal ($x$-$x&#39;$) and vertical ($y$-$y&#39;$) phase space. Then, we will simply assign an $ alpha$ function and $ beta$ function to each of these ellipses. So, for the ellipse traced by $ mathbf{v}_1$ in the $x$-$x&#39;$ plane, we have $ beta_{1x}$ and $ alpha_{1x}$, and then for the second eigenvector we have $ beta_{2x}$ and $ alpha_{2x}$. The same thing goes for the vertical dimension with $x$ replaced by $y$. . . The actual eigenvectors written in terms of the parameters are . $$ vec{v}_1 = begin{bmatrix} sqrt{ beta_{1x}} - frac{ alpha_{1x} + i(1-u)}{ sqrt{ beta_{1x}}} sqrt{ beta_{1y}}e^{i nu_1} - frac{ alpha_{1y} + iu}{ sqrt{ beta_{1y}}} e^{i nu_1} end{bmatrix}, quad vec{v}_2 = begin{bmatrix} sqrt{ beta_{2x}}e^{i nu_2} - frac{ alpha_{2x} + iu}{ sqrt{ beta_{2x}}}e^{i nu_2} sqrt{ beta_{2y}} - frac{ alpha_{2y} + i(1-u)}{ sqrt{ beta_{2y}}} end{bmatrix}$$ . So in addition to the phases $ mu_1$ and $ mu_2$ we have $ alpha_{1x}$, $ alpha_{2x}$, $ alpha_{1y}$, $ alpha_{2y}$, $ beta_{1x}$, $ beta_{2x}$, $ beta_{1y}$, and $ beta_{2y}$. That&#39;s pretty much it. There are a few other parameters we need to introduce to simplify the notation, but they are not independent. The first is $u$, which, as noted in the figure, determines the areas of the ellipses in one plane relative to the other. The second and third are $ nu_1$ and $ nu_2$, which are phase differences between the $x$ and $y$ components of the eigenvectors (in the animation they are either $0$ or $ pi$). I won&#39;t discuss these here. The last thing to note is that the parameters reduce to their 1D definitions when there is no coupling in the lattice. So we would have $ beta_{1x}, beta_{2y} rightarrow beta_{x}, beta_{y}$ and $ beta_{2x}, beta_{1y} rightarrow 0$, and similar for $ alpha$. The invariants and phase advances would also revert back to their original values: $J_{1,2} rightarrow J{x,y}$ and $ mu_{1,2} rightarrow mu_{x,y}$. . Floquet transformation . These eigenvectors can also be used to construct a transformation which removes both the variance in the focusing strength and the coupling between the planes, turning the coupled parametric oscillator into an uncoupled harmonic oscillator. In other words, we seek a matrix $ mathbf{V}$ such that . $$ mathbf{V^{-1} M V} = mathbf{P} = begin{bmatrix} cos{ mu_1} &amp; sin{ mu_1} &amp; 0 &amp; 0 - sin{ mu_1} &amp; cos{ mu_1} &amp; 0 &amp; 0 0 &amp; 0 &amp; cos{ mu_2} &amp; sin{ mu_2} 0 &amp; 0 &amp; - sin{ mu_2} &amp; cos{ mu_2} end{bmatrix} $$We can do this simply by rewriting the following equation (I haven&#39;t yet figured out how to number equations in Jupyter): . $$ mathbf{x} = Re left( sqrt{2J_1} mathbf{v}_1e^{-i psi_1} + sqrt{2J_2} mathbf{v}_2e^{-i psi_2} right)$$ . in matrix form as $ mathbf{x} = mathbf{V} mathbf{x}_n$ with . $$ mathbf{x}_n = begin{bmatrix} sqrt{2J_1} cos{ psi_1} - sqrt{2J_1} sin{ psi_1} sqrt{2J_2} cos{ psi_2} - sqrt{2J_2} sin{ psi_2} end{bmatrix} $$ $$ mathbf{V} = left[{Re( mathbf{v}_1), -Im( mathbf{v}_1), Re( mathbf{v}_2), -Im( mathbf{v}_2)} right]$$ . Let&#39;s observe the motion in these new coordinates $ mathbf{x}_n$. . . The motion is uncoupled after this transformation; i.e., particles move in a circle of area $2J_1$ in the $x_n$-$x_n&#39;$ plane at frequency $ mu_1$, and in a circle of area $2J_2$ in the $y_n$-$y_n&#39;$ plane at frequency $ mu_2$. . Conclusion . The method introduced here allows us to describe the evolution of a parametric oscillator using the minimum number of parameters. Our physical motivation was an accelerator lattice with linear, coupled forces. There is no agreed upon method to do this among accelerator physicists, but I like (and know) this method the best, and have used it in my research. I&#39;ve left out details which can be found in the paper by Lebedev and Bogacz. The paper by Ripken is also very helpful. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/differential%20equations/2021/01/25/coupled_parametric_oscillators.html",
            "relUrl": "/physics/accelerators/differential%20equations/2021/01/25/coupled_parametric_oscillators.html",
            "date": " • Jan 25, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Parametric oscillators",
            "content": "This post presents the solution to a general problem: what is the motion of a particle in one dimension (1D) in the presence of time-dependent, linear, periodic forces? This amounts to solving the following equation of motion: . $$ frac{d^2x}{dt^2} + k(t)x = 0,$$ . where $k(t + T) = k(t)$ for some $T$. This is a parametric oscillator, a harmonic oscillator whose physical properties are not static. For example, the oscillations of a pendulum (in the small angle approximation) on the surface of a planet whose gravitational pull varies periodically would be described by the above equation. The solution to this equation was derived by George William Hill in 1886 to study lunar motion, and for this reason it is known as Hill&#39;s equation. It also finds application in areas such as condensed matter physics, quantum optics, and accelerator physics. After setting up the physical problem, we will examine the solutions and discuss their relevance to the last application, accelerator physics. . Problem motivation . Accelerator physics . Particle accelerators are machines which produce groups of charged particles (known as beams), increase their kinetic energy, and guide them to a target. These machines are invaluable to modern scientific research. The most famous examples are colliders, such as the LHC, in which two beams are smashed together to generate fundamental particles. A lesser known fact is that the fields of condensed matter physics, material science, chemistry, and biology also benefit tremendously from accelerators; this is due to the effectiveness of scattering experiments in which the deflection of a beam after colliding with a target is used to learn information about the target. The scattered beam is composed of neutrons in spallation neutron sources such as SNS, electrons in electron scattering facilities such as CEBAF, or photons in synchrotron light sources such as APS. In addition to scientific research, accelerators find use in medicine, particularly for cancer treatment, and also in various industrial applications. . . A large detector at an interaction point in the LHC. There are generally a few beam properties which are very important to experimentalists; in colliders it is the energy and luminosity, in spallation sources it is the intensity, and in light sources it is the brightness. There is thus a constant need to push these parameters to new regions. For example, below is the famous Livingston plot which shows the energy achieved by various machines over the past century. . . Note: vertical axis scale is beam energy needed to produce the center of mass energy by collision with a resting proton (credit: Rasmus Ischebeck). There are many physics issues associated with the optimization of these beam parameters. Accelerator physics is a field of applied physics which studies these issues. The task of the accelerator physicist is to understand, control, and measure the journey of the beam from its creation to its final destination. The difficulty of this task has grown over time; the improvement accelerator performance has brought with it a staggering increase in size and complexity. The construction and operation of modern accelerators generally requires years of planning, thousands of scientists and engineers, and hundreds of millions or even billions of dollars. Despite this complexity, the underlying physics principles are quite simple, and the single particle motion in one of these machines can be understood analytically if a few approximations are made. In the end we will arrive at Hill&#39;s equation. . Controlling charged particle trajectories . There are three basic tasks an accelerator has to accomplish. First, it must increase the beam energy (acceleration). Second, it must guide the beam along a predetermined path (steering). Third, it must ensure the beam particles remain close together (focusing). It is helpful to use a coordinate system in which the $s$ axis points along the design trajectory, and the $x$ and $y$ axes defined in the plane transverse to $s$. In this way the motion is broken up into transverse and longitudinal dynamics. . . How are these tasks accomplished? Well, particles are charged, and the force on a point charge in an electromagnetic field is given by . $$ mathbf{F} = q left({ mathbf{E} + mathbf{v} times mathbf{B}} right),$$ . where $q$ is the particle charge, $ mathbf{v}$ is the particle velocity, $ mathbf{E}$ is the electric field, and $ mathbf{B}$ is the magnetic field. An accelerator consists of a series of elements, each with their own $ mathbf{E}$ and $ mathbf{B}$; the collection of these elements is called a lattice. We need to determine which electric and magnetic fields to use. . The first task, acceleration, is not the focus of this post. The remaining tasks, steering and focusing, concern the motion in the transverse plane. $ mathbf{B}$ fields, not $ mathbf{E}$ fields, are used since their effect grows with increased particle velocity. Any transverse magnetic field $ mathbf{B} = (B_x, B_y)^T$ can be written using a multipole expansion . $$B_y - iB_x = sum_{n=1}^{ infty} left({b_n - ia_n} right) left( frac{x + iy}{r_0} right)^{n-1}.$$ . We then have the normal multiple coefficients $ {b_n }$, and the skew multipole coefficients $ {a_n }$. The field lines corresponding to the first few terms are shown below. . . The dipole field $ mathbf{B} propto hat{y}$ is perfect for steering, producing the force $ mathbf{F} propto - hat{x}$ for a particle moving into the page. The quadrupole field $ mathbf{B} propto y hat{x} + x hat{y}$ produces the force $ mathbf{F}_{quad} propto -x hat{x} + y hat{y}$, which is focusing in the horizontal direction, but defocusing in the vertical direction; however, net focusing can still be achieved by alternating the direction of the quadrupoles. This is analogous to a beam of light passing through a series of converging and diverging lenses. If the spacing and curvature of the lenses is correctly chosen, a net focusing can be achieved. . . Focusing (QF) and defocusing (QD) quadrupoles modeled as magnetic lenses. The forces which result from these fields are linear, meaning they are proportional the $x$ or $y$ but not $x^2$, $y^3$, etc., and they are uncoupled, meaning the dynamics in the $x$ and $y$ dimensions are independent. Now, we may ask, can we really produce a perfect dipole or quadrupole field? The answer is no. In reality there will always be higher order multipoles present in the field, but people work very hard to ensure these are much smaller than the desired multipole. This video shows a bit of the construction process for these magnets. . Linearized equation of motion . For small oscillations, the equations of motion reduce to . $$ x&#39;&#39; approx - frac{q}{mc beta_s gamma_s} B_y(x, y, s), $$$$ y&#39;&#39; approx + frac{q}{mc beta_s gamma_s} B_x(x, y, s), $$where $x&#39; = dx/ds$, $m$ is the particle mass, $c$ is the speed of light in a vacuum, $ beta_s$ is the particle speed divided by $c$, and $ gamma_s = (1 - beta_s^2)^{-1/2}$. (For simplicity, the curved coordinate system has not been taken into account). We will ignore nonlinear terms since they greatly complicate the dynamics. We will also ignore coupling between the planes. With these approximations, we arrive at the equation of motion for a single particle in the transverse plane: . $$x&#39;&#39; + k(s)x = 0.$$ . Solution . Envelope function . The general solution to Hill&#39;s equation is given by . $$x(s) = sqrt{2J} ,w(s) cos left({ mu(s) + delta} right).$$ . This introduces an amplitude $w(s) = w(s + L)$ which we call the envelope function, as well as a phase $ mu$, both of which depend on $s$. The constants $J$ and $ delta$ are determined by the initial conditions. Let&#39;s plot this trajectory in a FODO (focus-off-defocus-off) lattice, which consists of evenly spaced focusing and defocusing quadrupoles. Here is the focusing strength within the lattice (QF is the focusing quadrupole and QD is the defocusing quadrupole): . . For now we can think of the lattice as repeating itself forever in the $s$ direction. Each black line below is represents the trajectory for a different initial position and slope; although the individual trajectories look rather complicated, the envelope function has a very simple form. . . Phase space . The particle motion becomes much easier to interpret if we observe it in position-momentum space, aka phase space. The following animation shows the evolution of the particle phase space coordinates at a single position in the lattice. The position shown is $s = nL/4$, where $n$ is the period number, which corresponds to the midpoint between the focusing and defocusing quadrupoles. . . . The particle jumps around an ellipse in phase space. The shape and orientation of the ellipse will change if we look at a different position in the lattice, but its area will be the same. So, the motion is determined by the dimensions and oriention of this ellipse throughout the lattice, as well as the location of the paricle on the ellipse boundary. This motivates the definition of the so-called Twiss parameters, which were first introduced by Courant and Snyder in 1958: . $$ beta = w^2, quad alpha = - frac{1}{2} beta&#39;, quad gamma = frac{1 + alpha^2}{ beta}.$$ . The dimensions of the phase space ellipse are nicely described by these parameters: . . The maximum extent of the ellipse is determined by $ beta$ in the $x$ direction and $ gamma$ in the $y$ direction. $ alpha$ is proportional to the slope of the $ beta$ function, and so determines the tilt angle of the ellipse. The position of a particle on the ellipse is given by the phase $ mu$. Finally, the invariant of the motion corresponding to the ellipse area is proportional to $2J = beta {x&#39;}^2 + 2 alpha xx&#39; + gamma x^2$ for any $x$ and $x&#39;$. The $ beta$ functions and phase advances in both dimensions are extremely important to measure and control in a real machine. . Transfer matrices . A helpful tool to pair with the parameterization we just introduced is the transfer matrix, a matrix which connects the phase space coordinates at two different positions: . $$ begin{bmatrix} x x&#39; end{bmatrix}_{s + L} = mathbf{M} begin{bmatrix} x x&#39; end{bmatrix}_{s}$$ . The transfer matrix can be written as $ mathbf{M} = mathbf{V} mathbf{P} mathbf{V}^{-1}$, where . $$ mathbf{V} = frac{1}{ sqrt{ beta}} begin{bmatrix} beta &amp; 0 - alpha &amp; 1 end{bmatrix}$$ and $$ mathbf{P} = begin{bmatrix} cos mu &amp; sin mu - sin mu &amp; cos mu end{bmatrix} $$ . The effect of $ mathbf{V}^{-1}$ is to deform the phase space ellipse into a circle while preserving its area. $ mathbf{P}$ is then just a rotation in phase space, and $ mathbf{V}$ then transforms back into a tilted ellipse. This is illustrated below. $ mathbf{V}$ can be thought of as a time-dependent transformation which removes the variance in the focusing strength, turning the parametric oscillator into a simple harmonic oscillator. It is often called the Floquet transformation. . . Conclusion . We&#39;ve presented the solution to Hill&#39;s equation, which describes a parameteric oscillator. The equation pops up in multiple areas, but we focused on its application in accelerator physics, in which Hill&#39;s equation describes the transverse motion of a single particle in an accelerator with perfectly linear magnetic fields. . The solution is best understood geometrically: particles move around the surface of an ellipse in phase space, the area of which is an invariant of the motion. The dimensions and orientation of the ellipse are determined by $ alpha$ and $ beta$, and the location of the paricle on the ellipse boundary is determined by $ mu$. These parameters can be used to construct a time-dependent transformation ($ mathbf{V}$) which turns the parametric oscillator into a simple harmonic oscillator. . The next post will examine how this treatment can be extended to include coupling between the horizontal and vertical dimensions. .",
            "url": "https://austin-hoover.github.io/blog/physics/accelerators/differential%20equations/2021/01/21/parametric_oscillators.html",
            "relUrl": "/physics/accelerators/differential%20equations/2021/01/21/parametric_oscillators.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "References",
            "content": "Accelerator physics . Accelerator Physics — Lee | Introduction to the Physics of High Energy Accelerators — Edwards/Syphers | Measurement and Control of Charged Particle Beams — Minty/Zimmerman | Particle Accelerator Physics — Wiedemann | Space Charge Physics for Particle Accelerators — Hofmann | . Machine learning . Neural Networks and Deep Learning — Nielsen | Pattern Recognition and Machine Learning — Bishop | . Mathematics . Mathematics of Classical and Quantum Physics — Byron/Fuller | Mathematical Methods for Physicists — Arfken/Weber | . Physics core . Classical Mechanics — Taylor | Classical Mechanics — Goldstein | Introduction to Electrodynamics — Griffiths | Introduction to Thermal Physics — Schroeder | Quantum Mechanics: Concepts and Applications — Zettili | Statistical Physics of Particles — Kardar | . Programming . Algorithms — Sedgewick/Wayne | Elements of Programming Interviews in Python — Aziz/Lee/Prakash | .",
            "url": "https://austin-hoover.github.io/blog/2021/01/01/references.html",
            "relUrl": "/2021/01/01/references.html",
            "date": " • Jan 1, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "About . Most posts on this blog will fall under one of the following categories: . Physics, particularly accelerator physics | Programming, data analysis, statistics, etc. | Philosophy, particularly philosophy of religion and philosophy of science | . Please visit my home page for my publications, CV, etc. .",
          "url": "https://austin-hoover.github.io/blog/_pages/about.html",
          "relUrl": "/_pages/about.html",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Archive",
          "content": "Archive . &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/theism/cosmological%20arguments/2022/03/22/arguing_about_gods_3b.html&quot;&gt; Arguing About Gods (part 3b) &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Mar 22, 2022 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/accelerators/plotting/2022/02/11/some_figures.html&quot;&gt; Some figures to illustrate beam injection and accumulation &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Feb 11, 2022 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/physics/accelerators/tomography/2021/10/16/tomographic_reconstruction_in_four_dimensions.html&quot;&gt; Tomographic reconstruction in four dimensions &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Oct 16, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/theism/cosmological%20arguments/2021/09/15/arguing_about_gods_3a.html&quot;&gt; Arguing About Gods (part 3a) &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Sep 15, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/theism/ontological%20arguments/2021/07/05/arguing_about_gods_2.html&quot;&gt; Arguing About Gods (part 2) &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Jul 5, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/physics/space%20charge/simulation/resonances/2021/07/01/space_charge_instabilities.html&quot;&gt; Space charge resonances and instabilities &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Jul 1, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/theism/arguments/2021/06/12/arguing_about_gods_1.html&quot;&gt; Arguing About Gods (part 1) &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Jun 12, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/accelerators/space%20charge/2021/05/27/painting_a_particle_beam.html&quot;&gt; Painting a particle beam &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; May 27, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/accelerators/space%20charge/differential%20equations/publications/2021/05/13/matched_Danilov_dist.html&quot;&gt; Computing matched envelopes &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; May 13, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/stylometry/machine%20learning/natural%20language%20processing/2021/04/29/authorship_identification.html&quot;&gt; Authorship identification &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Apr 29, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/physics/accelerators/nonlinear%20systems/resonances/2021/03/28/nonlinear_resonances.html&quot;&gt; Nonlinear resonances &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Mar 28, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/physics/accelerators/simulation/space%20charge/2021/02/22/PIC.html&quot;&gt; Particle-in-cell simulation &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Feb 22, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/physics/accelerators/differential%20equations/2021/01/25/coupled_parametric_oscillators.html&quot;&gt; Coupled parametric oscillators &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Jan 25, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/physics/accelerators/differential%20equations/2021/01/21/parametric_oscillators.html&quot;&gt; Parametric oscillators &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Jan 21, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot; style=&quot;text-align:left;&quot;&gt; &lt;a class=&quot;page-meta&quot; href=&quot;/blog/2021/01/01/references.html&quot;&gt; References &lt;/a&gt; &lt;span style=&quot;float:right; font-size:13px;&quot;&gt; Jan 1, 2021 &lt;/span&gt; &lt;/p&gt; &lt;/article&gt; .",
          "url": "https://austin-hoover.github.io/blog/archive",
          "relUrl": "/archive",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://austin-hoover.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}