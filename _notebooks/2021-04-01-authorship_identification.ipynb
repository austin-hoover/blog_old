{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship identification\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [stylometry, machine learning, natural language processing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last year I was able to take a few elective courses in the computer science department: Data Mining in the spring semester and Intro to Machine Learning in the fall semester. It was a nice change of pace to study something different than physics. One project I worked on was to perform authorship identification using support vector machines. It was a pretty fun project, so I thought I would post the results here. The goal of the project was to reproduce the results of [this paper](https://www.researchgate.net/publication/221655968_N-Gram_Feature_Selection_for_Authorship_Identification). Reproducing someone else's results is a good thing to do because 1) it validates their research and 2) it is a chance to actively learn about a topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stylometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation: the Federalist Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Federalist Papers are an important collection of 85 essays written by Hamilton, Madison, and Jay during 1787 and 1788. They were published under the alias \"Plubious\" at the time, and although it became well known that the three contributed to papers, the authorship of each individual paper was kept hidden for over a decade. It was actually in the interest Hamilton and Madison, both politicians, to keep the authorship a secret (they had both changed their position on a number of issues and didn't want their political opponents to use their old arguments against them). Days before his death, however, Hamilton allegedly wrote down who he believed to be the correct author of each essay, claiming over 60 for himself. Madison waited a number of years before publishing his own list, and in the end there were 12 essays to which both Madison and Hamilton claimed authorship. Many interesting details on the controversy can be found in a paper by [Adair](https://www.jstor.org/stable/1921883?read-now=1&seq=23#page_scan_tab_contents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images_next/Hamilton_and_Madison.png width=550>\n",
    "<figcaption>Alexander Hamilton (left) and James Madison (right). Credit: Wikipedia.</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways we could go about resolving this dispute. The first approach is to analyze the actual *content* of the text. For example, perhaps an essay draws from a reference which only Madison was intimately familiar with, or perhaps an essay is similar to some previous work by Hamilton. This was done many times over the next 150 years, but perhaps the final word on the subject was by Adair in 1944, who concluded that Madison likely wrote all 12 essays. An alternative approach is to determine the authorship using only the words on the page, i.e., to analyze the *style* of the text. For example, maybe Madison used many more commas than Hamilton. The field of *stylometry* attempts to statistically quantify these stylistic differences. David Holmes writes the following about stylometry:\n",
    "> *At its heart lies an assumption that authors have an unconscious aspect to their style, an aspect which cannot consciously be manipulated but which possesses features which are quantifiable and which may be distinctive.*\n",
    "\n",
    "I think this a valid assumption. The question is which features best characterize the author's style and which methods are best to use in the analysis of these features. Let's go back in time a bit to the first attempts at stylometry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Thomas Mendenhall](https://en.wikipedia.org/wiki/Thomas_Corwin_Mendenhall), a physicist, is considered the first to statistically analyze large literary texts. He presented the following interesting idea in an 1887 paper titled [*The Characteristic Curves of Composition*](https://www.jstor.org/stable/pdf/1764604.pdf): it is well-known that each element has a unique distribution of wavelengths in the light which it emits when it is heated; perhaps each author has a unique distribution of word lengths in the texts they have written. It's a really cool idea, and I highly recommend reading his original paper. Mendenhall tallied word lengths by hand for various books, usually in batches of 1000 words or so. Here is Fig. 2 from his paper which shows the characteristic curves for a few excerpts of *Oliver Twist*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images_next/Mendenhall_Fig2.png width=400>\n",
    "<figcaption>Distribution of word lengths in \"Oliver Twist\". Each curve is for a different sample of 1000 words.</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He showed that these curves are very interesting and that they do show similarities between different works by the same author. The use of these statistics for authorship identification was left for future work. \n",
    "\n",
    "The next significant advance in the statistical analysis of text was made by Zipf in 1932. Zipf found an interesting relationship between an integer $k$ and the frequency $f(k)$ of the $k$th most frequent word. This is often called a *rank-frequency* relationship where $k$ is the rank. The scaling law can be written as\n",
    "\n",
    "$$ f(k) \\propto k^{-1}. \\tag{1}$$\n",
    "\n",
    "The basic idea expressed by this power law is that short words are much more frequent than large words. Surprisingly, the law holds up very well, albeit not perfectly, for most texts. *Why* this is the case is still unknown; a comprehensive review of the current state of the law can be found [here](https://link.springer.com/article/10.3758/s13423-014-0585-6). The law also shows up in other situations such as national GDP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images_next/Cristelli_Fig1_big.png width=500>\n",
    "<figcaption>Zipf’s Law for National GDPs over time, which appear to be moving toward the prediction by Zipf's Law (red line). From Ref. [1].</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not directly applicable to authorship identification, the success of Zipf's Law was very encouraging and led to a flurry of new mathematical models in the following years. Stylometry had a landmark case in the 1960's when researchers used the frequency distributions of short function words — words we don't think about too much like \"upon\" or \"therefore\" — to support Adair's conclusion that Madison wrote the 12 disputed Federalist Papers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modern methods: Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the day, simple models created in the spirit of Zipf's Law are probably doomed to fail. The \"true\" underlying model must be intertwined with human psychology and therefore be very complex. There are now many algorithms available which instead build predictive models directly from data, and these can be readily applied to the problem of authorship identification. Here we focus on the use of the Support Vector Machine (SVM) to perform the supervised learning task. Let's briefly review the idea behind the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its basic form the SVM is a *linear, binary* classifier. Binary means there are two classes, and linear means we use a plane to separate the data. The equation for a plane in any number of dimensions is\n",
    "\n",
    "$$ y(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} + b = 0 \\tag{2}, $$\n",
    "\n",
    "where $\\mathbf{w}$ is the vector normal to the plane. Our data will consist of a set of vectors $\\left\\{\\mathbf{x}_i\\right\\}$, and we'll assign the points to class $t_i = +1$ if $y(\\mathbf{x}_i) > 0$ or $t_i = -1$ if $y(\\mathbf{x}_i) < 0$, i.e., $t_iy(\\mathbf{x}_i) > 0$. We want to use a plane which best classifies the data, which amounts to selecting the best $\\mathbf{w}$ and $b$ in Eq. (2). The SVM tries to find the optimal separating plane. Optimal in what sense? The metric used is the *margin*, which is defined as the distance from the nearest point to the plane. It can be shown that this distance is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images_next/svm.png width=300>\n",
    "<figcaption>Maximum margin separating plane. Credit: Wikipedia.</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters 50/50 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think what I'll try next is to identify artists from images of their paintings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
